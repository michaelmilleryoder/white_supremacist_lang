{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2f216d-1a61-4fe4-aace-628ea800ce14",
   "metadata": {},
   "source": [
    "# Load, construct dataset\n",
    "Examples of language from accounts and discourse spaces known for white supremacist extremism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435819b2-5b40-4679-bf58-bc622a2d97cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Qian+2018\n",
    "Try to match ideologies with tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e178c86-d157-4f67-9cd6-5cef90e5caac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load scraped data (Qian+2018)\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "dirpath = '/storage2/mamille3/white_supremacist_lang/data/qian2018'\n",
    "\n",
    "# Load tweet data\n",
    "path = os.path.join(dirpath, 'data.jsonl')\n",
    "with open(path, 'r') as f:\n",
    "    tweets = [json.loads(tweet) for tweet in f.read().splitlines()]\n",
    "    print(len(tweets))\n",
    "\n",
    "# Build df\n",
    "qian2018 = pd.json_normalize(tweets)\n",
    "# print(qian2018.columns)\n",
    "\n",
    "# Anonymize texts\n",
    "from tqdm.notebook import tqdm\n",
    "import pdb\n",
    "import re\n",
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "def remove_mentions(text, user_mentions):\n",
    "    \"\"\" Remove mentions from a text (not technically needed with NLTK tokenizer, but still helps) \"\"\"\n",
    "    new_text = text\n",
    "    usernames = [mention['username'] for mention in user_mentions]\n",
    "    for username in usernames:\n",
    "        new_text = re.sub(r'@+'+username, '@USER', new_text, flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "def remove_urls(text, urls):\n",
    "    new_text = text\n",
    "    urls = [entity['url'] for entity in urls]\n",
    "    for url in urls:\n",
    "        new_text = new_text.replace(url, '<URL>')\n",
    "    return new_text\n",
    "\n",
    "def process_text(text, user_mentions, urls):\n",
    "    new_text = text\n",
    "    if isinstance(user_mentions, list):\n",
    "        new_text = remove_mentions(new_text, user_mentions)\n",
    "    if isinstance(urls, list):\n",
    "        new_text = remove_urls(new_text, urls)\n",
    "    new_text = ' '.join(tokenizer.tokenize(new_text))\n",
    "    return new_text.lower()\n",
    "\n",
    "qian2018['processed_text'] = [process_text(text, user_mentions, urls) for text, user_mentions, urls in tqdm(zip(\n",
    "    qian2018['text'], qian2018['entities.mentions'], qian2018['entities.urls']), total=len(qian2018))]\n",
    "\n",
    "qian2018.rename(columns={'id': 'tweet_id'}, inplace=True)\n",
    "\n",
    "qian2018['id'] = 'qian2018_' + qian2018.index.astype(str)\n",
    "qian2018.set_index('id', inplace=True)\n",
    "qian2018.head()\n",
    "\n",
    "qian2018['dataset'] = 'qian2018'\n",
    "print(qian2018.columns)\n",
    "qian2018.head()\n",
    "\n",
    "qian2018['timestamp'] = pd.to_datetime(qian2018['created_at'])\n",
    "# qian2018.timestamp\n",
    "qian2018['source'] = 'twitter'\n",
    "\n",
    "# Format data\n",
    "# Need columns of 'id', 'text', 'dataset'\n",
    "data = qian2018[['processed_text', 'timestamp', 'dataset']].copy().rename(columns={'processed_text': 'text'})\n",
    "print(data.timestamp.dtype)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738766b1-b1a4-4484-9519-5c2596e06963",
   "metadata": {},
   "source": [
    "## ElSherief+2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79798d8c-6a81-4991-a512-6b9bf1e881b7",
   "metadata": {},
   "source": [
    "### Load scraped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43398872-8880-48db-99e2-cf5a42a1fd59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load saved out results\n",
    "import os\n",
    "import json\n",
    "\n",
    "dataset = 'elsherief2021'\n",
    "dirpath = os.path.join('/storage2/mamille3/white_supremacist_lang/data', dataset)\n",
    "\n",
    "# Load tweet data\n",
    "path = os.path.join(dirpath, 'data.jsonl')\n",
    "with open(path, 'r') as f:\n",
    "    tweets = [json.loads(tweet) for tweet in f.read().splitlines()]\n",
    "    print(len(tweets))\n",
    "\n",
    "# Load users\n",
    "path = os.path.join(dirpath, 'users.jsonl')\n",
    "with open(path, 'r') as f:\n",
    "    users = [json.loads(user) for user in f.read().splitlines()]\n",
    "    print(len(users))\n",
    "\n",
    "# Load the tweets into a tweet_dataframe\n",
    "import pandas as pd\n",
    "\n",
    "tweet_data = pd.json_normalize(tweets)\n",
    "print(tweet_data.columns)\n",
    "print(len(tweet_data))\n",
    "tweet_data.id = tweet_data.id.astype('int64')\n",
    "tweet_data.id.dtype\n",
    "\n",
    "tweet_data.head()\n",
    "\n",
    "# Merge in user info\n",
    "user_data = pd.json_normalize(users)\n",
    "user_data.drop_duplicates(subset='id', inplace=True)\n",
    "user_data.set_index('id', drop=True, inplace=True)\n",
    "\n",
    "# print(len(user_data))\n",
    "# print(user_data.columns)\n",
    "user_data.head()\n",
    "\n",
    "elsherief2021_hydrated = tweet_data.join(user_data, on='author_id', rsuffix='_user')\n",
    "print(len(elsherief2021_hydrated))\n",
    "print(elsherief2021_hydrated.columns)\n",
    "elsherief2021_hydrated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01df5f-8bbc-4257-a1fe-6e5fcf71c4a8",
   "metadata": {},
   "source": [
    "### Use Qian+2018 labels to specify ideologies of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcba84-10c7-448e-95c6-41546da15b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlap with the ~4k tweets that are overlaps with Qian+2018\n",
    "# Load Qian+2018 tweet IDs\n",
    "\n",
    "fpath = '/storage2/mamille3/data/hate_speech/qian2018/white_supremacist_tweets.csv'\n",
    "qian2018_tweet_ids = pd.read_csv(fpath)['tweet id']\n",
    "print(len(qian2018_tweet_ids))\n",
    "qian2018_tweet_ids.dtype\n",
    "\n",
    "# Check for overlap with usernames in Qian+2018 set from selected ideologies\n",
    "# Load Qian+2018 scraped users\n",
    "import os\n",
    "import json\n",
    "\n",
    "dirpath = '/storage2/mamille3/white_supremacist_lang/data/qian2018'\n",
    "\n",
    "# Load users\n",
    "path = os.path.join(dirpath, 'users.jsonl')\n",
    "with open(path, 'r') as f:\n",
    "    users = [json.loads(user) for user in f.read().splitlines()]\n",
    "    # print(len(users))\n",
    "\n",
    "user_data = pd.json_normalize(users)\n",
    "# user_data.id = user_data.id.astype('int64')\n",
    "user_data.drop_duplicates(subset='id', inplace=True)\n",
    "user_data.set_index('id', drop=True, inplace=True)\n",
    "# print(len(user_data))\n",
    "print(user_data.columns)\n",
    "user_data.head()\n",
    "\n",
    "# White grievance tweets\n",
    "\n",
    "# Load stage 2 annotations, implicit categories which include white grievance\n",
    "stg2 = pd.read_csv('/storage2/mamille3/data/hate_speech/elsherief2021/implicit_hate_v1_stg2_posts.tsv', sep='\\t')\n",
    "white_grievance = stg2.query('implicit_class==\"white_grievance\" or extra_implicit_class==\"white_grievance\"').rename(columns={'post': 'text'})\n",
    "print(len(white_grievance))\n",
    "white_grievance.head()\n",
    "\n",
    "# Hydrated overlap with Qian+2018 ideologies\n",
    "id_matches = elsherief2021_hydrated[elsherief2021_hydrated['id'].isin(qian2018_tweet_ids)]\n",
    "# print(len(id_matches))\n",
    "\n",
    "# Check overlap of usernames with author IDs\n",
    "user_matches = elsherief2021_hydrated[elsherief2021_hydrated['author_id'].isin(user_data.index)]\n",
    "# print(len(user_matches))\n",
    "\n",
    "# Add white grievance to hydrated ElSherief+2021 tweets\n",
    "# May have duplicates (and could remove them if I match them with tweet ids)\n",
    "elsherief2021 = pd.concat([white_grievance, id_matches, user_matches]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "elsherief2021\n",
    "\n",
    "elsherief2021.rename(columns={'id': 'tweet_id'}, inplace=True)\n",
    "\n",
    "elsherief2021['id'] = 'elsherief2021_' + elsherief2021.index.astype(str)\n",
    "elsherief2021.set_index('id', inplace=True)\n",
    "elsherief2021.head()\n",
    "\n",
    "elsherief2021['dataset'] = 'elsherief2021'\n",
    "\n",
    "# Format data\n",
    "# Need index of id, 'text', 'dataset'\n",
    "elsherief2021['processed_text'] = [process_text(text, user_mentions, urls) for text, user_mentions, urls in tqdm(zip(\n",
    "    elsherief2021['text'], elsherief2021['entities.mentions'], elsherief2021['entities.urls']), total=len(elsherief2021))]\n",
    "elsherief2021.columns\n",
    "\n",
    "elsherief2021['timestamp'] = pd.to_datetime(elsherief2021['created_at'])\n",
    "elsherief2021['source'] = 'twitter'\n",
    "elsherief2021 = elsherief2021[['processed_text', 'timestamp', 'dataset', 'source']].rename(columns={'processed_text': 'text'})\n",
    "elsherief2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b67527e-e4d2-44e6-8808-b695b96011b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93447\n",
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qian2018         88069\n",
       "elsherief2021     5378\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, elsherief2021])\n",
    "print(len(data))\n",
    "\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7d928-bb07-4364-9974-6509df641c75",
   "metadata": {},
   "source": [
    "## De Gibert+2019 (Stormfront) corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266390db-1658-478f-ac94-64c0a63cf084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "98447\n",
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qian2018         88069\n",
       "elsherief2021     5378\n",
       "degibert2019      5000\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "stormfront_dirpath = '/storage2/mamille3/data/hate_speech/degibert2019/'\n",
    "stormfront_data = pd.read_csv(os.path.join(stormfront_dirpath, 'combined_data.csv'))\n",
    "# print(len(stormfront_data))\n",
    "stormfront_data.head()\n",
    "\n",
    "# Group by comment\n",
    "stormfront_data['sentence_id'] = stormfront_data['sentence_id'].astype(int)\n",
    "degibert2019 = stormfront_data.sort_values(['comment_id', 'sentence_id']).groupby('comment_id').agg(\n",
    "    {\n",
    "     'sentence_id': lambda x: list(x.astype(str)),\n",
    "        'user_id': 'first',\n",
    "        'text': ' '.join, \n",
    "    })\n",
    "degibert2019['dataset'] = 'degibert2019'\n",
    "degibert2019['id'] = 'degibert2019_' + degibert2019.index.astype(str)\n",
    "degibert2019.set_index('id', inplace=True)\n",
    "\n",
    "degibert2019 = degibert2019[['text', 'dataset']] # no timestamp\n",
    "print(len(degibert2019))\n",
    "degibert2019.head()\n",
    "\n",
    "degibert2019['source'] = 'stormfront'\n",
    "\n",
    "data = pd.concat([data, degibert2019])\n",
    "print(len(data))\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa97be9-397d-42ed-b24f-3ff0ecd72a33",
   "metadata": {},
   "source": [
    "## Patriot Front dump (from Unicorn Riot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b136275d-7cd6-4e16-94e2-72c4160cdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_dirpath = '/storage2/mamille3/data/patriotfront/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696ca93-ba67-41a0-a2da-2a08921377d1",
   "metadata": {},
   "source": [
    "### 2017 Vanguard America-Patriot Front Discord dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cde2e60-a3c6-4dfc-87e9-20e815ea3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25133\n"
     ]
    }
   ],
   "source": [
    "# Load dump from xz file\n",
    "# Join channels with messages to be able to select general channel\n",
    "\n",
    "dirpath = os.path.join(base_dirpath, '2017-Vanguard_America-Patriot_Front/Discord/dump')\n",
    "channels = pd.read_csv(os.path.join(dirpath, 'channels.csv'), index_col=0)\n",
    "\n",
    "messages = pd.read_csv(os.path.join(dirpath, 'messages.csv'))\n",
    "messages.dropna(subset='message', inplace=True)\n",
    "# print(len(messages))\n",
    "messages = messages.join(channels, on='channel_id', rsuffix='_channel')\n",
    "# print(len(messages))\n",
    "messages2017 = messages.query('name == \"general\"')\n",
    "print(len(messages2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd2125-72bd-4566-9290-0cada2aa67d1",
   "metadata": {},
   "source": [
    "### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9dcb76-14c1-4984-a63e-4ca70d36c314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front_And_Center\n",
      "MI_Goy_Scouts_Official\n",
      "20997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46130"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages2018 = []\n",
    "for dump in ['Front_And_Center', 'MI_Goy_Scouts_Official']:\n",
    "    print(dump)\n",
    "    dirpath = os.path.join(base_dirpath, '2018/Discord', f'dump_{dump}')\n",
    "    messages = pd.read_csv(os.path.join(dirpath, 'messages.csv'))\n",
    "    channels = pd.read_csv(os.path.join(dirpath, 'channels.csv')).set_index('id')\n",
    "    # Remove messages that are just images\n",
    "    messages.dropna(subset='message', inplace=True)\n",
    "    messages = messages.join(channels, on='channel_id', rsuffix='_channel')\n",
    "    messages2018.append(messages.query('name == \"general\"'))\n",
    "    \n",
    "messages2018 = pd.concat(messages2018)\n",
    "print(len(messages2018))\n",
    "messages2018.columns\n",
    "\n",
    "pf_messages = pd.concat([messages2017, messages2018])\n",
    "len(pf_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698aef1-f323-45e8-8bc6-f17382c28055",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b862e31-81fa-4fb6-a616-ac17463a0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b333f-f76e-4e84-84b5-0d06a6686563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load common first names\n",
    "fpath = '../resources/us_first_names_1990.csv'\n",
    "names = pd.read_csv(fpath, skiprows=[0])\n",
    "names = names.query('Rank <= 300')\n",
    "len(names)\n",
    "\n",
    "common_names = set(names['Name'].str.lower()).union(names['Name.1'].str.lower())\n",
    "print(len(common_names))\n",
    "list(common_names)[:20]\n",
    "\n",
    "# Remove spencer, guy\n",
    "common_names -= {'spencer', 'guy'}\n",
    "\n",
    "len(common_names)\n",
    "\n",
    "# Tokenize, remove common first names (except Spencer, for Richard Spencer)\n",
    "import nltk\n",
    "\n",
    "pf_messages['processed'] = pf_messages['message'].map(lambda x: ' '.join([wd for wd in nltk.word_tokenize(str(x)) if wd not in common_names]).lower())\n",
    "pf_messages['processed']\n",
    "\n",
    "pf_messages['timestamp'] = pd.to_datetime(pf_messages.timestamp, utc=True)\n",
    "\n",
    "patriotfront = pf_messages[['processed', 'timestamp']].reset_index(drop=True).rename(columns={'processed': 'text'})\n",
    "patriotfront['dataset'] = 'patriotfront'\n",
    "patriotfront['source'] = 'discord'\n",
    "patriotfront['id'] = 'patriotfront_' + patriotfront.index.astype(str)\n",
    "patriotfront.set_index('id', inplace=True)\n",
    "patriotfront.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a62ccc7-b6b7-49b8-a6f6-c0f4d39fdf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144577\n",
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qian2018         88069\n",
       "patriotfront     46130\n",
       "elsherief2021     5378\n",
       "degibert2019      5000\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, patriotfront])\n",
    "print(len(data))\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0b353-2c29-48d8-9006-b74424736e42",
   "metadata": {},
   "source": [
    "## Alatawi+2022 annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b481ca5e-fc78-4984-8c08-eaf613346dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qian2018         88069\n",
       "patriotfront     46130\n",
       "elsherief2021     5378\n",
       "degibert2019      5000\n",
       "alatawi2021       1100\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "fpath = '/storage2/mamille3/white_supremacist_lang/data/alatawi2021_white_supremacist_annotated_tweets.csv'\n",
    "tweets = pd.read_csv(fpath)\n",
    "\n",
    "# Count various annotation thresholds for white supremacy\n",
    "agreed_tweets = tweets.query('`Voting and Final Labels` == 1')\n",
    "len(agreed_tweets)\n",
    "\n",
    "alatawi2021 = agreed_tweets[['input.text']].rename(columns={'input.text': 'text'}).reset_index(drop=True) # no timestamp\n",
    "alatawi2021['dataset'] = 'alatawi2021'\n",
    "alatawi2021['source'] = 'twitter'\n",
    "alatawi2021.index = 'alatawi2021_' + alatawi2021.index.astype(str)\n",
    "alatawi2021\n",
    "\n",
    "alatawi2021['text'] = alatawi2021['text'].str.lower()\n",
    "alatawi2021\n",
    "\n",
    "data = pd.concat([data, alatawi2021])\n",
    "print(len(data))\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3862d-3c7b-40f2-93a6-1cefd134dc74",
   "metadata": {},
   "source": [
    "## ADL HEATMap extracted propaganda quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8ef8a6-1ff6-4ddc-abc6-e8ac3b6007d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adl_0</th>\n",
       "      <td>it's okay to be white</td>\n",
       "      <td>2022-03-31 00:00:00+00:00</td>\n",
       "      <td>adl_heatmap</td>\n",
       "      <td>offline_flyers_banners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1</th>\n",
       "      <td>europa - the last battle</td>\n",
       "      <td>2022-03-31 00:00:00+00:00</td>\n",
       "      <td>adl_heatmap</td>\n",
       "      <td>offline_flyers_banners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_2</th>\n",
       "      <td>free ram</td>\n",
       "      <td>2022-03-30 00:00:00+00:00</td>\n",
       "      <td>adl_heatmap</td>\n",
       "      <td>offline_flyers_banners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_3</th>\n",
       "      <td>for the nation against the state</td>\n",
       "      <td>2022-03-30 00:00:00+00:00</td>\n",
       "      <td>adl_heatmap</td>\n",
       "      <td>offline_flyers_banners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_4</th>\n",
       "      <td>reclaim america</td>\n",
       "      <td>2022-03-30 00:00:00+00:00</td>\n",
       "      <td>adl_heatmap</td>\n",
       "      <td>offline_flyers_banners</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text                 timestamp  \\\n",
       "adl_0             it's okay to be white 2022-03-31 00:00:00+00:00   \n",
       "adl_1          europa - the last battle 2022-03-31 00:00:00+00:00   \n",
       "adl_2                          free ram 2022-03-30 00:00:00+00:00   \n",
       "adl_3  for the nation against the state 2022-03-30 00:00:00+00:00   \n",
       "adl_4                   reclaim america 2022-03-30 00:00:00+00:00   \n",
       "\n",
       "           dataset                  source  \n",
       "adl_0  adl_heatmap  offline_flyers_banners  \n",
       "adl_1  adl_heatmap  offline_flyers_banners  \n",
       "adl_2  adl_heatmap  offline_flyers_banners  \n",
       "adl_3  adl_heatmap  offline_flyers_banners  \n",
       "adl_4  adl_heatmap  offline_flyers_banners  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data (quotes extracted by Ahmad Diab)\n",
    "import pandas as pd\n",
    "\n",
    "fpath = '../data/adl_quotes.csv'\n",
    "quotes = pd.read_csv(fpath)\n",
    "quotes.columns\n",
    "\n",
    "quotes['timestamp'] = pd.to_datetime(quotes.date, format='%m/%d/%y', errors='coerce', utc=True).fillna(\n",
    "    pd.to_datetime(quotes.date, format='%y-%b', errors='coerce', utc=True))\n",
    "\n",
    "adl = quotes[['quote', 'timestamp']].drop_duplicates(subset='quote').reset_index(drop=True).rename(columns={'quote': 'text'})\n",
    "adl['dataset'] = 'adl_heatmap'\n",
    "adl['source'] = 'offline_flyers_banners'\n",
    "adl.index = 'adl_' + adl.index.astype(str)\n",
    "adl.head()\n",
    "\n",
    "# data = data.query('dataset != \"adl_heatmap\"')\n",
    "# data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc3b3c1f-6442-49ae-8cb8-93d8e703124c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m adl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m adl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(nltk\u001b[38;5;241m.\u001b[39mword_tokenize(x)))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mad\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ad' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "import nltk\n",
    "\n",
    "adl['text'] = adl['text'].map(lambda x: ' '.join(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa107d2-804e-4ef6-9a2a-8a93326432aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adl_0</th>\n",
       "      <td>it 's okay to be white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1</th>\n",
       "      <td>europa - the last battle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_2</th>\n",
       "      <td>free ram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_3</th>\n",
       "      <td>for the nation against the state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_4</th>\n",
       "      <td>reclaim america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1024</th>\n",
       "      <td>our patience has its limits one day we will sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1025</th>\n",
       "      <td>for ourselves and our posterity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1026</th>\n",
       "      <td>save our land join the klan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1027</th>\n",
       "      <td>join the kkk and fight for race and nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adl_1028</th>\n",
       "      <td>jew media</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text\n",
       "adl_0                                it 's okay to be white\n",
       "adl_1                              europa - the last battle\n",
       "adl_2                                              free ram\n",
       "adl_3                      for the nation against the state\n",
       "adl_4                                       reclaim america\n",
       "...                                                     ...\n",
       "adl_1024  our patience has its limits one day we will sh...\n",
       "adl_1025                    for ourselves and our posterity\n",
       "adl_1026                        save our land join the klan\n",
       "adl_1027         join the kkk and fight for race and nation\n",
       "adl_1028                                          jew media\n",
       "\n",
       "[1029 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adl[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f373d4db-d9c7-4ac8-83a1-d22b67724f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146706\n",
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qian2018         88069\n",
       "patriotfront     46130\n",
       "elsherief2021     5378\n",
       "degibert2019      5000\n",
       "alatawi2021       1100\n",
       "adl_heatmap       1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, adl])\n",
    "print(len(data))\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bd29d-8dfc-48ae-ab39-23b3eb0abfd9",
   "metadata": {},
   "source": [
    "## Iron March data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9b4baa-cb61-4678-a62a-ae02cfa1168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467536f7657d4136ba5be7b0f9424e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ironmarch_0</th>\n",
       "      <td>congrats on 1,488</td>\n",
       "      <td>2017-07-28 16:40:17+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_1</th>\n",
       "      <td>i approve of this avatar .</td>\n",
       "      <td>2017-06-28 06:40:14+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_2</th>\n",
       "      <td>i have more reputation than u , fag</td>\n",
       "      <td>2017-06-22 06:49:22+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_3</th>\n",
       "      <td>hi rostislav , danke für den willkommensgruss ...</td>\n",
       "      <td>2017-06-15 18:10:15+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_4</th>\n",
       "      <td>how to kill time at work ?</td>\n",
       "      <td>2017-06-14 11:27:41+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_196037</th>\n",
       "      <td>based barg</td>\n",
       "      <td>2017-11-20 22:32:14+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_196038</th>\n",
       "      <td>&amp; gt ; tfw you notice that helicopters are fly...</td>\n",
       "      <td>2017-11-20 22:52:23+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_196039</th>\n",
       "      <td>thoughts on anglos ?</td>\n",
       "      <td>2017-11-21 00:58:53+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_196040</th>\n",
       "      <td>i think its hilarious you guys tried to intent...</td>\n",
       "      <td>2017-11-21 02:58:31+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironmarch_196041</th>\n",
       "      <td>the `` read siege '' movement receives a stron...</td>\n",
       "      <td>2017-11-21 03:41:24+00:00</td>\n",
       "      <td>ironmarch</td>\n",
       "      <td>ironmarch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196042 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text  \\\n",
       "ironmarch_0                                       congrats on 1,488   \n",
       "ironmarch_1                              i approve of this avatar .   \n",
       "ironmarch_2                     i have more reputation than u , fag   \n",
       "ironmarch_3       hi rostislav , danke für den willkommensgruss ...   \n",
       "ironmarch_4                              how to kill time at work ?   \n",
       "...                                                             ...   \n",
       "ironmarch_196037                                         based barg   \n",
       "ironmarch_196038  & gt ; tfw you notice that helicopters are fly...   \n",
       "ironmarch_196039                               thoughts on anglos ?   \n",
       "ironmarch_196040  i think its hilarious you guys tried to intent...   \n",
       "ironmarch_196041  the `` read siege '' movement receives a stron...   \n",
       "\n",
       "                                 timestamp    dataset     source  \n",
       "ironmarch_0      2017-07-28 16:40:17+00:00  ironmarch  ironmarch  \n",
       "ironmarch_1      2017-06-28 06:40:14+00:00  ironmarch  ironmarch  \n",
       "ironmarch_2      2017-06-22 06:49:22+00:00  ironmarch  ironmarch  \n",
       "ironmarch_3      2017-06-15 18:10:15+00:00  ironmarch  ironmarch  \n",
       "ironmarch_4      2017-06-14 11:27:41+00:00  ironmarch  ironmarch  \n",
       "...                                    ...        ...        ...  \n",
       "ironmarch_196037 2017-11-20 22:32:14+00:00  ironmarch  ironmarch  \n",
       "ironmarch_196038 2017-11-20 22:52:23+00:00  ironmarch  ironmarch  \n",
       "ironmarch_196039 2017-11-21 00:58:53+00:00  ironmarch  ironmarch  \n",
       "ironmarch_196040 2017-11-21 02:58:31+00:00  ironmarch  ironmarch  \n",
       "ironmarch_196041 2017-11-21 03:41:24+00:00  ironmarch  ironmarch  \n",
       "\n",
       "[196042 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "fpath = '/storage2/mamille3/white_supremacist_lang/data/iron_march_201911/csv/core_search_index.csv'\n",
    "posts = pd.read_csv(fpath)\n",
    "\n",
    "def preprocess(inp):\n",
    "    return ' '.join(nltk.word_tokenize(str(inp))).lower()\n",
    "\n",
    "# Tokenize, lowercase\n",
    "# posts['processed'] = [' '.join(nltk.word_tokenize(str(x))).lower() for x in tqdm(posts['index_content'])]\n",
    "with Pool(15) as p:\n",
    "    posts['processed'] = list(tqdm(p.imap(preprocess, posts['index_content']), total=len(posts)))\n",
    "posts['processed']\n",
    "posts.columns\n",
    "\n",
    "posts['timestamp'] = pd.to_datetime(posts.index_date_created, unit='s', utc=True)\n",
    "\n",
    "ironmarch = posts[['processed', 'timestamp']].reset_index(drop=True).rename(columns={'processed': 'text'})\n",
    "ironmarch['dataset'] = 'ironmarch'\n",
    "ironmarch['source'] = 'ironmarch'\n",
    "ironmarch.index = 'ironmarch_' + ironmarch.index.astype(str)\n",
    "print(ironmarch.timestamp.dtype)\n",
    "ironmarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a0599f-ef2c-42c8-a086-e4dcc2ec7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ironmarch        196042\n",
       "qian2018          88069\n",
       "patriotfront      46130\n",
       "elsherief2021      5378\n",
       "degibert2019       5000\n",
       "alatawi2021        1100\n",
       "adl_heatmap        1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, ironmarch])\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83aad1f-adb1-48fa-a92d-9c972bc4b163",
   "metadata": {},
   "source": [
    "## 4chan datasets (Jokubausaite+2020, Papasavva+2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f99b7d04-e021-4f7b-b045-848641a9415f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17772/3054778316.py:28: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs.append(pd.read_csv(fpath))\n",
      "/tmp/ipykernel_17772/3054778316.py:28: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs.append(pd.read_csv(fpath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684703\n",
      "Index(['thread_id', 'id', 'timestamp', 'body', 'subject', 'author',\n",
      "       'image_file', 'image_md5', 'country_code', 'country_name',\n",
      "       'unix_timestamp'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17772/3054778316.py:42: DtypeWarning: Columns (36,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  papasavva2020_posts = pd.read_csv(fpath,index_col=0).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068516\n",
      "Index(['archived_on', 'replies', 'images', 'archived', 'no', 'fsize',\n",
      "       'filename', 'tim', 'troll_country', 'closed', 'country_name',\n",
      "       'bumplimit', 'extracted_poster_id', 'time', 'imagelimit',\n",
      "       'semantic_url', 'now', 'md5', 'name', 'tn_w', 'h', 'ext', 'resto', 'w',\n",
      "       'tn_h', 'com', 'entities', 'perspectives.TOXICITY',\n",
      "       'perspectives.SEVERE_TOXICITY', 'perspectives.INFLAMMATORY',\n",
      "       'perspectives.PROFANITY', 'perspectives.INSULT', 'perspectives.OBSCENE',\n",
      "       'perspectives.SPAM', 'entitites', 'perspectives', 'm_img', 'tail_size',\n",
      "       'sub', 'trip', 'filedeleted', 'since4pass', 'id', 'unique_ips', 'xa18',\n",
      "       'xa19l', 'xa19s'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e92951544994e2cb7dee391b6adc415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4chan_0</th>\n",
       "      <td>germany 's situation is that of total despair ...</td>\n",
       "      <td>2017-03-09 14:11:59+00:00</td>\n",
       "      <td>4chan</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4chan_1</th>\n",
       "      <td>haut die glatzen bis sie platzen</td>\n",
       "      <td>2017-03-09 14:13:53+00:00</td>\n",
       "      <td>4chan</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4chan_2</th>\n",
       "      <td>how the politics works in krautland ? there is...</td>\n",
       "      <td>2017-03-09 14:19:38+00:00</td>\n",
       "      <td>4chan</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4chan_3</th>\n",
       "      <td>godspeed krautbros</td>\n",
       "      <td>2017-03-09 14:22:26+00:00</td>\n",
       "      <td>4chan</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4chan_4</th>\n",
       "      <td>the system is terribly rigged pro political pa...</td>\n",
       "      <td>2017-03-09 14:23:36+00:00</td>\n",
       "      <td>4chan</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "4chan_0  germany 's situation is that of total despair ...   \n",
       "4chan_1                   haut die glatzen bis sie platzen   \n",
       "4chan_2  how the politics works in krautland ? there is...   \n",
       "4chan_3                                 godspeed krautbros   \n",
       "4chan_4  the system is terribly rigged pro political pa...   \n",
       "\n",
       "                        timestamp dataset source  \n",
       "4chan_0 2017-03-09 14:11:59+00:00   4chan  4chan  \n",
       "4chan_1 2017-03-09 14:13:53+00:00   4chan  4chan  \n",
       "4chan_2 2017-03-09 14:19:38+00:00   4chan  4chan  \n",
       "4chan_3 2017-03-09 14:22:26+00:00   4chan  4chan  \n",
       "4chan_4 2017-03-09 14:23:36+00:00   4chan  4chan  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Jokubausaite+2020 data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dirpath = '/storage2/mamille3/data/4chan/jokubausaite2020/'\n",
    "\n",
    "selected = [\n",
    "    # 'president trump', # too focused just on Trump\n",
    "    # 'trump', # too focused just on Trump\n",
    "    'kraut/pol/ and afd',\n",
    "    'national socialism',\n",
    "    # 'islam', # Super Islamophobic and antisemitic but not necessarily white supremacist ideology\n",
    "    'fascism',\n",
    "    'dixie',\n",
    "    # 'hinduism', # Not super white supremacist, though some antisemitism\n",
    "    # 'black nationalism', # super racist and white nationalist, but some actual Black nationalism\n",
    "    'kraut/pol/', # yep, German nationalists. Some German, but lots of white supremacy\n",
    "    'ethnostate',\n",
    "    'white',\n",
    "    'chimpout',\n",
    "    'feminist apocalypse',\n",
    "    '(((krautgate)))',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for general in selected:\n",
    "    fpath = os.path.join(dirpath, f'{general.replace(\"/\", \" \")} general.csv')\n",
    "    dfs.append(pd.read_csv(fpath))\n",
    "\n",
    "jokubausaite2020_posts = pd.concat(dfs)\n",
    "jokubausaite2020_posts.reset_index(drop=True)\n",
    "print(len(jokubausaite2020_posts))\n",
    "print(jokubausaite2020_posts.columns)\n",
    "\n",
    "jokubausaite2020_posts['timestamp'] = pd.to_datetime(jokubausaite2020_posts.timestamp, utc=True)\n",
    "jokubausaite2020_posts['timestamp']\n",
    "\n",
    "# Load Papasavva+2020 data with selected flags\n",
    "import pandas as pd\n",
    "\n",
    "fpath = '/storage2/mamille3/white_supremacist_lang/data/papasavva2020_white_supremacist_flag_posts.csv'\n",
    "papasavva2020_posts = pd.read_csv(fpath,index_col=0).reset_index(drop=True)\n",
    "print(len(papasavva2020_posts))\n",
    "print(papasavva2020_posts.columns)\n",
    "\n",
    "papasavva2020_posts['timestamp'] = pd.to_datetime(papasavva2020_posts.time, unit='s', utc=True)\n",
    "\n",
    "# Merge, remove duplicates, get text\n",
    "j = jokubausaite2020_posts[['id', 'body', 'timestamp']]\n",
    "p = papasavva2020_posts.drop(columns='id').rename(columns={'no': 'id', 'com': 'body'})[['id', 'body', 'timestamp']]\n",
    "fourchan = pd.concat([j, p]).drop_duplicates('id').dropna(subset='body')\n",
    "len(fourchan)\n",
    "\n",
    "# Process text (remove HTML, tokenize)\n",
    "from html.parser import HTMLParser\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def remove_special(text):\n",
    "    text = text.replace('>', '')\n",
    "    text = re.sub(r'\\d{7,}', '', text)\n",
    "    text = re.sub(r'\\S+(?:\\.com|\\.org|\\.edu)\\S*|https?:\\/\\/\\S*', '', text) # Remove URLs\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    # Remove HTML\n",
    "    text = strip_tags(str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove special characters\n",
    "    text = remove_special(text)\n",
    "    # Tokenize\n",
    "    text = ' '.join(nltk.word_tokenize(str(text))).lower()\n",
    "    return text\n",
    "\n",
    "def process_chunk(texts):\n",
    "    return [process_text(text) for text in texts]\n",
    "\n",
    "n_processes = 20\n",
    "p = Pool(n_processes)\n",
    "chunks = np.array_split(fourchan.body, n_processes)\n",
    "res = list(tqdm(p.imap(process_chunk, chunks), total=len(chunks)))\n",
    "# res = list(map(process_chunk, chunks))\n",
    "    \n",
    "# fourchan['processed'] = [process_text(text) for text in tqdm(fourchan.body)]\n",
    "fourchan['processed'] = [processed for processed_texts in res for processed in processed_texts]\n",
    "\n",
    "chan = fourchan[['processed', 'timestamp']].reset_index(drop=True).rename(columns={'processed': 'text'})\n",
    "chan['dataset'] = '4chan'\n",
    "chan['source'] = '4chan'\n",
    "chan.index = '4chan_' + chan.index.astype(str)\n",
    "print(chan.timestamp.dtype)\n",
    "chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060e3803-5eef-4ca7-a9d6-cbfc0d6a3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4chan            3675508\n",
       "ironmarch         196042\n",
       "qian2018           88069\n",
       "patriotfront       46130\n",
       "elsherief2021       5378\n",
       "degibert2019        5000\n",
       "alatawi2021         1100\n",
       "adl_heatmap         1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, chan])\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7dcd9-a4bf-4cdb-8d96-244fc2c7f71a",
   "metadata": {},
   "source": [
    "## Stormfront archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "254c12f8-c84d-4522-a5df-6b231255ef4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f180b05f842fdbfc6ef4806fc5442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53abd9e9802c4d92808ff6da313e07e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/762585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "datetime64[ns, UTC]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "dirpath = '/storage2/mamille3/white_supremacist_lang/data/stormfront_archive/processed/'\n",
    "dfs = []\n",
    "for fname in tqdm(os.listdir(dirpath)):\n",
    "    fpath = os.path.join(dirpath, fname)\n",
    "    dfs.append(pd.read_csv(fpath))\n",
    "posts = pd.concat(dfs).reset_index(drop=True) \n",
    "\n",
    "# Split up breadcrumb\n",
    "posts[[f'breadcrumb{i}' for i in range(5)]] = posts.thread_breadcrumb.str.split(' > ', expand=True)\n",
    "\n",
    "# Try to remove non-English posts\n",
    "exclude = ['Nederland & Vlaanderen', \n",
    "            'Srbija',\n",
    "            'en Español y Portugués',\n",
    "            'Italia',\n",
    "            'Croatia',\n",
    "            'South Africa', # some Boer/Dutch\n",
    "            'en Français',\n",
    "            'Russia',\n",
    "            'Baltic / Scandinavia', # but contains lots of English\n",
    "            'Hungary', # but contains lots of English\n",
    "            'Opposing Views Forum',\n",
    "           'Computer Talks'\n",
    "           ]\n",
    "\n",
    "formatted = [f'Stormfront {el}' for el in exclude]\n",
    "posts = posts.query('breadcrumb2!=@formatted').dropna(subset='text')\n",
    "print(len(posts))\n",
    "posts.columns\n",
    "\n",
    "# Tokenize and prepare\n",
    "import nltk\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def preprocess(inp):\n",
    "    text = re.sub(r'Quote:\\n\\n\\n\\n\\nOriginally Posted by .*\\n\\n\\n', '', inp) # Remove quote tag\n",
    "    text = re.sub(r'\\S+(?:\\.com|\\.org|\\.edu)\\S*|https?:\\/\\/\\S*', '', text) # Remove URLs\n",
    "    text = ' '.join(nltk.word_tokenize(str(text))).lower()\n",
    "    return text\n",
    "\n",
    "# posts['processed'] = [' '.join(nltk.word_tokenize(str(x))).lower() for x in tqdm(posts['text'])]\n",
    "with Pool(20) as p:\n",
    "    posts['processed'] = list(tqdm(p.imap(preprocess, posts['text']), total=len(posts)))\n",
    "\n",
    "posts['timestamp'] = pd.to_datetime(posts.timestamp, errors='coerce', utc=True)\n",
    "posts.timestamp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f610e7a-e148-4c86-b0c2-6682e885efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4chan            3675508\n",
       "stormfront        762585\n",
       "ironmarch         196042\n",
       "qian2018           88069\n",
       "patriotfront       46130\n",
       "elsherief2021       5378\n",
       "degibert2019        5000\n",
       "alatawi2021         1100\n",
       "adl_heatmap         1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stormfront = posts[['processed', 'timestamp']].reset_index(drop=True).rename(columns={'processed': 'text'})\n",
    "stormfront['dataset'] = 'stormfront'\n",
    "stormfront['source'] = 'stormfront'\n",
    "stormfront.index = 'stormfront_' + stormfront.index.astype(str)\n",
    "stormfront.head()\n",
    "\n",
    "data = pd.concat([data, stormfront])\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedee77a-615e-4753-bbc3-b7d24808cee0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calderón+2021 DailyStormer and American Renaissance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "573b4cc6-8426-44e1-abdc-664f705fe6c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc524936caf4c14bb73c5dc21527a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dirpath = '/storage2/mamille3/white_supremacist_lang/data/calderon2021/'\n",
    "dailystormer_fpath = os.path.join(dirpath, 'd_stormer_documents.json')\n",
    "\n",
    "with open(dailystormer_fpath) as f:\n",
    "    ds_docs = json.load(f)\n",
    "len(ds_docs)\n",
    "\n",
    "dstormer = pd.json_normalize(ds_docs)\n",
    "dstormer['source'] = 'daily_stormer'\n",
    "\n",
    "# Load American Renaissance\n",
    "amren_fpath = os.path.join(dirpath, 'amran_documents.json')\n",
    "\n",
    "with open(amren_fpath) as f:\n",
    "    amren_json = json.load(f)\n",
    "len(amren_json)\n",
    "\n",
    "amren = pd.json_normalize(amren_json)\n",
    "amren['source'] = 'american_renaissance'\n",
    "\n",
    "# articles = pd.concat([dstormer[['date', 'title', 'author_wording']], amren[['date', 'title', 'author_wording']]], \n",
    "#                      keys=['daily_stormer', 'american_renaissance'], \n",
    "#                      names=['dataset', 'old_index']).reset_index(level='dataset').reset_index(drop=True)\n",
    "articles = pd.concat([dstormer[['date', 'title', 'author_wording', 'source']], amren[['date', 'title', 'author_wording', 'source']]]).reset_index(drop=True)\n",
    "print(len(articles))\n",
    "articles.columns\n",
    "\n",
    "# Tokenize and prepare\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "# import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def preprocess(inp):\n",
    "    # text = re.sub(r'Quote:\\n\\n\\n\\n\\nOriginally Posted by .*\\n\\n\\n', '', inp) # Remove quote tag\n",
    "    # text = re.sub(r'\\S+(?:\\.com|\\.org|\\.edu)\\S*|https?:\\/\\/\\S*', '', text) # Remove URLs\n",
    "    # text = re.sub(r'[A-Za-z]\\.[A-Za-z]', \n",
    "    text = ' '.join(nltk.word_tokenize(str(inp.replace('.', '. ')))).lower()\n",
    "    return text\n",
    "\n",
    "# articles['text'] = [' '.join(nltk.word_tokenize(str(x))).lower() for x in tqdm(articles['author_wording'])]\n",
    "with Pool(20) as p:\n",
    "    articles['text'] = list(tqdm(p.imap(preprocess, articles['title'] + ' ' + articles['author_wording']), total=len(articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "756d4045-1e0c-4c6a-af4e-19ee5b0002b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    7564\n",
       "2016    7448\n",
       "2015    5694\n",
       "2013    2783\n",
       "2017    2490\n",
       "1912     243\n",
       "2012      13\n",
       "2104       4\n",
       "2105       3\n",
       "2011       2\n",
       "2103       2\n",
       "2010       2\n",
       "2005       1\n",
       "2915       1\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['date'].str.slice(0,4).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e94d03-1ffb-4b5c-aec0-6dd152a036c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.loc[~articles.date.str.startswith('20'), 'date'] = '' # remove date errors. Could extract real date by parsing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28670720-f603-4492-851b-08fde3475fd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "calderon2021 = articles.drop(columns=['author_wording', 'title']).rename(columns={'date': 'timestamp'})\n",
    "calderon2021['timestamp'] = pd.to_datetime(calderon2021.timestamp, errors='coerce', utc=True)\n",
    "# calderon2021['dataset'] =  'calderon2021_' + calderon2021['dataset']\n",
    "calderon2021['dataset'] =  'calderon2021'\n",
    "calderon2021.index = 'calderon2021_' + calderon2021.index.astype(str)\n",
    "print(calderon2021.timestamp.dtype)\n",
    "calderon2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee0ed48-1b81-4f01-8ee5-af578c76ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.dataset != 'calderon2021']\n",
    "# data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11bc3f69-e53b-4b9b-95b2-60672c81234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4chan            3675508\n",
       "stormfront        762585\n",
       "ironmarch         196042\n",
       "qian2018           88069\n",
       "patriotfront       46130\n",
       "calderon2021       26250\n",
       "elsherief2021       5378\n",
       "degibert2019        5000\n",
       "alatawi2021         1100\n",
       "adl_heatmap         1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, calderon2021])\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04d3f6-652b-4104-ace0-b5ed754bc84b",
   "metadata": {},
   "source": [
    "## Pruden+2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c2a6-75df-47f9-8433-f574584a7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "source_info = {\n",
    "    'AndersBehringBreivikManifesto': {'year': 2011, 'source': 'breivik_manifesto'},\n",
    "    'Enoch-Powells-Rivers-of-Blood-new-analysis-pdf': {'year': 1968, 'source': 'enoch_powell_rivers_of_blood_speech'},\n",
    "    'Jean-Raspail-Camp-of-the-Saints': {'year': 1973, 'source': 'raspail_camp_of_the_saints_book'},\n",
    "    'Lane_White Genocide Manifesto': {'year': 1988, 'source': 'lane_white_genocide_manifesto'},\n",
    "    'Renaud Camus - The Great Replacement - Part I-RWTS (2012)': {'year': 2012, 'source': 'camus_the_great_replacement_book'},\n",
    "    'Turner - The Turner Diaries': {'year': 1978, 'source': 'pierce_the_turner_diaries_book'},\n",
    "}\n",
    "    \n",
    "dfs = []\n",
    "dirpath = '/storage2/mamille3/white_supremacist_lang/data/pruden2022/'\n",
    "for fname in sorted(os.listdir(dirpath)):\n",
    "    print(fname)\n",
    "    fpath = os.path.join(dirpath, fname)\n",
    "    if fname == 'AndersBehringBreivikManifesto.txt':\n",
    "        with open(fpath, encoding='latin-1') as f:\n",
    "            text = [line.strip() for line in re.split(r'\\n\\s+', f.read()) if len(line.strip()) > 0]\n",
    "        print(len(text))\n",
    "    else:\n",
    "        with open(fpath, encoding='latin-1') as f:\n",
    "            text = [line.strip() for line in f.read().splitlines() if len(line.strip()) > 0]\n",
    "        print(len(text))\n",
    "    df = pd.DataFrame({'title': fname.split('.txt')[0], 'text': text})\n",
    "    df['year'] = df.title.map(lambda x: source_info[x]['year'])\n",
    "    df['source'] = df.title.map(lambda x: source_info[x]['source'])\n",
    "    df['timestamp'] = pd.to_datetime(df.year, format='%Y', utc=True)\n",
    "    dfs.append(df)\n",
    "texts =  pd.concat(dfs)\n",
    "\n",
    "# Preprocess\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "\n",
    "texts['processed'] = [' '.join(nltk.word_tokenize(str(x))).lower() for x in tqdm(texts['text'])]\n",
    "\n",
    "pruden2022 = texts[['processed', 'timestamp', 'source']].reset_index(drop=True).rename(columns={'processed': 'text'})\n",
    "pruden2022['dataset'] = 'pruden2022'\n",
    "pruden2022.index = 'pruden2022_' + pruden2022.index.astype(str)\n",
    "print(pruden2022.timestamp.dtype)\n",
    "pruden2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c1e47f8-ec52-4fb5-8ef5-92889c5d0fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4chan            3675508\n",
       "stormfront        762585\n",
       "ironmarch         196042\n",
       "qian2018           88069\n",
       "patriotfront       46130\n",
       "calderon2021       26250\n",
       "pruden2022         21942\n",
       "elsherief2021       5378\n",
       "degibert2019        5000\n",
       "alatawi2021         1100\n",
       "adl_heatmap         1029\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, pruden2022])\n",
    "print(data.timestamp.dtype)\n",
    "data.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e248b3-49f1-4c3c-ac6e-f9b2850ae3d5",
   "metadata": {},
   "source": [
    "# Save out/load from tmp (just for speed of stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d367493-eae3-409a-85bf-fe6a457d33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join domain info, save out\n",
    "dataset_info = pd.DataFrame([\n",
    "    {'dataset': 'qian2018', 'domain': 'tweet/short propaganda'},\n",
    "    {'dataset': 'elsherief2021', 'domain': 'tweet/short propaganda'},\n",
    "    {'dataset': 'degibert2019', 'domain': 'forum'},\n",
    "    {'dataset': 'patriotfront', 'domain': 'chat'},\n",
    "    {'dataset': 'alatawi2021', 'domain': 'tweet/short propaganda'},\n",
    "    {'dataset': 'adl_heatmap', 'domain': 'tweet/short propaganda'},\n",
    "    {'dataset': 'ironmarch', 'domain': 'forum'},\n",
    "    {'dataset': '4chan', 'domain': 'forum'},\n",
    "    {'dataset': 'stormfront', 'domain': 'forum'},\n",
    "    {'dataset': 'calderon2021', 'domain': 'long-form'},\n",
    "    {'dataset': 'pruden2022', 'domain': 'long-form'},\n",
    "]).set_index('dataset')\n",
    "dataset_info\n",
    "data = data.join(dataset_info, on='dataset')\n",
    "\n",
    "outpath = '../tmp/white_supremacist_corpus.pkl'\n",
    "data.to_pickle(outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
