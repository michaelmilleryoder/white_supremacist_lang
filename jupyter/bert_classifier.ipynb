{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414ceb45-5e92-490b-9f10-ceae11b2abc5",
   "metadata": {},
   "source": [
    "# Test GPU functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5f0026-02e9-4b14-9fb8-984f04b6f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a5ac6-20a7-4b7f-9da5-b5071b81ff11",
   "metadata": {},
   "source": [
    "# Format input data\n",
    "Put it in a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853e7e10-5ce0-46b3-80f9-e4f7426d1fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4850296 entries, qian2018_0 to pruden2022_161\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Dtype              \n",
      "---  ------     -----              \n",
      " 0   text       object             \n",
      " 1   timestamp  datetime64[ns, UTC]\n",
      " 2   dataset    object             \n",
      " 3   source     object             \n",
      " 4   domain     object             \n",
      " 5   label      int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(4)\n",
      "memory usage: 259.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4374274 entries, reddit_match_0 to twitter_match_89106\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Dtype              \n",
      "---  ------      -----              \n",
      " 0   text        object             \n",
      " 1   timestamp   datetime64[ns, UTC]\n",
      " 2   dataset     object             \n",
      " 3   source      object             \n",
      " 4   domain      object             \n",
      " 5   word_count  int64              \n",
      " 6   label       int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(4)\n",
      "memory usage: 267.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9224570 entries, qian2018_0 to twitter_match_89106\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      " 1   label   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 211.1+ MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cb55aab1bd46cc9ebcbdeaf21c4971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8303 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629364bc3a5b4a6285a7000ed996a924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Load white supremacist data\n",
    "import pandas as pd\n",
    "\n",
    "path = '../tmp/white_supremacist_train_corpus.pkl'\n",
    "ws_data = pd.read_pickle(path).assign(label=1)\n",
    "ws_data.info()\n",
    "\n",
    "# Load neutral data\n",
    "path = '../tmp/neutral_train_corpus.pkl'\n",
    "neutral_data = pd.read_pickle(path).assign(label=0)\n",
    "neutral_data.info()\n",
    "\n",
    "# Combine, shuffle and sample if desired\n",
    "selected_cols = ['text', 'label']\n",
    "data = pd.concat([ws_data[selected_cols], neutral_data[selected_cols]])\n",
    "data.info()\n",
    "\n",
    "# Make a HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(data).train_test_split(test_size=0.1)\n",
    "dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_data = dataset.map(preprocess, batched=True)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467387c-620f-4d89-89e1-1b5199645bdc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a0003-75d0-41b3-8994-fae739b862f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/mamille3/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/mamille3/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8302113\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 778326\n",
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394543' max='778326' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394543/778326 15:27:51 < 15:02:33, 7.09 it/s, Epoch 1.52/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.279200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-160000\n",
      "Configuration saved in ./results/checkpoint-160000/config.json\n",
      "Model weights saved in ./results/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-160000/special_tokens_map.json\n",
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-320000\n",
      "Configuration saved in ./results/checkpoint-320000/config.json\n",
      "Model weights saved in ./results/checkpoint-320000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-320000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-320000/special_tokens_map.json\n",
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metrics = {'accuracy': load_metric('accuracy'), \n",
    "           'f1': load_metric('f1')}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {metric_name: metric.compute(predictions=predictions, references=labels) for metric_name, metric in metrics.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"results/checkpoint-160\")\n",
    "\n",
    "batch_size = 16\n",
    "checkpoint = batch_size * int(1e4)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=checkpoint,\n",
    "    # eval_steps=checkpoint,\n",
    "    save_steps=checkpoint,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8bd612-1169-47b1-9703-c49ff4e4b99b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 922457\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/white_supremacist_lang/conda_env/lib/python3.9/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m   2751\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2752\u001b[0m     eval_dataset: Optional[Dataset] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2753\u001b[0m     ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2754\u001b[0m     metric_key_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2755\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m   2756\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2757\u001b[0m \u001b[38;5;124;03m    Run evaluation and returns metrics.\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m \n\u001b[1;32m   2759\u001b[0m \u001b[38;5;124;03m    The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;124;03m    (pass it to the init `compute_metrics` argument).\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \n\u001b[1;32m   2762\u001b[0m \u001b[38;5;124;03m    You can also subclass and override this method to inject custom behavior.\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \n\u001b[1;32m   2764\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m \u001b[38;5;124;03m        eval_dataset (`Dataset`, *optional*):\u001b[39;00m\n\u001b[1;32m   2766\u001b[0m \u001b[38;5;124;03m            Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\u001b[39;00m\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;124;03m            not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\u001b[39;00m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;124;03m            method.\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;124;03m        ignore_keys (`Lst[str]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;124;03m            A list of keys in the output of your model (if it is a dictionary) that should be ignored when\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;124;03m            gathering predictions.\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;124;03m        metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\u001b[39;00m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;124;03m            An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;124;03m            \"eval_bleu\" if the prefix is \"eval\" (default)\u001b[39;00m\n\u001b[1;32m   2775\u001b[0m \n\u001b[1;32m   2776\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;124;03m        A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\u001b[39;00m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;124;03m        dictionary also contains the epoch number which comes from the training state.\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/white_supremacist_lang/conda_env/lib/python3.9/site-packages/transformers/trainer.py:2959\u001b[0m, in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2957\u001b[0m observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   2958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2959\u001b[0m     observed_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;66;03m# For batch samplers, batch_size is not known by the dataloader in advance.\u001b[39;00m\n\u001b[1;32m   2961\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/white_supremacist_lang/conda_env/lib/python3.9/site-packages/transformers/trainer.py:3093\u001b[0m, in \u001b[0;36m_pad_across_processes\u001b[0;34m(self, tensor, pad_index)\u001b[0m\n\u001b[1;32m   3089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nested_gather\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3090\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \u001b[38;5;124;03m    Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\u001b[39;00m\n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m    concatenating them to `gathered`\u001b[39;00m\n\u001b[0;32m-> 3093\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efef63-852f-42e0-ae46-ac6ae8633eaf",
   "metadata": {},
   "source": [
    "# Evaluate on unseen test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9062d947-76c0-4747-8566-30a146f3e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1999 entries, alatawi2021_0 to alatawi2021_1998\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     1999 non-null   object\n",
      " 1   dataset  1999 non-null   object\n",
      " 2   source   1999 non-null   object\n",
      " 3   domain   1999 non-null   object\n",
      " 4   label    1999 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 93.7+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d639cef21841b29189b988d27ed1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = '../tmp/annotated_test_corpus.pkl'\n",
    "annotated = pd.read_pickle(path)\n",
    "annotated.info()\n",
    "\n",
    "alatawi2021 = annotated.query('dataset==\"alatawi2021\"')\n",
    "test_dataset = Dataset.from_pandas(alatawi2021)\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0a84af-5004-4fd0-9622-5e4eb1057343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, domain, text, dataset, source. If id, domain, text, dataset, source are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 32\n",
      "/home/mamille3/white_supremacist_lang/conda_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0954207181930542,\n",
       " 'eval_accuracy': {'accuracy': 0.5597798899449725},\n",
       " 'eval_f1': {'f1': 0.7018970189701897}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169d14d-af44-4978-8e07-c522d57e8368",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old/1-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1605169b-4dd7-4d2f-bd94-7016d05d6a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85 s ± 23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Load white supremacist data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = '../tmp/white_supremacist_train_corpus.pkl'\n",
    "data = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c488edba-06d8-4650-9cda-0b05970448fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.2 s ± 28.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Load white supremacist data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = '../data/white_supremacist_train_corpus.json'\n",
    "data = pd.read_json(path, orient='table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
