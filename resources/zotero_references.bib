
@article{kiesling_cl_2018,
	title = {Interactional {Stancetaking} in {Online} {Forums}},
	volume = {44},
	issn = {01272713},
	abstract = {This paper evaluates the investment performance of Malaysian-based international equity funds. The results on the overall fund performance using Jensen's (1968) model indicate that, on average, international funds have significant negative risk-adjusted returns over the study period from 2008-2010. Since the model ignores market timing activity, it implicitly attributes the overall negative return to manager's poor stock selection ability. However, the performance breakdown results on managerial expertise using the models of Treynor and Mazuy (1966) and Henriksson and Merton (1981) show evidence of positive selectivity and negative market timing returns. Taken together, the highly significant negative timing returns suggest that, on average, international fund managers have perverse market timing ability. The paper finds little evidence that Malaysian investors achieve diversification benefits from investing in overseas equity markets.},
	number = {4},
	journal = {Computational Linguistics},
	author = {Kiesling, Scott F. and Pavalanathan, Umashanthi and Fitzpatrick, Jim and Han, Xiaochuang and Eisenstein, Jacob},
	year = {2018},
	note = {ISBN: 9781608459858},
	keywords = {Fund performance, International equity funds, Market timing, Security selection},
	pages = {638--718},
}

@incollection{johnstone_discourse_medium,
	title = {Chapter 7: {Discourse} and {Medium}},
	booktitle = {Discourse {Analysis}},
	author = {Johnstone, Barbara},
}

@inproceedings{wiegand_naacl2021,
	title = {Implicitly {Abusive} {Language}-{What} does it actually look like and why are we not getting there?},
	url = {http://thelawdictionary.org/abusive-language},
	abstract = {Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs to be changed in the design of such datasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of {theAssociation} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Wiegand, Michael and Ruppenhofer, Josef and Eder, Elisabeth},
	year = {2021},
	pages = {576--587},
}

@article{Brown2017part2,
	title = {What is hate speech? {Part} 2: {Family} resemblances},
	volume = {36},
	issn = {15730522},
	doi = {10.1007/s10982-017-9300-x},
	abstract = {The issue of hate speech has received significant attention from legal scholars and philosophers alike. But the vast majority of this attention has been focused on presenting and critically evaluating arguments for and against hate speech bans as opposed to the prior task of conceptually analysing the term ‘hate speech’ itself. This two-part article aims to put right that imbalance. It goes beyond legal texts and judgements and beyond the legal concept hate speech in an attempt to understand the general concept hate speech. And it does so using a range of wellknown methods of conceptual analysis that are distinctive of analytic philosophy. One of its main aims is to explode the myth that emotions, feelings, or attitudes of hate or hatred are part of the essential nature of hate speech. It also argues that hate speech is best conceived as a family resemblances concept. One important implication is that when looking at the full range of ways of combating hate speech, including but not limited to the use of criminal law, there is every reason to embrace an understanding of hate speech as a heterogeneous collection of expressive phenomena. Another is that it would be unsound to reject hate speech laws on the premise that they are effectively in the business of criminalising emotions, feelings, or attitudes of hate or hatred.},
	number = {5},
	urldate = {2022-07-06},
	journal = {Law and Philosophy},
	author = {Brown, Alexander},
	month = oct,
	year = {2017},
	note = {Publisher: Springer Netherlands},
	pages = {561--613},
}

@article{Brown2017part1,
	title = {What is hate speech? {Part} 1: {The} {Myth} of {Hate}},
	volume = {36},
	issn = {15730522},
	doi = {10.1007/s10982-017-9297-1},
	abstract = {The issue of hate speech has received significant attention from legal scholars and philosophers alike. But the vast majority of this attention has been focused on presenting and critically evaluating arguments for and against hate speech bans as opposed to the prior task of conceptually analysing the term ‘hate speech’ itself. This two-part article aims to put right that imbalance. It goes beyond legal texts and judgements and beyond the legal concept hate speech in an attempt to understand the general concept hate speech. And it does so using a range of well-known methods of conceptual analysis that are distinctive of analytic philosophy. One of its main aims is to explode the myth that emotions, feelings, or attitudes of hate or hatred are part of the essential nature of hate speech. It also argues that hate speech is best conceived as a family resemblances concept. One important implication is that when looking at the full range of ways of combating hate speech, including but not limited to the use of criminal law, there is every reason to embrace an understanding of hate speech as a heterogeneous collection of expressive phenomena. Another is that it would be unsound to reject hate speech laws on the premise that they are effectively in the business of criminalising emotions, feelings, or attitudes of hate or hatred.},
	number = {4},
	journal = {Law and Philosophy},
	author = {Brown, Alexander},
	month = aug,
	year = {2017},
	note = {Publisher: Springer Netherlands},
	pages = {419--468},
}

@article{KoppelLREC2011,
	title = {Authorship attribution in the wild},
	volume = {45},
	issn = {1574020X},
	doi = {10.1007/s10579-009-9111-2},
	abstract = {Most previous work on authorship attribution has focused on the case{\textbackslash}rin which we need to attribute an anonymous document to one of a small set of{\textbackslash}rcandidate authors. In this paper, we consider authorship attribution as found in the{\textbackslash}rwild: the set of known candidates is extremely large (possibly many thousands) and{\textbackslash}r{\textbackslash}nmight not even include the actual author. Moreover, the known texts and the{\textbackslash}ranonymous texts might be of limited length. We show that even in these difficult{\textbackslash}rcases, we can use similarity-based methods along with multiple randomized feature{\textbackslash}rsets to achieve high precision. Moreover, we show the precise relationship between{\textbackslash}r{\textbackslash}nattribution precision and four parameters: the size of the candidate set, the quantity{\textbackslash}r{\textbackslash}nof known-text by the candidates, the length of the anonymous text and a certain{\textbackslash}r{\textbackslash}nrobustness score associated with a attribution.},
	number = {1},
	journal = {Language Resources and Evaluation},
	author = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	year = {2011},
	note = {ISBN: 1574-0218},
	keywords = {Authorship attribution, Open candidate set, Randomized feature set},
	pages = {83--94},
}

@article{SchwartzPLoS2013,
	title = {Personality, {Gender}, and {Age} in the {Language} of {Social} {Media}: {The} {Open}-{Vocabulary} {Approach}},
	volume = {8},
	issn = {19326203},
	doi = {10.1371/journal.pone.0073791},
	abstract = {We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our open-vocabulary technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase 'sick of' and the word 'depressed'), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive 'my' when mentioning their 'wife' or 'girlfriend' more often than females use 'my' with 'husband' or 'boyfriend'). To date, this represents the largest study, by an order of magnitude, of language and personality.},
	number = {9},
	journal = {PLoS ONE},
	author = {Schwartz, H. Andrew and Eichstaedt, Johannes C. and Kern, Margaret L. and Dziurzynski, Lukasz and Ramones, Stephanie M. and Agrawal, Megha and Shah, Achal and Kosinski, Michal and Stillwell, David and Seligman, Martin E.P. and Ungar, Lyle H.},
	year = {2013},
	pmid = {24086296},
	note = {arXiv: 1690219.1690245‎
ISBN: 1932-6203},
}

@article{BleiJordan2003,
	title = {Modeling annotated data},
	issn = {15312291},
	url = {http://portal.acm.org/citation.cfm?doid=860435.860460},
	doi = {10.1145/860435.860460},
	abstract = {We consider the problem of modeling annotated data---data with multiple{\textbackslash}ntypes where the instance of one type (such as a caption) serves as{\textbackslash}na description of the other type (such as an image). We describe three{\textbackslash}nhierarchical probabilistic mixture models which aim to describe such{\textbackslash}ndata, culminating in correspondence latent Dirichlet allocation,{\textbackslash}na latent variable model that is effective at modeling the joint distribution{\textbackslash}nof both types and the conditional distribution of the annotation{\textbackslash}ngiven the primary type. We conduct experiments on the Corel database{\textbackslash}nof images and captions, assessing performance in terms of held-out{\textbackslash}nlikelihood, automatic annotation, and text-based image retrieval.},
	journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval  - SIGIR '03},
	author = {Blei, David M. and Jordan, Michael I.},
	year = {2003},
	pmid = {20502217},
	note = {ISBN: 1581136463},
	keywords = {automatic image annotation, empirical bayes, image retrieval, methods, probabilistic graphical models, variational},
	pages = {127},
}

@article{BammanGender2014,
	title = {Gender and variation in social media},
	volume = {18},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/josl.12080/abstract},
	doi = {10.1111/josl.12080/abstract},
	abstract = {We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections, and that in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms. [206 words]},
	number = {2},
	journal = {Journal of Sociolinguistics},
	author = {Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler},
	year = {2014},
	keywords = {computational methods, computer-mediated communication, gender},
	pages = {1--46},
}

@incollection{brown_case_2018,
	title = {Case \#1: {African} {Diasporic} {Dialogue}},
	booktitle = {English and {Emprie}},
	author = {Brown, David},
	year = {2018},
}

@article{kim_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	issn = {10709908},
	url = {http://arxiv.org/abs/1408.5882},
	doi = {10.3115/v1/D14-1181},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	author = {Kim, Yoon},
	year = {2014},
	pmid = {10463930},
	note = {arXiv: 1408.5882
ISBN: 9781937284961},
	pages = {1746--1751},
}

@inproceedings{khatri_detecting_nodate,
	title = {Detecting {Offensive} {Content} in {Open}-domain {Conversations} using {Two} {Stage} {Semi}-supervision},
	abstract = {As open-ended human-chatbot interaction becomes commonplace, sensitive content detection gains importance. In this work, we propose a two stage semi-supervised approach to bootstrap large-scale data for automatic sensitive language detection from publicly available web resources. We explore various data selection methods including 1) using a blacklist to rank online discussion forums by the level of their sensitiveness followed by randomly sampling utterances and 2) training a weakly supervised model in conjunction with the blacklist for scoring sentences from online discussion forums to curate a dataset. Our data collection strategy is flexible and allows the models to detect implicit sensitive content for which manual annotations may be difficult. We train models using publicly available annotated datasets as well as using the proposed large-scale semi-supervised datasets. We evaluate the performance of all the models on Twitter and Toxic Wikipedia comments testsets as well as on a manually annotated spoken language dataset collected during a large scale chatbot competition. Results show that a model trained on this collected data outperforms the baseline models by a large margin on both in-domain and out-of-domain testsets, achieving an F1 score of 95.5\% on an out-of-domain testset compared to a score of 75\% for models trained on public datasets. We also showcase that large scale two stage semi-supervision generalizes well across multiple classes of sensitivities such as hate speech, racism, sexual and pornographic content, etc. without even providing explicit labels for these classes, leading to an average recall of 95.5\% versus the models trained using annotated public datasets which achieve an average recall of 73.2\% across seven sensitive classes on out-of-domain testsets.},
	author = {Khatri, Chandra and Hedayatnia, Behnam and Goel, Rahul and Venkatesh, Anushree and Gabriel, Raefer and Mandal, Arindam},
}

@article{gelber_differentiating_2021,
	title = {Differentiating hate speech: a systemic discrimination approach},
	volume = {24},
	issn = {17438772},
	doi = {10.1080/13698230.2019.1576006},
	abstract = {In this paper I develop a systemic discrimination approach to defining a narrowly construed category of ‘hate speech’, as speech that harms to a sufficient degree to warrant government regulation. This is important due to the lack of definitional clarity, and the extraordinarily wide usage, of the term. This article extends current literature on how hate speech can harm by identifying under what circumstances speakers have the capacity to harm, and under what circumstances targets are vulnerable to harm. It also shows how the capacity to harm can be mobile and involve the construction of new targets. Finally, it bridges the gap between conceptual understandings of hate speech and policy designed to regulate it.},
	number = {4},
	journal = {Critical Review of International Social and Political Philosophy},
	author = {Gelber, Katharine},
	year = {2021},
	note = {Publisher: Routledge},
	keywords = {Hate speech, authority, harm, systemic discrimination},
	pages = {393--414},
}

@article{mollas_ethos_2020,
	title = {{ETHOS}: an {Online} {Hate} {Speech} {Detection} {Dataset}},
	url = {http://arxiv.org/abs/2006.08328},
	doi = {10.1007/s40747-021-00608-2},
	abstract = {Online hate speech is a recent problem in our society that is rising at a steady pace by leveraging the vulnerabilities of the corresponding regimes that characterise most social media platforms. This phenomenon is primarily fostered by offensive comments, either during user interaction or in the form of a posted multimedia context. Nowadays, giant corporations own platforms where millions of users log in every day, and protection from exposure to similar phenomena appears to be necessary in order to comply with the corresponding legislation and maintain a high level of service quality. A robust and reliable system for detecting and preventing the uploading of relevant content will have a significant impact on our digitally interconnected society. Several aspects of our daily lives are undeniably linked to our social profiles, making us vulnerable to abusive behaviours. As a result, the lack of accurate hate speech detection mechanisms would severely degrade the overall user experience, although its erroneous operation would pose many ethical concerns. In this paper, we present 'ETHOS', a textual dataset with two variants: binary and multi-label, based on YouTube and Reddit comments validated using the Figure-Eight crowdsourcing platform. Furthermore, we present the annotation protocol used to create this dataset: an active sampling procedure for balancing our data in relation to the various aspects defined. Our key assumption is that, even gaining a small amount of labelled data from such a time-consuming process, we can guarantee hate speech occurrences in the examined material.},
	author = {Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08328},
}

@article{Egami2018,
	title = {How to {Make} {Causal} {Inferences} {Using} {Texts}},
	volume = {1050},
	journal = {Stat},
	author = {Egami, Naoki and Fong, Christian J. and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2018},
	pages = {6},
}

@inproceedings{Park2017,
	title = {One-step and {Two}-step {Classification} for {Abusive} {Language} {Detection} on {Twitter}},
	url = {http://arxiv.org/abs/1706.01206},
	doi = {10.18653/v1/W17-3006},
	abstract = {Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	author = {Park, Ji Ho and Fung, Pascale},
	year = {2017},
	note = {arXiv: 1706.01206},
}

@article{biber_styles_1989,
	title = {Styles of stance in {English}: {Lexical} and grammatical marking of evidentiality and affect},
	volume = {9},
	number = {1},
	journal = {Text-Interdisciplinary Journal for the Study of Discourse},
	author = {Biber, Douglas and {Finegan}},
	year = {1989},
	pages = {93--124},
}

@incollection{oakley_supporting_2017,
	title = {Supporting one another: {Nonbinary} community building on {Tumblr}},
	url = {https://books.google.com/books?id=NR0xDwAAQBAJ&pg=PT8&source=gbs_selected_pages&cad=3#v=onepage&q&f=false},
	booktitle = {Sex in the {Digital} {Age}},
	author = {Oakley, Abigail},
	editor = {Nixon, Paul G and Düsterhöft, Isabel K.},
	year = {2017},
}

@article{rosa_unsettling_2017,
	title = {Unsettling race and language: {Toward} a raciolinguistic perspective},
	volume = {46},
	issn = {14698013},
	doi = {10.1017/S0047404517000562},
	abstract = {This article presents what we term a raciolinguistic perspective, which theorizes the historical and contemporary co-naturalization of language and race. Rather than taking for granted existing categories for parsing and classifying race and language, we seek to understand how and why these categories have been co-naturalized, and to imagine their denaturalization as part of a broader structural project of contesting white supremacy. We explore five key components of a raciolinguistic perspective: (i) historical and contemporary colonial co-naturalizations of race and language; (ii) perceptions of racial and linguistic difference; (iii) regimentations of racial and linguistic categories; (iv) racial and linguistic intersections and assemblages; and (v) contestations of racial and linguistic power formations. These foci reflect our investment in developing a careful theorization of various forms of racial and linguistic inequality on the one hand, and our commitment to the imagination and creation of more just societies on the other. (Race, language ideologies, colonialism, governmentality, enregisterment, structural inequality)∗.},
	number = {5},
	journal = {Language in Society},
	author = {Rosa, Jonathan and Flores, Nelson},
	year = {2017},
	note = {ISBN: 0047404517000},
	keywords = {★},
	pages = {621--647},
}

@article{schwartz_authorship_2013,
	title = {Authorship {Attribution} of {Micro}-{Messages}},
	issn = {9781937284978},
	abstract = {Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author’s unique “signature”, and show that such signatures are typical of many authors when writing very short texts.We also present a new authorship attribution feature (“flexible patterns”) and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.},
	number = {October},
	journal = {Empirical Methods in Natural Language Processing},
	author = {Schwartz, Roy and Tsur, Oren and Rappoport, Ari and Koppel, Moshe},
	year = {2013},
	note = {ISBN: 9781937284978},
	pages = {1880--1891},
}

@article{tan_assessing_2019,
	title = {Assessing {Social} and {Intersectional} {Biases} in {Contextualized} {Word} {Representations}},
	issn = {23318422},
	abstract = {Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.},
	journal = {arXiv},
	author = {Tan, Yi Chern and Celis, L. Elisa},
	year = {2019},
	note = {arXiv: 1911.01485},
}

@inproceedings{WaseemHovy2016,
	title = {Hateful {Symbols} or {Hateful} {People}? {Predictive} {Features} for {Hate} {Speech} {Detection} on {Twitter}},
	isbn = {978-1-941643-81-5},
	doi = {10.18653/v1/N16-2013},
	abstract = {Hate speech in the form of racist and sex-ist remarks are a common occurrence on social media. For that reason, many so-cial media services address the problem of identifying hate speech, but the defini-tion of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lo-mas, 2015). We provide a list of criteria founded in critical race theory, and use them to an-notate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in con-junction with character n-grams for hate-speech detection. We also present a dic-tionary based the most indicative words in our data.},
	booktitle = {Proceedings of the {NAACL}-{HLT} 2016},
	author = {Waseem, Zeerak and Hovy, Dirk},
	year = {2016},
	pages = {88--93},
}

@article{So2019,
	title = {Race, {Writing}, and {Computation}: {Racial} {Difference} and the {US} {Novel}, 1880-2000},
	url = {https://culturalanalytics.org/article/11057.pdf},
	doi = {10.22148/16.031},
	journal = {Journal of Cultural Analytics},
	author = {So, Richard and Long, Hoyt and Zhu, Yuancheng},
	year = {2019},
	pages = {1--30},
}

@book{caswell_urgent_2021,
	series = {Routledge {Studies} in {Archives}},
	title = {Urgent {Archives}: {Enacting} {Liberatory} {Memory} {Work}},
	isbn = {978-1-00-038606-6},
	url = {https://books.google.com/books?id=76AsEAAAQBAJ},
	publisher = {Taylor \& Francis},
	author = {Caswell, M.},
	year = {2021},
	lccn = {2020053440},
}

@article{ansley_stirring_1989,
	title = {Stirring the {Ashes}: {Race} {Class} and the {Future} of {Civil} {Rights} {Scholarship}},
	volume = {74},
	url = {http://scholarship.law.cornell.edu/clrhttp://scholarship.law.cornell.edu/clr/vol74/iss6/1},
	number = {6},
	journal = {Cornell Law Review},
	author = {Ansley, Frances Lee},
	year = {1989},
	pages = {993--1077},
}

@article{singh_detecting_2021,
	title = {Detecting fake news stories via multimodal analysis},
	volume = {72},
	issn = {2330-1643},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24359},
	doi = {10.1002/asi.24359},
	abstract = {Filtering, vetting, and verifying digital information is an area of core interest in information science. Online fake news is a specific type of digital misinformation that poses serious threats to democratic institutions, misguides the public, and can lead to radicalization and violence. Hence, fake news detection is an important problem for information science research. While there have been multiple attempts to identify fake news, most of such efforts have focused on a single modality (e.g., only text-based or only visual features). However, news articles are increasingly framed as multimodal news stories, and hence, in this work, we propose a multimodal approach combining text and visual analysis of online news stories to automatically detect fake news. Drawing on key theories of information processing and presentation, we identify multiple text and visual features that are associated with fake or credible news articles. We then perform a predictive analysis to detect features most strongly associated with fake news. Next, we combine these features in predictive models using multiple machine-learning techniques. The experimental results indicate that a multimodal approach outperforms single-modality approaches, allowing for better fake news detection.},
	language = {en},
	number = {1},
	urldate = {2022-12-06},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Singh, Vivek K. and Ghosh, Isha and Sonagara, Darshan},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24359},
	pages = {3--17},
}

@inproceedings{assimakopoulos_annotating_2020,
	address = {Marseille, France},
	title = {Annotating for {Hate} {Speech}: {The} {MaNeCo} {Corpus} and {Some} {Input} from {Critical} {Discourse} {Analysis}},
	abstract = {This paper presents a novel scheme for the annotation of hate speech in corpora of Web 2.0 commentary. The proposed scheme is motivated by the critical analysis of posts made in reaction to news reports on the Mediterranean migration crisis and LGBTIQ+ matters in Malta, which was conducted under the auspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realization that hate speech is not a clear-cut category to begin with, appears to belong to a continuum of discriminatory discourse and is often realized through the use of indirect linguistic means, it is argued that annotation schemes for its detection should refrain from directly including the label 'hate speech,' as different annotators might have different thresholds as to what constitutes hate speech and what not. In view of this, we suggest a multi-layer annotation scheme, which is pilot-tested against a binary ±hate speech classification and appears to yield higher inter-annotator agreement. Motivating the postulation of our scheme, we then present the MaNeCo corpus on which it will eventually be used; a substantial corpus of on-line newspaper comments spanning 10 years.},
	booktitle = {Proceedings of the 12th {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2020)},
	author = {Assimakopoulos, Stavros and Muskat, Rebecca Vella and Van Der Plas, Lonneke and Gatt, Albert},
	year = {2020},
	keywords = {annotation, corpus creation, hate speech, newspaper comments},
	pages = {5088--5097},
}

@inproceedings{ma_domain_2019,
	address = {Hong Kong, China},
	title = {Domain {Adaptation} with {BERT}-based {Domain} {Classification} and {Data} {Selection}},
	url = {https://aclanthology.org/D19-6109},
	doi = {10.18653/v1/D19-6109},
	abstract = {The performance of deep neural models can deteriorate substantially when there is a domain shift between training and test data. For example, the pre-trained BERT model can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks. However, the fine-tuned BERT model suffers considerably at zero-shot when applied to a different domain. In this paper, we present a novel two-step domain adaptation framework based on curriculum learning and domain-discriminative data selection. The domain adaptation is conducted in a mostly unsupervised manner using a small target domain validation set for hyper-parameter tuning. We tested the framework on four large public datasets with different domain similarities and task types. Our framework outperforms a popular discrepancy-based domain adaptation method on most transfer tasks while consuming only a fraction of the training budget.},
	urldate = {2022-12-01},
	booktitle = {Proceedings of the 2nd {Workshop} on {Deep} {Learning} {Approaches} for {Low}-{Resource} {NLP} ({DeepLo} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Xiaofei and Xu, Peng and Wang, Zhiguo and Nallapati, Ramesh and Xiang, Bing},
	month = nov,
	year = {2019},
	pages = {76--83},
}

@inproceedings{jiang_instance_2007,
	address = {Prague, Czech Republic},
	title = {Instance {Weighting} for {Domain} {Adaptation} in {NLP}},
	url = {https://aclanthology.org/P07-1034},
	urldate = {2022-12-01},
	booktitle = {Proceedings of the 45th {Annual} {Meeting} of the {Association} of {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Jing and Zhai, ChengXiang},
	month = jun,
	year = {2007},
	pages = {264--271},
}

@inproceedings{field_controlled_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Controlled {Analyses} of {Social} {Biases} in {Wikipedia} {Bios}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3512134},
	doi = {10.1145/3485447.3512134},
	abstract = {Social biases on Wikipedia, a widely-read global platform, could greatly influence public opinion. While prior research has examined man/woman gender bias in biography articles, possible influences of other demographic attributes limit conclusions. In this work, we present a methodology for analyzing Wikipedia pages about people that isolates dimensions of interest (e.g., gender), from other attributes (e.g., occupation). Given a target corpus for analysis (e.g. biographies about women), we present a method for constructing a comparison corpus that matches the target corpus in as many attributes as possible, except the target one. We develop evaluation metrics to measure how well the comparison corpus aligns with the target corpus and then examine how articles about gender and racial minorities (cis. women, non-binary people, transgender women, and transgender men; African American, Asian American, and Hispanic/Latinx American people) differ from other articles. In addition to identifying suspect social biases, our results show that failing to control for covariates can result in different conclusions and veil biases. Our contributions include methodology that facilitates further analyses of bias in Wikipedia articles, findings that can aid Wikipedia editors in reducing biases, and a framework and evaluation metrics to guide future work in this area.},
	urldate = {2022-11-16},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Field, Anjalie and Park, Chan Young and Lin, Kevin Z. and Tsvetkov, Yulia},
	month = apr,
	year = {2022},
	keywords = {NLP, Wikipedia, gender bias, matching, racial bias},
	pages = {2624--2635},
}

@article{park_multilingual_2021,
	title = {Multilingual {Contextual} {Affective} {Analysis} of {LGBT} {People} {Portrayals} in {Wikipedia}},
	volume = {15},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/18077},
	doi = {10.1609/icwsm.v15i1.18077},
	abstract = {Specific lexical choices in narrative text reflect both the writer's attitudes towards people in the narrative and influence the audience's reactions. Prior work has examined descriptions of people in English using contextual affective analysis, a natural language processing (NLP) technique that seeks to analyze how people are portrayed along dimensions of power, agency, and sentiment. Our work presents an extension of this methodology to multilingual settings, which is enabled by a new corpus that we collect and a new multilingual model. We additionally show how word connotations differ across languages and cultures, highlighting the difficulty of generalizing existing English datasets and methods. We then demonstrate the usefulness of our method by analyzing Wikipedia biography pages of members of the LGBT community across three languages: English, Russian, and Spanish. Our results show systematic differences in how the LGBT community is portrayed across languages, surfacing cultural differences in narratives and signs of social biases. Practically, this model can be used to identify Wikipedia articles for further manual analysis---articles that might contain content gaps or an imbalanced representation of particular social groups.},
	language = {en},
	urldate = {2022-11-16},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Park, Chan Young and Yan, Xinru and Field, Anjalie and Tsvetkov, Yulia},
	month = may,
	year = {2021},
	keywords = {Social network analysis, communities identification, expertise and authority discovery},
	pages = {479--490},
}

@book{hubscher_antisemitism_2022,
	address = {London},
	edition = {1},
	title = {Antisemitism on {Social} {Media}},
	isbn = {978-1-00-320049-9},
	url = {https://www.taylorfrancis.com/books/9781003200499},
	language = {en},
	urldate = {2022-11-09},
	publisher = {Routledge},
	author = {Hübscher, Monika and Mering, Sabine von},
	month = jan,
	year = {2022},
	doi = {10.4324/9781003200499},
}

@article{minnix_globalist_2017,
	title = {"{Globalist} {Scumbags}": {Composition}'s {Global} {Turn} in a {Time} of {Fake} {News}, {Globalist} {Conspiracy}, and {Nationalist} {Literacy}},
	volume = {5},
	copyright = {Copyright (c) 2017 Christopher Minnix},
	issn = {2326-5620},
	shorttitle = {"{Globalist} {Scumbags}"},
	url = {https://licsjournal.org/index.php/LiCS/article/view/746},
	doi = {10.21623/1.5.2.5},
	abstract = {The past twenty years have witnessed a significant and sustained global turn in American higher education, with many US colleges and universities pursuing curricular and institutional programs that prepare students for lives of global engagement. Many institutions have redefined their civic and ethical institutional goals to foster not only national citizenship but global citizenship. While global education has consistently been attacked by the right for many years, the scope and virulence of attacks on global higher education have been amplified in the lead-up to the 2016 presidential campaign and following the election of Donald Trump. Portrayals of global education as a global conspiracy have circulated across a range of fake-news, alt-right, and hard right publications, as have calls for nationalist visions of civic education that instill an appreciation for American exceptionalism. This article maps out how global higher education is constructed in the populist rhetoric of the political right, both in accounts from fake news sources and hard right news sources and in the educational policy discourse of conservative organizations like the National Association of Scholars. It then explores the consequences of anti-global education rhetoric for the global turn in rhetoric and composition studies and maps out both a critical and political response.},
	language = {en},
	number = {2},
	urldate = {2022-11-08},
	journal = {Literacy in Composition Studies},
	author = {Minnix, Christopher},
	month = dec,
	year = {2017},
	note = {Number: 2},
	keywords = {civic literacy, global education, global turn in rhetoric and composition studies, nationalism, populism},
	pages = {63--83},
}

@article{mcneil-willson_understanding_2022,
	title = {Understanding the \#plandemic: {Core} framings on {Twitter} and what this tells us about countering online far right {COVID}-19 conspiracies},
	copyright = {Copyright (c) 2022 First Monday},
	issn = {1396-0466},
	shorttitle = {Understanding the \#plandemic},
	url = {https://firstmonday.org/ojs/index.php/fm/article/view/12614},
	doi = {10.5210/fm.v27i5.12614},
	abstract = {This paper examines the need and possibility for developing online resilience-based approaches in response to COVID-19 vaccine conspiracies, often linked to the far right. Examining three datasets collected between December 2020 and April 2021, this paper details conspiracy narratives that have developed around COVID-19 vaccines, with specific focus on understanding the deployment of the idea of a planned pandemic or so-called ‘\#plandemic’. This is then used to consider where existing resilience-based approaches to countering off-line polarisation and extremism might posit an appropriate online response. The article identifies four key \#plandemic framings of COVID-19 vaccines — as control, as reset, as unnecessary and as unsafe — and analyses how these themes are constructed, to find that they are often created through hostile and confrontational interaction with other users. Based on these findings, the conclusion suggests companies shift their focus away from ‘negative’ approaches to content moderation (e.g., content removal) and towards resilience-building responses that cultivate flexible individual identities, build community support networks, and/or engage users with national and supranational democratic structures, as a more effective response to the sharing of online conspiracies.},
	language = {en},
	urldate = {2022-11-08},
	journal = {First Monday},
	author = {McNeil-Willson, Richard},
	month = may,
	year = {2022},
}

@article{mcilroy-young_welcome_2019,
	title = {From “{Welcome} {New} {Gabbers}” to the {Pittsburgh} {Synagogue} {Shooting}: {The} {Evolution} of {Gab}},
	volume = {13},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {From “{Welcome} {New} {Gabbers}” to the {Pittsburgh} {Synagogue} {Shooting}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/3264},
	doi = {10.1609/icwsm.v13i01.3264},
	abstract = {Gab, an online social media platform with very little content moderation, has recently come to prominence as an alt-right community and a haven for hate speech. We document the evolution of Gab since its inception until a Gab user carried out the most deadly attack on the Jewish community in US history. We investigate Gab language use, study how topics evolved over time, and find that the shooters’ posts were among the most consistently anti-Semitic on Gab, but that hundreds of other users were even more extreme.},
	language = {en},
	urldate = {2022-11-08},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {McIlroy-Young, Reid and Anderson, Ashton},
	month = jul,
	year = {2019},
	pages = {651--654},
}

@inproceedings{kalmar_twitter_2018,
	address = {New York, NY, USA},
	series = {{SMSociety} '18},
	title = {Twitter, {Gab}, and {Racism}: {The} {Case} of the {Soros} {Myth}},
	isbn = {978-1-4503-6334-1},
	shorttitle = {Twitter, {Gab}, and {Racism}},
	url = {https://doi.org/10.1145/3217804.3217939},
	doi = {10.1145/3217804.3217939},
	abstract = {With pressure on Twitter from governments and others intent on preventing the spread of racist, including anti-Semitic and Islamophobic, messages, it appears that some right-wing populist users have moved to Gab, where they are less likely to be censored. There appears to be a split between those who are willing to play by the new rules on Twitter, and those who prefer the ability to express their racism more openly on Gab. We believe that this split reflects a broader, ongoing split within the far right. In the United States and in Europe, the more "moderate" right, hopeful of electoral successes, conserves its Islamophobic and anti-migrant rhetoric, but is moving away from overt anti-Semitism. Meantime the diehard anti-Semites develop their own, separate networks, online and offline, and these may now include Gab. Our test case is what we call the "Soros Myth," which accuses the Hungarian-born, American-Jewish financier George Soros of instigating or supporting an astonishingly large array of causes and events that right-wing populist resent, including the mass migration of Muslim refugees to Europe. The article discusses the methodology of gathering relevant information on Twitter and Gab, and the preliminary results, which strongly support our working hypothesis that when it comes to the Soros Myth, the more overtly anti-Semitic content is now found on Gab.},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Social} {Media} and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Kalmar, Ivan and Stevens, Christopher and Worby, Nicholas},
	month = jul,
	year = {2018},
	keywords = {Gab, Islamophobia, Soros, Twitter, anti-Semitism, far-right, populism},
	pages = {330--334},
}

@techreport{fried_masculinities_nodate,
	title = {Masculinities and preventing violent extremism: making the connections},
	institution = {Promundo-US},
	author = {Fried, Abby and Lauro, Giovanna and Barker, Gary},
}

@inproceedings{feng_language-agnostic_2022,
	address = {Dublin, Ireland},
	title = {Language-agnostic {BERT} {Sentence} {Embedding}},
	url = {https://aclanthology.org/2022.acl-long.62},
	doi = {10.18653/v1/2022.acl-long.62},
	abstract = {While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80\%. Composing the best of these methods produces a model that achieves 83.7\% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5\% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
	month = may,
	year = {2022},
	pages = {878--891},
}

@inproceedings{Sap2020,
	title = {Social {Bias} {Frames}: {Reasoning} about {Social} and {Power} {Implications} of {Language}},
	url = {https://aclanthology.org/2020.acl-main.486/},
	doi = {10.18653/v1/2020.acl-main.486},
	abstract = {Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that "we shouldn't lower our standards to hire more women," most listeners will infer the implicature intended by the speaker -- that "women (candidates) are less qualified." Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80\% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	year = {2020},
	note = {arXiv: 1911.03891},
	pages = {5477--5490},
}

@inproceedings{levi_identifying_2019,
	address = {Hong Kong, China},
	title = {Identifying {Nuances} in {Fake} {News} vs. {Satire}: {Using} {Semantic} and {Linguistic} {Cues}},
	shorttitle = {Identifying {Nuances} in {Fake} {News} vs. {Satire}},
	url = {https://aclanthology.org/D19-5004},
	doi = {10.18653/v1/D19-5004},
	abstract = {The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.},
	urldate = {2022-10-17},
	booktitle = {Proceedings of the {Second} {Workshop} on {Natural} {Language} {Processing} for {Internet} {Freedom}: {Censorship}, {Disinformation}, and {Propaganda}},
	publisher = {Association for Computational Linguistics},
	author = {Levi, Or and Hosseini, Pedram and Diab, Mona and Broniatowski, David},
	month = nov,
	year = {2019},
	pages = {31--35},
}

@inproceedings{rottger_hatecheck_2021,
	title = {{HATECHECK}: {Functional} {Tests} for {Hate} {Speech} {Detection} {Models}},
	url = {https://github.com/paul-rottger/hatecheck-data.},
	abstract = {Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HATECHECK's utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Röttger, Paul and Vidgen, Bertram and Nguyen, Dong and Waseem, Zeerak and Margetts, Helen and Pierrehumbert, Janet B},
	year = {2021},
	pages = {41--58},
}

@inproceedings{ousidhoum_multilingual_2019,
	title = {Multilingual and {Multi}-{Aspect} {Hate} {Speech} {Analysis}},
	abstract = {Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Ousidhoum, Nedjma and Lin, Zizheng and Zhang, Hongming and Song, Yangqiu and Yeung, Dit-Yan},
	year = {2019},
	pages = {4675--4684},
}

@inproceedings{sap_annotators_2022,
	address = {Seattle, United States},
	title = {Annotators with {Attitudes}: {How} {Annotator} {Beliefs} {And} {Identities} {Bias} {Toxic} {Language} {Detection}},
	shorttitle = {Annotators with {Attitudes}},
	url = {https://aclanthology.org/2022.naacl-main.431},
	doi = {10.18653/v1/2022.naacl-main.431},
	abstract = {The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.},
	urldate = {2022-10-14},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and Zhou, Xuhui and Choi, Yejin and Smith, Noah A.},
	month = jul,
	year = {2022},
	pages = {5884--5906},
}

@inproceedings{qian_leveraging_2018,
	title = {Leveraging {Intra}-{User} and {Inter}-{User} {Representation} {Learning} for {Automated} {Hate} {Speech} {Detection}},
	abstract = {Hate speech detection is a critical, yet challenging problem in Natural Language Processing (NLP). Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor. The central problem is that social media posts are short and noisy, and most existing hate speech detection solutions take each post as an isolated input instance, which is likely to yield high false positive and negative rates. In this paper, we radically improve automated hate speech detection by presenting a novel model that leverages intra-user and inter-user representation learning for robust hate speech detection on Twitter. In addition to the target Tweet, we collect and analyze the user's historical posts to model intra-user Tweet representations. To suppress the noise in a single Tweet, we also model the similar Tweets posted by all other users with reinforced inter-user representation learning techniques. Experimentally, we show that leverag-ing these two representations can significantly improve the f-score of a strong bidirectional LSTM baseline model by 10.1\%.},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Qian, Jing and Elsherief, Mai and Belding, Elizabeth M and Wang, William Yang},
	year = {2018},
	pages = {118--123},
}

@inproceedings{unsvag_effects_2018,
	title = {The {Effects} of {User} {Features} on {Twitter} {Hate} {Speech} {Detection}},
	abstract = {The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics , but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classification performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.},
	urldate = {2022-03-22},
	booktitle = {Proceedings of the {Second} {Workshop} on {Abusive} {Language} {Online} ({ALW2})},
	author = {Unsvåg, Elise Fehn and Gambäck, Björn},
	year = {2018},
	pages = {75--85},
}

@inproceedings{hutchinson_towards_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Towards {Accountability} for {Machine} {Learning} {Datasets}: {Practices} from {Software} {Engineering} and {Infrastructure}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Towards {Accountability} for {Machine} {Learning} {Datasets}},
	url = {https://doi.org/10.1145/3442188.3445918},
	doi = {10.1145/3442188.3445918},
	abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
	urldate = {2022-10-04},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	month = mar,
	year = {2021},
	keywords = {datasets, machine learning, requirements engineering},
	pages = {560--575},
}

@article{lee_community-based_2022,
	title = {Community-based strategies for combating misinformation: {Learning} from a popular culture fandom},
	shorttitle = {Community-based strategies for combating misinformation},
	url = {https://misinforeview.hks.harvard.edu/article/community-based-strategies-for-combating-misinformation-learning-from-a-popular-culture-fandom/},
	doi = {10.37016/mr-2020-105},
	abstract = {Through the lens of one of the fastest-growing international fandoms, this study explores everyday misinformation in the context of networked online environments. Findings show that fans experience a range of misinformation, similar to what we see in other political, health, or crisis contexts. However, the strong sense of community and shared purpose of the group},
	language = {en-US},
	urldate = {2022-09-30},
	journal = {Harvard Kennedy School Misinformation Review},
	author = {Lee, Jin Ha and Santero, Nicole and Bhattacharya, Arpita and May, Emma and Spiro, Emma S.},
	month = sep,
	year = {2022},
}

@misc{gallwitz_rise_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Rise} and {Fall} of '{Social} {Bot}' {Research}},
	url = {https://papers.ssrn.com/abstract=3814191},
	abstract = {The idea that social media platforms like Twitter are inhabited by vast numbers of “social bots” has become widely accepted in recent years. “Social bots” are assumed to be automated social media accounts operated by malicious actors with the goal of manipulating public opinion. They are credited with the ability to produce content autonomously and to interact with human users. “Social bot” activity has been reported in many different political contexts, including Donald Trump’s election and the Brexit referendum in 2016. However, the relevant publications either use crude and questionable heuristics to discriminate between supposed “social bots” and humans or—in the vast majority of the cases—fully rely on the output of automatic bot detection tools, most commonly Botometer. We point out fundamental theoretical flaws of these approaches. Also, we closely and systematically inspected hundreds of accounts that had been counted or even presented as “social bots” in peer-reviewed studies. We were unable to find a single “social bot”. Instead, we found mostly accounts undoubtedly operated by human users, the vast majority of them using Twitter in an inconspicious and unremarkable fashion without the slightest traces of automation. We conclude that studies claiming to investigate the prevalence or influence of “social bots” have, in reality, just investigated false positives and artifacts of the flawed detection methods employed.},
	language = {en},
	urldate = {2022-09-26},
	author = {Gallwitz, Florian and Kreil, Michael},
	month = mar,
	year = {2021},
	keywords = {Bot detection, Botometer, False positives, Social bots},
}

@article{dancy_ai_2022,
	title = {{AI} and {Blackness}: {Toward} {Moving} {Beyond} {Bias} and {Representation}},
	volume = {3},
	issn = {2637-6415},
	shorttitle = {{AI} and {Blackness}},
	doi = {10.1109/TTS.2021.3125998},
	abstract = {In this article, we argue that AI ethics must move beyond the concepts of race-based representation and bias, and toward those that probe the deeper relations that impact how these systems are designed, developed, and deployed. Many recent discussions on ethical considerations of bias in AI systems have centered on racial bias. We contend that antiblackness in AI requires more of an examination of the ontological space that provides a foundation for the design, development, and deployment of AI systems. We examine what this contention means from the perspective of the sociocultural context in which AI systems are designed, developed, and deployed and focus on intersections with anti-Black racism (antiblackness). To bring these multiple perspectives together and show an example of antiblackness in the face of attempts at de-biasing, we discuss results from auditing an existing open-source semantic network (ConceptNet). We use this discussion to further contextualize antiblackness in design, development, and deployment of AI systems and suggest questions one may ask when attempting to combat antiblackness in AI systems.},
	number = {1},
	journal = {IEEE Transactions on Technology and Society},
	author = {Dancy, Christopher L. and Saucier, P. Khalil},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Technology and Society},
	keywords = {Artificial intelligence, Ethics, Human intelligence, Internet, K.2.b people, K.4 computers and society, K.4.1.c ethics, K.4.2 social issues, Knowledge based systems, Ontologies, Search engines},
	pages = {31--40},
}

@article{jones_im_2022,
	title = {‘{I}'m a boy, can't you see that?’: {Dialogic} embodiment and the construction of agency in trans youth discourse},
	issn = {0047-4045, 1469-8013},
	shorttitle = {‘{I}'m a boy, can't you see that?},
	url = {https://www.cambridge.org/core/product/identifier/S0047404522000252/type/journal_article},
	doi = {10.1017/S0047404522000252},
	abstract = {Abstract
            This article offers discourse analysis of young transgender people's interaction, in which they describe being rendered powerless through misgendering or misrepresentation. It argues that the young people's collective responses to these moments enable them to challenge the ideologies underpinning their marginalisation, and to recontextualise the language used by others to describe their bodies. Stance-taking, the production of affect, and constructed dialogue are shown to be key tools in their production of an agentive, mutual identity. The article thus provides close analysis of dialogic embodiment, a process by which the body is quite literally spoken into being. By critiquing the cisnormative structures which inform and enable the young people's marginalisation, the article responds to the call for a trans linguistics (Zimman 2020) and reflects upon the author's positionality as a cisgender researcher. (Embodiment, affective stance, agency, trans identity, cisnormativity, trans linguistics)*},
	language = {en},
	urldate = {2022-09-10},
	journal = {Language in Society},
	author = {Jones, Lucy},
	month = sep,
	year = {2022},
	pages = {1--22},
}

@article{buolamwini_gender_nodate,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classiﬁcation}},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classiﬁcation system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We ﬁnd that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classiﬁcation systems using our dataset and show that darker-skinned females are the most misclassiﬁed group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classiﬁcation systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	author = {Buolamwini, Joy and Gebru, Timnit},
	pages = {15},
}

@article{ilbury_tale_2022,
	title = {A tale of two cities: {The} discursive construction of ‘place’ in gentrifying {East} {London}},
	volume = {51},
	issn = {0047-4045, 1469-8013},
	shorttitle = {A tale of two cities},
	url = {https://www.cambridge.org/core/product/identifier/S0047404521000130/type/journal_article},
	doi = {10.1017/S0047404521000130},
	abstract = {Abstract
            In recent years, the East End of London has been dramatically transformed from a poor, working-class area, to one of the most fashionable neighbourhoods in the world. Adding to a growing body of research which examines the sociolinguistic dynamics of gentrifying neighbourhoods, this article draws on data from two ethnographic projects to examine how young people from the gentrified (i.e. working-class) and gentrifier (i.e. middle-class) communities index place attachment in East London. I demonstrate that for the gentrified community, place attachment is related to the ethnic and cultural genealogy of the immediate, local neighbourhood. Whilst for the gentrifiers, place identity is associated with the cosmopolitan economic and social opportunities of the city. I argue that whilst these communities occupy the same physical neighbourhood, these discourses suggest that they conceptually and socioculturally reside in two very different cities. (Gentrification, place, space, East London)*},
	language = {en},
	number = {3},
	urldate = {2022-09-02},
	journal = {Language in Society},
	author = {Ilbury, Christian},
	month = jun,
	year = {2022},
	pages = {511--534},
}

@article{ozalp_antisemitism_2020,
	title = {Antisemitism on {Twitter}: {Collective} {Efficacy} and the {Role} of {Community} {Organisations} in {Challenging} {Online} {Hate} {Speech}},
	volume = {6},
	issn = {2056-3051, 2056-3051},
	shorttitle = {Antisemitism on {Twitter}},
	url = {http://journals.sagepub.com/doi/10.1177/2056305120916850},
	doi = {10.1177/2056305120916850},
	abstract = {In this article, we conduct a comprehensive study of online antagonistic content related to Jewish identity posted on Twitter between October 2015 and October 2016 by UK-based users. We trained a scalable supervised machine learning classifier to identify antisemitic content to reveal patterns of online antisemitism perpetration at the source. We built statistical models to analyze the inhibiting and enabling factors of the size (number of retweets) and survival (duration of retweets) of information flows in addition to the production of online antagonistic content. Despite observing high temporal variability, we found that only a small proportion (0.7\%) of the content was antagonistic. We also found that antagonistic content was less likely to disseminate in size or survive for a longer period. Information flows from antisemitic agents on Twitter gained less traction, while information flows emanating from capable and willing counter-speech actors—that is, Jewish organizations—had a significantly higher size and survival rates. This study is the first to demonstrate that Sampson’s classic sociological concept of collective efficacy can be observed on social media (SM). Our findings suggest that when organizations aiming to counter harmful narratives become active on SM platforms, their messages propagate further and achieve greater longevity than antagonistic messages. On SM, counter-speech posted by credible, capable and willing actors can be an effective measure to prevent harmful narratives. Based on our findings, we underline the value of the work by community organizations in reducing the propagation of cyberhate and increasing trust in SM platforms.},
	language = {en},
	number = {2},
	urldate = {2022-09-01},
	journal = {Social Media + Society},
	author = {Ozalp, Sefa and Williams, Matthew L. and Burnap, Pete and Liu, Han and Mostafa, Mohamed},
	month = apr,
	year = {2020},
	pages = {205630512091685},
}

@inproceedings{hessel_unsupervised_2019,
	address = {Hong Kong, China},
	title = {Unsupervised {Discovery} of {Multimodal} {Links} in {Multi}-image, {Multi}-sentence {Documents}},
	url = {https://www.aclweb.org/anthology/D19-1210},
	doi = {10.18653/v1/D19-1210},
	abstract = {Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difﬁculty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We ﬁnd that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can sufﬁce to predict links between speciﬁc sentences and speciﬁc images within the same document at test time.},
	language = {en},
	urldate = {2022-08-31},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hessel, Jack and Lee, Lillian and Mimno, David},
	year = {2019},
	pages = {2034--2045},
}

@article{kong_slipping_2022,
	title = {Slipping to the {Extreme}: {A} {Mixed} {Method} to {Explain} {How} {Extreme} {Opinions} {Infiltrate} {Online} {Discussions}},
	volume = {16},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {Slipping to the {Extreme}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/19312},
	abstract = {Qualitative research provides methodological guidelines for observing and studying communities and cultures on online social media platforms. However, such methods demand considerable manual effort from researchers and can be overly focused and narrowed to certain online groups. This work proposes a complete solution to accelerate the qualitative analysis of problematic online speech, focusing on opinions emerging from online communities by leveraging machine learning algorithms. First, we employ qualitative methods of deep observation for understanding problematic online speech. This initial qualitative study constructs an ontology of problematic speech, which contains social media postings annotated with their underlying opinions. The qualitative study dynamically constructs the set of opinions, simultaneous with labeling the postings. Next, we use keywords to collect a large dataset from three online social media platforms (Facebook, Twitter, and Youtube). Finally, we introduce an iterative data exploration procedure to augment the dataset. It alternates between a data sampler --- which balances exploration and exploitation of unlabeled data --- the automatic labeling of the sampled data, the manual inspection by the qualitative mapping team, and, finally, the retraining of the automatic opinion classifiers. We present both qualitative and quantitative results. First, we show that our human-in-the-loop method successfully augments the initial qualitatively labeled and narrowly focused dataset and constructs a more encompassing dataset. Next, we present detailed case studies of the dynamics of problematic speech in a far-right Facebook group, exemplifying its mutation from conservative to extreme. Finally, we examine the dynamics of opinion emergence and co-occurrence, and we hint at some pathways through which extreme opinions creep into the mainstream online discourse.},
	language = {en},
	urldate = {2022-08-30},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Kong, Quyu and Booth, Emily and Bailo, Francesco and Johns, Amelia and Rizoiu, Marian-Andrei},
	month = may,
	year = {2022},
	keywords = {Web and Social Media},
	pages = {524--535},
}

@article{michael_what_nodate,
	title = {{WHAT} {DO} {NLP} {RESEARCHERS} {BELIEVE}? {RESULTS} {OF} {THE} {NLP} {COMMUNITY} {METASURVEY}},
	abstract = {We present the results of the NLP Community Metasurvey. Run from May to June 2022, the survey elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split almost exactly in half on questions about the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed metaquestions, asking respondents to predict the distribution of survey responses. This allows us not only to gain insight on the spectrum of beliefs held by NLP researchers, but also to uncover false sociological beliefs where the community’s predictions don’t match reality. We find such mismatches on a wide range of issues. Among other results, the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its own belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.},
	language = {en},
	author = {Michael, Julian and Holtzman, Ari and Parrish, Alicia and Mueller, Aaron and Wang, Alex and Chen, Angelica and Madaan, Divyam and Nangia, Nikita and Pang, Richard Yuanzhe and Phang, Jason and Bowman, Samuel R},
	pages = {31},
}

@inproceedings{field_survey_2021,
	address = {Online},
	title = {A {Survey} of {Race}, {Racism}, and {Anti}-{Racism} in {NLP}},
	url = {https://aclanthology.org/2021.acl-long.149},
	doi = {10.18653/v1/2021.acl-long.149},
	abstract = {Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Field, Anjalie and Blodgett, Su Lin and Waseem, Zeerak and Tsvetkov, Yulia},
	month = aug,
	year = {2021},
	pages = {1905--1925},
}

@article{calderon_linguistic_2021,
	title = {Linguistic {Patterns} for {Code} {Word} {Resilient} {Hate} {Speech} {Identification}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/23/7859},
	doi = {10.3390/s21237859},
	abstract = {The permanent transition to online activity has brought with it a surge in hate speech discourse. This has prompted increased calls for automatic detection methods, most of which currently rely on a dictionary of hate speech words, and supervised classification. This approach often falls short when dealing with newer words and phrases produced by online extremist communities. These code words are used with the aim of evading automatic detection by systems. Code words are frequently used and have benign meanings in regular discourse, for instance, “skypes, googles, bing, yahoos” are all examples of words that have a hidden hate speech meaning. Such overlap presents a challenge to the traditional keyword approach of collecting data that is specific to hate speech. In this work, we first introduced a word embedding model that learns the hidden hate speech meaning of words. With this insight on code words, we developed a classifier that leverages linguistic patterns to reduce the impact of individual words. The proposed method was evaluated across three different datasets to test its generalizability. The empirical results show that the linguistic patterns approach outperforms the baselines and enables further analysis on hate speech expressions.},
	language = {en},
	number = {23},
	urldate = {2022-08-22},
	journal = {Sensors},
	author = {Calderón, Fernando H. and Balani, Namrita and Taylor, Jherez and Peignon, Melvyn and Huang, Yen-Hao and Chen, Yi-Shin},
	month = jan,
	year = {2021},
	note = {Number: 23
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hate speech, linguistic patterns, social media},
	pages = {7859},
}

@article{jokubauskaite_generally_2020,
	title = {Generally {Curious}: {Thematically} {Distinct} {Datasets} of {General} {Threads} on 4chan/pol/},
	volume = {14},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {Generally {Curious}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/7351},
	abstract = {Over the second half of the 2010s, the /pol/ (‘politically incorrect’) forum on the 4chan image board has emerged as a space within which various extreme political ideologies are discussed and cultivated, occasionally informing off-site acts of political extremism. While previous research has often studied this space as a unified whole, it is relevant to more specifically demarcate different publics within 4chan’s /pol/ board, apart from studying it as an ‘amorphous blob’. This paper focuses specifically on ‘generals’ - recurring threads with a specific thematic focus identified by a particular vernacular phrase or tag. By identifying them it is possible to partition the board’s archive into multiple distinct datasets comprising discussions about a particular topic, such as Donald Trump, the Syria war, or British politics. We provide a dataset containing 58,841 opening posts and 13,697,738 replies to those, divided over 329 thematically distinct general thread collections. In this paper we outline our data collection and query protocol, the structure of the data and its rationale, as well as a number of suggested research uses for this new data.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Jokubauskaitė, Emilija and Peeters, Stijn},
	month = may,
	year = {2020},
	pages = {863--867},
}

@article{papasavva_raiders_2020,
	title = {Raiders of the {Lost} {Kek}: 3.5 {Years} of {Augmented} 4chan {Posts} from the {Politically} {Incorrect} {Board}},
	volume = {14},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {Raiders of the {Lost} {Kek}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/7354},
	abstract = {This paper presents a dataset with over 3.3M threads and 134.5M posts from the Politically Incorrect board (/pol/) of the imageboard forum 4chan, posted over a period of almost 3.5 years (June 2016-November 2019). To the best of our knowledge, this represents the largest publicly available 4chan dataset, providing the community with an archive of posts that have been permanently deleted from 4chan and are otherwise inaccessible. We augment the data with a set of additional labels, including toxicity scores and the named entities mentioned in each post. We also present a statistical analysis of the dataset, providing an overview of what researchers interested in using it can expect, as well as a simple content analysis, shedding light on the most prominent discussion topics, the most popular entities mentioned, and the toxicity level of each post. Overall, we are confident that our work will motivate and assist researchers in studying and understanding 4chan, as well as its role on the greater Web. For instance, we hope this dataset may be used for cross-platform studies of social media, as well as being useful for other types of research like natural language processing. Finally, our dataset can assist qualitative work focusing on in-depth case studies of specific narratives, events, or social theories.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Papasavva, Antonis and Zannettou, Savvas and Cristofaro, Emiliano De and Stringhini, Gianluca and Blackburn, Jeremy},
	month = may,
	year = {2020},
	pages = {885--894},
}

@techreport{sahlgren_learning_2018,
	title = {Learning {Representations} for {Detecting} {Abusive} {Language}},
	url = {https://aclanthology.org/W18-5115.pdf},
	abstract = {This paper discusses the question whether it is possible to learn a generic representation that is useful for detecting various types of abusive language. The approach is inspired by recent advances in transfer learning and word embed-dings, and we learn representations from two different datasets containing various degrees of abusive language. We compare the learned representation with two standard approaches; one based on lexica, and one based on data-specific n-grams. Our experiments show that learned representations do contain useful information that can be used to improve detection performance when training data is limited.},
	author = {Sahlgren, Magnus and Isbister, Tim and Olsson, Fredrik},
	year = {2018},
	pages = {115--123},
}

@misc{taylor_surfacing_2017,
	title = {Surfacing contextual hate speech words within social media},
	url = {https://arxiv.org/pdf/1711.10093.pdf},
	abstract = {Social media platforms have recently seen an increase in the occurrence of hate speech discourse which has led to calls for improved detection methods. Most of these rely on annotated data, keywords, and a classification technique. While this approach provides good coverage, it can fall short when dealing with new terms produced by online extremist communities which act as original sources of words which have alternate hate speech meanings. These code words (which can be both created and adopted words) are designed to evade automatic detection and often have benign meanings in regular discourse. As an example, "skypes", "googles", and "yahoos" are all instances of words which have an alternate meaning that can be used for hate speech. This overlap introduces additional challenges when relying on keywords for both the collection of data that is specific to hate speech, and downstream classification. In this work, we develop a community detection approach for finding extremist hate speech communities and collecting data from their members. We also develop a word embedding model that learns the alternate hate speech meaning of words and demonstrate the candidacy of our code words with several annotation experiments, designed to determine if it is possible to recognize a word as being used for hate speech without knowing its alternate meaning. We report an inter-annotator agreement rate of K=0.871, and K=0.676 for data drawn from our extremist community and the keyword approach respectively, supporting our claim that hate speech detection is a contextual task and does not depend on a fixed list of keywords. Our goal is to advance the domain by providing a high quality hate speech dataset in addition to learned code words that can be fed into existing classification approaches, thus improving the accuracy of automated detection.},
	author = {Taylor, Jherez and Peignon, Melvyn and Chen, Yi-Shin},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10093},
}

@article{thompson_watching_2001,
	title = {{WATCHING} {THE} {STORMFRONT}: {White} {Nationalists} and the {Building} of {Community} in {Cyberspace}},
	volume = {45},
	number = {1},
	journal = {Social Analysis: The International Journal of Anthropology},
	author = {Thompson, Kevin C},
	year = {2001},
	note = {Publication Title: COMPUTER-MEDIATED COMMUNICATION IN AUSTRALIAN ANTHROPOLOGY AND SOCIOLOGY
Volume: 45
Issue: 1},
	pages = {32--52},
}

@article{ansah_violent_2021,
	title = {{VIOLENT} {WORDS}: {STRATEGIES} {AND} {LEGAL} {IMPACTS} {OF} {WHITE} {SUPREMACIST} {LANGUAGE}},
	volume = {28},
	number = {3},
	journal = {Virginia Journal of Social Policy \& the Law},
	author = {Ansah, Tawia},
	year = {2021},
	pages = {305--340},
}

@article{ferber_racial_2000,
	title = {Racial {Warriors} and {Weekend} {Warriors}: {The} {Construction} of {Masculinity} in {Mythopoetic} and {White} {Supremacist} {Discourse}},
	volume = {3},
	issn = {1097-184X, 1552-6828},
	shorttitle = {Racial {Warriors} and {Weekend} {Warriors}},
	url = {http://journals.sagepub.com/doi/10.1177/1097184X00003001002},
	doi = {10.1177/1097184X00003001002},
	abstract = {This article explores contemporary white male backlash by comparing the discourses of two movements usually assumed to have little in common: the mythopoetic men's movement and the contemporary white supremacist movement. While there are clear differences, the author argues that they are similar in important ways, and with consequences. These two discourses reinforce broader reactionary discourses about gender and share a number of common assumptions. They both construct gender in essentialist terms, depict contemporary American men as demasculinized, blame contemporary social problems on this demasculinization, blame women and the women's movement for this demasculin-ization, and seek to help men rediscover their lost masculinity and reassert their rightful authority. Both movements encourage white men to see themselves as victims and argue that (white) men are the truly oppressed minority in today's world.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {Men and Masculinities},
	author = {Ferber, Abby L.},
	month = jul,
	year = {2000},
	pages = {30--56},
}

@incollection{bouvier_covert_2020,
	address = {Cham},
	title = {Covert {Hate} {Speech}: {White} {Nationalists} and {Dog} {Whistle} {Communication} on {Twitter}},
	isbn = {978-3-030-41420-7 978-3-030-41421-4},
	shorttitle = {Covert {Hate} {Speech}},
	url = {http://link.springer.com/10.1007/978-3-030-41421-4_7},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Twitter, the {Public} {Sphere}, and the {Chaos} of {Online} {Deliberation}},
	publisher = {Springer International Publishing},
	author = {Bhat, Prashanth and Klein, Ofra},
	editor = {Bouvier, Gwen and Rosenbaum, Judith E.},
	year = {2020},
	doi = {10.1007/978-3-030-41421-4_7},
	pages = {151--172},
}

@incollection{pruden_birds_2022,
	title = {Birds of a {Feather}: {A} {Comparative} {Analysis} of {White} {Supremacist} and {Violent} {Male} {Supremacist} {Discourses}},
	url = {https://link.springer.com/10.1007/978-3-030-99804-2_9},
	author = {Pruden, Meredith L. and Lokmanoglu, Ayse D. and Peterscheck, Anne and Veilleux-Lepage, Yannick},
	year = {2022},
	doi = {10.1007/978-3-030-99804-2_9},
	pages = {215--254},
}

@techreport{guhl_safe_2020,
	title = {A {Safe} {Space} to {Hate}: {White} {Supremacist} {Mobilisation} on {Telegram}},
	url = {www.isdglobal.org},
	author = {Guhl, Jakob and Davey, Jacob},
	year = {2020},
}

@article{marko_extremist_2022,
	title = {Extremist language in anti-{COVID}-19 conspiracy discourse on {Facebook}},
	issn = {1740-5904},
	url = {https://www.tandfonline.com/doi/full/10.1080/17405904.2022.2110134},
	doi = {10.1080/17405904.2022.2110134},
	journal = {Critical Discourse Studies},
	author = {Marko, Karoline},
	month = aug,
	year = {2022},
	pages = {1--20},
}

@article{brown_wwwhatecom_2009,
	title = {{WWW}.{HATE}.{COM}: {White} supremacist discourse on the internet and the construction of whiteness ideology},
	volume = {20},
	issn = {10646175},
	doi = {10.1080/10646170902869544},
	abstract = {The Internet connects people from different parts of the world in a way that no other technological innovation can. Extremists and hate mongrels use the Internet to access a potential audience of millions and to create a breeding ground for hate. In recent years, White supremacist organizations use chat rooms and Internet broadcasts to highlight a "racist double standard" and to promote racist ideologies. The influence of White supremacy is understudied and often ignored in communication studies on Whiteness and race. This study examines White supremacist discourse to show how it frames people of African descent using blatantly racist and offensive stereotypical overgeneralizations that set foundations of Whiteness ideology. I argue that this discourse aids our understanding of the contemporary constructions of both White and Black identities in the contemporary United States by showing how constructions of racist representations are integral to systems of White privilege. © Taylor \& Francis Group, LLC.},
	number = {2},
	journal = {Howard Journal of Communications},
	author = {Brown, Christopher},
	month = apr,
	year = {2009},
	keywords = {And White supremacy, Internet, Racial hierarchy, Whiteness},
	pages = {189--208},
}

@inproceedings{simons_bootstrapped_2020,
	title = {A {Bootstrapped} {Model} to {Detect} {Abuse} and {Intent} in {White} {Supremacist} {Corpora}},
	isbn = {978-1-72818-800-3},
	doi = {10.1109/ISI49825.2020.9280551},
	abstract = {Intelligence analysts face a difficult problem: distinguishing extremist rhetoric from potential extremist violence. Many are content to express abuse against some target group, but only a few indicate a willingness to engage in violence. We address this problem by building a predictive model for intent, bootstrapping from a seed set of intent words, and language templates expressing intent. We design both an n-gram and attention-based deep learner for intent and use them as colearners to improve both the basis for prediction and the predictions themselves. They converge to stable predictions in a few rounds. We merge predictions of intent with predictions of abusive language to detect posts that indicate a desire for violent action. We validate the predictions by comparing them to crowd-sourced labelling. The methodology can be applied to other linguistic properties for which a plausible starting point can be defined.},
	booktitle = {Proceedings - 2020 {IEEE} {International} {Conference} on {Intelligence} and {Security} {Informatics}, {ISI} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Simons, B. and Skillicorn, D. B.},
	month = nov,
	year = {2020},
}

@techreport{ferber_reading_2000,
	title = {{READING} {RIGHT}: {THE} {WESTERN} {TRADITION} {IN} {WHITE} {SUPREMACIST} {DISCOURSE}},
	author = {Ferber, Abby L and Kimmel, Michael},
	year = {2000},
	note = {Publication Title: Source: Sociological Focus
Volume: 33
Issue: 2},
	pages = {193--213},
}

@article{alatawi_detecting_2021,
	title = {Detecting {White} {Supremacist} {Hate} {Speech} {Using} {Domain} {Specific} {Word} {Embedding} with {Deep} {Learning} and {BERT}},
	volume = {9},
	issn = {21693536},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497095},
	doi = {10.1109/ACCESS.2021.3100435},
	abstract = {White supremacist hate speech is one of the most recently observed harmful content on social media. The critical influence of these radical groups is no longer limited to social media and can negatively affect society by promoting racial hatred and violence. Traditional channels of reporting hate speech have proved inadequate due to the tremendous explosion of information and the implicit nature of hate speech. Therefore, it is necessary to detect such speech automatically and in a timely manner. This research investigates the feasibility of automatically detecting white supremacist hate speech on Twitter using deep learning and natural language processing techniques. Two deep learning models are investigated in this research. The first approach utilizes a bidirectional Long Short-Term Memory (BiLSTM) model along with domain-specific word embeddings extracted from white supremacist corpus to capture the semantic of white supremacist slangs and coded words. The second approach utilizes one of the most recent language models, which is Bidirectional Encoder Representations from Transformers (BERT). The BiLSTM model achieved 0.75 F1-score and BERT reached a 0.80 F1-score. Both models are tested on a balanced dataset combined from Twitter and a Stormfront dataset compiled from white supremacist forum.},
	urldate = {2022-08-11},
	journal = {IEEE Access},
	author = {Alatawi, Hind S. and Alhothali, Areej M. and Moria, Kawthar M.},
	year = {2021},
	note = {arXiv: 2010.00357
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {BERT, NLP, Twitter, deep learning, hate speech, white supremacist},
	pages = {106363--106374},
}

@techreport{hartung_identifying_2017,
	title = {Identifying {Right}-{Wing} {Extremism} in {German} {Twitter} {Profiles}: a {Classification} {Approach}},
	abstract = {Social media platforms are used by an increasing number of extremist political actors for mobilization, recruiting or radicalization purposes. We propose a machine learning approach to support manual monitoring aiming at identifying right-wing extremist content in Ger-man Twitter profiles. We frame the task as profile classification, based on textual cues, traits of emotionality in language use, and linguistic patterns. A quantitative evaluation reveals a limited precision of 25 \% with a close-to-perfect recall of 95 \%. This leads to a considerable reduction of the workload of human analysts in detecting right-wing extremist users.},
	author = {Hartung, Matthias and Klinger, Roman and Schmidtke, Franziska and Vogel, Lars},
	year = {2017},
	keywords = {classification, extremism monitoring, social media},
}

@article{noauthor_handbook_nodate,
	title = {The {Handbook} of {Discourse} {Analysis} - 2015 - {Tannen} - {Critical} {Discourse} {Analysis}},
}

@article{nagai_oral_2022,
	title = {Oral {English} {Proficiency} {Tests}, {Interpretive} {Labor}, and the {Neoliberal} {University}},
	issn = {1055-1360},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jola.12374},
	doi = {10.1111/jola.12374},
	journal = {Journal of Linguistic Anthropology},
	author = {Nagai, Julia and Everhart, Edwin K.},
	month = jul,
	year = {2022},
}

@inproceedings{vail_toward_2018,
	title = {Toward objective, multifaceted characterization of psychotic disorders: {Lexical}, structural, and disfluency markers of spoken language},
	isbn = {978-1-4503-5692-3},
	doi = {10.1145/3242969.3243020},
	abstract = {Psychotic disorders are forms of severe mental illness characterized by abnormal social function and a general sense of disconnect with reality. The evaluation of such disorders is often complex, as their multifaceted nature is often difficult to quantify. Multimodal behavior analysis technologies have the potential to help address this need and supply timelier and more objective decision support tools in clinical settings. While written language and nonverbal behaviors have been previously studied, the present analysis takes the novel approach of examining the rarely-studied modality of spoken language of individuals with psychosis as naturally used in social, face-to-face interactions. Our analyses expose a series of language markers associated with psychotic symptom severity, as well as interesting interactions between them. In particular, we examine three facets of spoken language: (1) lexical markers, through a study of the function of words; (2) structural markers, through a study of grammatical fluency; and (3) disfluency markers, through a study of dialogue self-repair. Additionally, we develop predictive models of psychotic symptom severity, which achieve significant predictive power on both positive and negative psychotic symptom scales. These results constitute a significant step toward the design of future multimodal clinical decision support tools for computational phenotyping of mental illness.},
	booktitle = {{ICMI} 2018 - {Proceedings} of the 2018 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Vail, Alexandria K. and Baker, Justin T. and Liebson, Elizabeth and Morency, Louis Philippe},
	month = oct,
	year = {2018},
	pages = {170--178},
}

@article{girard_computational_2022,
	title = {Computational analysis of spoken language in acute psychosis and mania},
	volume = {245},
	issn = {15732509},
	doi = {10.1016/j.schres.2021.06.040},
	abstract = {Objectives: This study aimed to (1) determine the feasibility of collecting behavioral data from participants hospitalized with acute psychosis and (2) begin to evaluate the clinical information that can be computationally derived from such data. Methods: Behavioral data was collected across 99 sessions from 38 participants recruited from an inpatient psychiatric unit. Each session started with a semi-structured interview modeled on a typical “clinical rounds” encounter and included administration of the Positive and Negative Syndrome Scale (PANSS). Analysis: We quantified aspects of participants' verbal behavior during the interview using lexical, coherence, and disfluency features. We then used two complementary approaches to explore our second objective. The first approach used predictive models to estimate participants' PANSS scores from their language features. Our second approach used inferential models to quantify the relationships between individual language features and symptom measures. Results: Our predictive models showed promise but lacked sufficient data to achieve clinically useful accuracy. Our inferential models identified statistically significant relationships between numerous language features and symptom domains. Conclusion: Our interview recording procedures were well-tolerated and produced adequate data for transcription and analysis. The results of our inferential modeling suggest that automatic measurements of expressive language contain signals highly relevant to the assessment of psychosis. These findings establish the potential of measuring language during a clinical interview in a naturalistic setting and generate specific hypotheses that can be tested in future studies. This, in turn, will lead to more accurate modeling and better understanding of the relationships between expressive language and psychosis.},
	journal = {Schizophrenia Research},
	author = {Girard, Jeffrey M. and Vail, Alexandria K. and Liebenthal, Einat and Brown, Katrina and Kilciksiz, Can Misel and Pennant, Luciana and Liebson, Elizabeth and Öngür, Dost and Morency, Louis Philippe and Baker, Justin T.},
	month = jul,
	year = {2022},
	pmid = {34456131},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bipolar disorder, Language, Negative symptoms, Positive symptoms, Schizophrenia},
	pages = {97--115},
}

@article{althoff_large-scale_2016,
	title = {Large-scale {Analysis} of {Counseling} {Conversations}: {An} {Application} of {Natural} {Language} {Processing} to {Mental} {Health}},
	issn = {2307-387X},
	url = {http://arxiv.org/abs/1605.04462},
	doi = {10.1016/j.pec.2011.04.013.The},
	abstract = {Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.},
	author = {Althoff, Tim and Clark, Kevin and Leskovec, Jure},
	year = {2016},
	pmid = {7728691},
	note = {arXiv: 1605.04462
ISBN: 9781577356783},
}

@inproceedings{benton_multi-task_2017,
	title = {Multi-{Task} {Learning} for {Mental} {Health} using {Social} {Media} {Text}},
	volume = {1},
	isbn = {978-1-5108-3860-4},
	url = {http://www.aclweb.org/anthology/E17-1015},
	abstract = {We introduce initial groundwork for estimating suicide risk and mental health in a deep learning framework. By modeling multiple conditions, the system learns to make predictions about suicide risk and mental health at a low false positive rate. Conditions are modeled as tasks in a multi-task learning (MTL) framework, with gender prediction as an additional auxiliary task. We demonstrate the effectiveness of multi-task learning by comparison to a well-tuned single-task baseline with the same number of parameters. Our best MTL model predicts potential suicide attempt, as well as the presence of atypical mental health, with AUC {\textgreater} 0.8. We also find additional large improvements using multi-task learning on mental health tasks with limited training data.},
	booktitle = {15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics} ({EACL})},
	author = {Benton, Adrian and Mitchell, Margaret and Hovy, Dirk},
	year = {2017},
	note = {arXiv: 1712.03538},
	keywords = {benton2017multitask},
	pages = {152--162},
}

@techreport{pruden_maintaining_2021,
	title = {"{Maintaining} {Frame}" in the {Incelosphere}: {Mapping} the {Discourses}, {Representations} and {Geographies} of {Involuntary} {Celibates} {Online}},
	author = {Pruden, Meredith L},
	year = {2021},
}

@inproceedings{qian_hierarchical_2018,
	title = {Hierarchical {CVAE} for {Fine}-{Grained} {Hate} {Speech} {Classification}},
	abstract = {Existing work on automated hate speech detection typically focuses on binary classification or on differentiating among a small set of categories. In this paper, we propose a novel method on a fine-grained hate speech classification task, which focuses on differentiating among 40 hate groups of 13 different hate group categories. We first explore the Conditional Variational Autoencoder (CVAE) (Larsen et al., 2016; Sohn et al., 2015) as a discriminative model and then extend it to a hierarchical architecture to utilize the additional hate category information for more accurate prediction. Experimentally, we show that incorporating the hate category information for training can significantly improve the classification performance and our proposed model outperforms commonly-used discrimi-native models.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Qian, Jing and Elsherief, Mai and Belding, Elizabeth and Wang, William Yang},
	year = {2018},
	pages = {3550--3559},
}

@misc{fairclough_critical_2012,
	title = {Critical discourse analysis and analysis of argumentation},
	author = {Fairclough, Isabela and Fairclough, Normal},
	year = {2012},
	note = {Pages: 78-116
Publication Title: Political Discourse Analysis: A Method for Advanced Students},
}

@article{manaworapong_language_2022,
	title = {Language, gender, and patriarchy in {Mulan}: a diachronic analysis of a {Disney} {Princess} movie},
	volume = {9},
	issn = {2662-9992},
	url = {https://www.nature.com/articles/s41599-022-01244-y},
	doi = {10.1057/s41599-022-01244-y},
	abstract = {{\textless}p{\textgreater}Movies can implicitly promote social and ideological norms on a mass scale, making them powerful socialization agents, especially among children. However, Hollywood movies are no longer confined to the influence of American ideals, as media companies now have to consider the growing influence of markets such as China. With this in mind, this paper explores the portrayal of gender, power, and gendered roles across two versions of Disney’s Mulan (1998 and 2020). Specifically, it explores male-coded and female-coded characters’ talk for portrayals of gender and the enactment of assigned roles through conversational strategies and the content of talk. Findings indicate a subtle shift in the distribution of “dominant” discourse between the two versions, despite female-coded characters being framed as dutiful wives, brides-to-be, and/or mothers in both movies. Specifically, in Mulan 2020, male-coded characters are portrayed as more “feminine” through their talk, while female-coded characters—particularly Mulan and Xianniang—are portrayed as more “masculine”, further highlighting a recent trend for more nuanced portrayals of gender in Disney movies. Based on these results and others, the paper argues that the portrayal of gender in Mulan 2020, while still primarily associated with heteronormative roles in service of a patriarchal world, has undergone subtle changes that may reflect American and Chinese influences.{\textless}/p{\textgreater}},
	number = {1},
	journal = {Humanities and Social Sciences Communications},
	author = {Manaworapong, Pimpatchanok and Bowen, Neil Evan Jon Anthony},
	month = dec,
	year = {2022},
	pages = {224},
}

@article{Garg2018,
	title = {Word {Embeddings} {Quantify} 100 {Years} of {Gender} and {Ethnic} {Stereotypes}},
	volume = {115},
	issn = {0027-8424},
	doi = {10.1073/pnas.1720347115},
	abstract = {Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.},
	number = {16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	year = {2018},
	pmid = {29615513},
	note = {arXiv: 1711.08412
ISBN: 1720347115},
	keywords = {★},
	pages = {E3635--E3644},
}

@inproceedings{Field2019,
	title = {Contextual {Affective} {Analysis}: {A} {Case} {Study} of {People} {Portrayals} in {Online} \#{MeToo} {Stories}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/download/3358/3226/},
	abstract = {In October 2017, numerous women accused producer Harvey Weinstein of sexual harassment. Their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. These events are broadly referred to as the \#MeToo movement, named for the use of the hashtag "\#metoo" on social media platforms like Twitter and Facebook. The movement has widely been referred to as "empowering" because it has amplified the voices of previously unheard women over those of traditionally powerful men. In this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. Using a corpus of online media articles about the \#MeToo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. We show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. While we focus on media coverage of the \#MeToo movement, our method for contextual affective analysis readily generalizes to other domains.},
	urldate = {2022-07-25},
	booktitle = {Proc. {ICWSM}},
	author = {Field, Anjalie and Bhat, Gayatri and Tsvetkov, Yulia},
	year = {2019},
	note = {arXiv: 1904.04164},
}

@inproceedings{Wang2018,
	title = {It's going to be okay: {Measuring} {Access} to {Support} in {Online} {Communities}},
	url = {http://www.aclweb.org/anthology/D18-1004},
	abstract = {People use online platforms to seek out support for their informational and emotional needs. Here, we ask what effect does revealing one's gender have on receiving support. To answer this, we create (i) a new dataset and method for identifying supportive replies and (ii) new methods for inferring gender from text and name. We apply these methods to create a new massive corpus of 102M online interactions with gender-labeled users, each rated by degree of supportiveness. Our analysis shows widespread and consistent disparity in support: identifying as a woman is associated with higher rates of support-but also higher rates of disparagement.},
	booktitle = {Emnlp},
	author = {Wang, Zijian and Jurgens, David},
	year = {2018},
	keywords = {★},
	pages = {33--45},
}

@inproceedings{blodgett_language_2020,
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of "{Bias}" in {NLP}},
	doi = {10.18653/v1/2020.acl-main.485},
	abstract = {We survey 146 papers analyzing "bias" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing "bias" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating "bias" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing "bias" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of "bias"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé, Hal and Wallach, Hanna},
	year = {2020},
	note = {arXiv: 2005.14050
Issue: c},
	keywords = {★},
	pages = {5454--5476},
}

@inproceedings{danescu-niculescu-mizil_no_2013,
	title = {No {Country} for {Old} {Members} : {User} {Lifecycle} and {Linguistic} {Change} in {Online} {Communities}},
	isbn = {978-1-4503-2035-1},
	doi = {10.1145/2488388.2488416},
	abstract = {Vibrant online communities are in constant flux. As members join and depart, the interactional norms evolve, stimulating fur- ther changes to the membership and its social dynamics. Linguistic change—in the sense of innovation that becomes accepted as the norm—is essential to this dynamic process: it both facilitates indi- vidual expression and fosters the emergence of a collective identity. We propose a framework for tracking linguistic change as it hap- pens and for understanding how specific users react to these evolv- ing norms. By applying this framework to two large online commu- nities we show that users follow a determined two-stage lifecycle with respect to their susceptibility to linguistic change: a linguisti- cally innovative learning phase in which users adopt the language of the community followed by a conservative phase in which users stop changing and the evolving community norms pass them by. Building on this observation, we show how this framework can be used to detect, early in a user’s career, how long she will stay active in the community. Thus, this work has practical signifi- cance for those who design and maintain online communities. It also yields new theoretical insights into the evolution of linguis- tic norms and the complex interplay between community-level and individual-level linguistic change.},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	author = {Danescu-Niculescu-Mizil, Cristian and West, Robert and Jurafsky, Dan and Potts, Christopher},
	year = {2013},
	note = {ISSN: 0165-0157},
	keywords = {community norms, conventions, language, lifecycle, linguistic change, reviews, social influence, user abandonment, ★},
	pages = {307--317},
}

@article{siegel_trumping_2021,
	title = {Trumping {Hate} on {Twitter}? {Online} {Hate} {Speech} in the 2016 {U}.{S}. {Election} {Campaign} and its {Aftermath}},
	volume = {16},
	url = {https://www.nowpublishers.com/article/Details/QJPS-19045},
	urldate = {2022-07-21},
	journal = {Quarterly Journal of Political Science},
	author = {Siegel, Alexandra A. and Nikitin, , Evgenii and Barberá, Pablo and Sterling, Joanna and Pullen, Bethany and Bonneau, Richard and Nagler, Jonathan and Tucker, Joshua A.},
	year = {2021},
	pages = {71--104},
}

@incollection{perry_white_2016,
	title = {White pride worldwide: {Constructing} global identities online},
	url = {https://www.google.com/books/edition/The_Globalization_of_Hate/mpcUDAAAQBAJ?hl=en&gbpv=1&dq=B.+Perry+and+R.+Scrivens,+%E2%80%9CWhite+pride+worldwide:+Constructing+global+identities+online,%E2%80%9DJ.+Schweppe+and+M+Walters+(eds.),+The+Globalisationof++Hate:++Internationalising++Hate++Crime%3F++New++York:++Oxford++UniversityPress,+pp.65%E2%80%9378,+2016.&pg=PA65&printsec=frontcover},
	urldate = {2022-07-21},
	author = {Perry, Barbara and Scrivens, Ryan},
	year = {2016},
}

@article{duffy_web_2003,
	title = {Web of hate: {A} fantasy theme analysis of the rhetorical vision of hate groups online},
	volume = {27},
	issn = {01968599},
	doi = {10.1177/0196859903252850},
	abstract = {The development and growth of the Internet and World Wide Web have provided a new and persuasive medium for business, education, and social interaction. Examination of hate group Web sites reveal world views that cast organizations' aims in mainstream and traditionally American terms. This article uses Ernest Bormann's fantasy theme analysis to examine hate group Web sites as a means to understand the world views expressed and the resulting potential for persuasion. © 2003 Sage Publications.},
	number = {3},
	urldate = {2022-07-21},
	journal = {Journal of Communication Inquiry},
	author = {Duffy, Margaret E.},
	year = {2003},
	keywords = {Dramatism, Hate speech, Internet, Racism, Symbolic convergence theory},
	pages = {291--312},
}

@inproceedings{berglind_levels_2019,
	title = {Levels of hate in online environments},
	isbn = {978-1-4503-6868-1},
	doi = {10.1145/3341161.3343521},
	abstract = {Hate speech in online environments is a severe problem for many reasons. The space for reasoning and argumentation shrinks, individuals refrain from expressing their opinions, and polarization of views increases. Hate speech contributes to a climate where threats and even violence are increasingly regarded as acceptable. The amount and the intensity of hate expressions vary greatly between different digital environments. To analyze the level of hate in a given online environment, to study the development over time and to compare the level of hate within online environments we have developed the notion of a hate level. The hate level encapsulates the level of hate in a given digital environment. We present methods to automatically determine the hate level, utilizing transfer learning on pre-trained language models with annotated data to create automated hate detectors. We evaluate our approaches on a set of websites and discussion forums.},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}, {ASONAM} 2019},
	publisher = {Association for Computing Machinery, Inc},
	author = {Berglind, Tor and Pelzer, Björn and Kaati, Lisa},
	month = aug,
	year = {2019},
	pages = {842--847},
}

@article{dias_oliva_fighting_2021,
	title = {Fighting {Hate} {Speech}, {Silencing} {Drag} {Queens}? {Artificial} {Intelligence} in {Content} {Moderation} and {Risks} to {LGBTQ} {Voices} {Online}},
	volume = {25},
	issn = {19364822},
	doi = {10.1007/s12119-020-09790-w},
	abstract = {Companies operating internet platforms are developing artificial intelligence tools for content moderation purposes. This paper discusses technologies developed to measure the ‘toxicity’ of text-based content. The research builds upon queer linguistic studies that have indicated the use of ‘mock impoliteness’ as a form of interaction employed by LGBTQ people to cope with hostility. Automated analyses that disregard such a pro-social function may, contrary to their intended design, actually reinforce harmful biases. This paper uses ‘Perspective’, an AI technology developed by Jigsaw (formerly Google Ideas), to measure the levels of toxicity of tweets from prominent drag queens in the United States. The research indicated that Perspective considered a significant number of drag queen Twitter accounts to have higher levels of toxicity than white nationalists. The qualitative analysis revealed that Perspective was not able to properly consider social context when measuring toxicity levels and failed to recognize cases in which words, that might conventionally be seen as offensive, conveyed different meanings in LGBTQ speech.},
	number = {2},
	journal = {Sexuality and Culture},
	author = {Dias Oliva, Thiago and Antonialli, Dennys Marcelo and Gomes, Alessandra},
	month = apr,
	year = {2021},
	note = {Publisher: Springer},
	keywords = {Artificial intelligence, Content moderation, Drag queens, Hate speech, Queer linguistics, Toxicity},
	pages = {700--732},
}

@techreport{magu_detecting_2017,
	title = {Detecting the {Hate} {Code} on {Social} {Media}},
	url = {www.aaai.org},
	abstract = {Social media has become an indispensable part of the everyday lives of millions of people around the world. It provides a platform for expressing opinions and beliefs, communicated to a massive audience. However, this ease with which people can express themselves has also allowed for the large scale spread of propaganda and hate speech. To prevent violating the abuse policies of social media platforms and also to avoid detection by automatic systems like Google's Conversation AI, racists have begun to use a code (a movement termed Operation Google). This involves substituting references to communities by benign words that seem out of context, in hate filled posts or Tweets. For example, users have used the words Googles and Bings to represent the African-American and Asian communities, respectively. By generating the list of users who post such content, we move a step forward from classifying tweets by allowing us to study the usage pattern of these concentrated set of users.},
	author = {Magu, Rijul and Joshi, Kshitij and Luo, Jiebo},
	year = {2017},
	keywords = {Poster Papers},
}

@inproceedings{oussalah_detecting_2018,
	title = {On detecting online radicalization using natural language processing},
	volume = {11315 LNCS},
	isbn = {978-3-030-03495-5},
	doi = {10.1007/978-3-030-03496-2_4},
	abstract = {This paper suggests a new approach for radicalization detection using natural language processing techniques. Although, intuitively speaking, detection of radicalization from only language cues is not trivial and very debatable, the advances in computational linguistics together with the availability of large corpus that allows application of machine learning techniques opens us new horizons in the field. This paper advocates a two stage detection approach where in the first phase a radicalization score is obtained by analyzing mainly inherent characteristics of negative sentiment. In the second phase, a machine learning approach based on hybrid KNN-SVM and a variety of features, which include 1, 2 and 3-g, personality traits, emotions, as well as other linguistic and network related features were employed. The approach is validated using both Twitter and Tumblr dataset.},
	urldate = {2022-07-21},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	publisher = {Springer Verlag},
	author = {Oussalah, Mourad and Faroughian, F. and Kostakos, Panos},
	year = {2018},
	note = {ISSN: 16113349},
	keywords = {Machine learning, Natural language processing, Radicalization},
	pages = {21--27},
}

@inproceedings{figea_measuring_2016,
	title = {Measuring online affects in a white supremacy forum},
	isbn = {978-1-5090-3865-7},
	doi = {10.1109/ISI.2016.7745448},
	abstract = {Since the inception of the World Wide Web, security agencies, researchers, and analysts have focused much of their attention on the sentiment found on hate-inspired web-forums. Here, one of their goals has been to detect and measure users' affects that are expressed in the forums as well as identify how users' affects change over time. Manual inspection has been one way to do this; however, as the number of discussion posts and sub-forums increase, there has been a growing need for an automated system that can assist humans in their analysis. The aim of this paper, then, is to detect and measure a number of affects expressed in written text on Stormfront.org, the most visited hate forum on the Web. To do this, we used a machine learning approach where we trained a model to recognize affects on three sub-forums: Ideology and Philosophy, For Stormfront Ladies Only, and Stormfront Ireland. The training data consisted of manual annotated data and the affects we focused on were racism, aggression, and worries. Results indicate that even though measuring affects is a subjective process, machine learning is a promising way forward to analyze and measure the presence of different affects on hate forums.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligence} and {Security} {Informatics}: {Cybersecurity} and {Big} {Data}, {ISI} 2016},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Figea, Leo and Kaati, Lisa and Scrivens, Ryan},
	month = nov,
	year = {2016},
	pages = {85--90},
}

@inproceedings{DeGibert2019,
	title = {Hate {Speech} {Dataset} from a {White} {Supremacy} {Forum}},
	doi = {10.18653/v1/w18-5102},
	abstract = {Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic. Due to the massive rise of user-generated web content on social media, the amount of hate speech is also steadily increasing. Over the past years, interest in online hate speech detection and, particularly, the automation of this task has continuously grown, along with the societal impact of the phenomenon. This paper describes a hate speech dataset composed of thousands of sentences manually labelled as containing hate speech or not. The sentences have been extracted from Stormfront, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting dataset and several baseline experiments with different classification models. The dataset is publicly available.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Abusive} {Language} {Online} ({ALW2})},
	author = {de Gibert, Ona and Perez, Naiara and García-Pablos, Aitor and Cuadros, Montse},
	year = {2019},
	pages = {11--20},
}

@techreport{marcellino_foreign_nodate,
	title = {Foreign {Interference} in the 2020 {Election}: {Tools} for {Detecting} {Online} {Election} {Interference}},
	abstract = {F oreign election interference is a serious threat to U.S. democratic processes, something that became visible and received public attention in the wake of the 2016 U.S. general election. In the aftermath of that election, it became clear that agents acting on behalf of the Russian government went online and engaged in a very sophisticated malign information effort meant to sow chaos and inflame partisan divides in the U.S. electorate (Marcellino, Cox, et al., 2020). Because of the seriousness of the threat and concerns that such threats are likely to be ongoing, improving the detection of such efforts is critical. That desire to help bolster our democratic processes from illicit interference motivated our current study, which attempted to pilot improved detection methods prior to the 2020 election-we wanted to detect any such efforts in time to provide warning rather than post hoc. We found convincing evidence of a coordinated effort, likely foreign, to use social media to attempt to influence the U.S. presidential election. We examined two kinds of suspicious accounts working in concert toward this end: The first kind is trolls: fake personas spreading a variety of hyperpartisan themes. 1 The second kind is superconnectors: highly networked accounts that can spread messages effectively and quickly. Both kinds of accounts cluster only in certain online communities, engage both liberal and conservative audiences, and exacerbate political divisions in the United States. This report is the second of a four-part series (Figure 1) for the California Governor's Office of Emergency Services designed to help analyze, forecast, and mitigate threats by foreign actors targeting local, state, and national C O R P O R A T I O N KEY FINDINGS ■ We found credible evidence of interference in the 2020 election on Twitter. ■ This interference includes posts from troll accounts (fake personas spreading hyperpartisan themes) and super-connector accounts that appear designed to spread information. ■ This interference effort intends to sow division and undermine confidence in American democracy. ■ This interference serves Russia's interests and matches Russia's interference playbook. ■ Our methods can help identify online interference by foreign adversaries, allowing for proactive measures.},
	author = {Marcellino, William and Johnson, Christian and Posard, Marek N and Helmus, Todd C},
}

@book{marcellino_detecting_nodate,
	title = {Detecting conspiracy theories on social media : improving machine learning to detect and understand online conspiracy theories},
	isbn = {978-1-977406-89-7},
	abstract = {Conspiracy theories circulated online via social media contribute to a shift in public discourse away from facts and analysis and can contribute to direct public harm. Social media platforms face a difficult technical and policy challenge in trying to mitigate harm from online conspiracy theory language. As part of Google's Jigsaw unit's effort to confront emerging threats and incubate new technology to help create a safer world, RAND researchers conducted a modeling effort to improve machine-learning (ML) technology for detecting conspiracy theory language. They developed a hybrid model using linguistic and rhetorical theory to boost performance. They also aimed to synthesize existing research on conspiracy theories using new insight from this improved modeling effort. This report describes the results of that effort and offers recommendations to counter the effects of conspiracy theories that are spread online. Introduction: Detecting and Understanding Online Conspiracy Language -- Making Sense of Conspiracy Theories -- Modeling Conspiracy Theories: A Hybrid Approach -- Conclusion and Recommendations -- Appendix A: Data and Methodology -- Appendix B: Stance: Text Analysis and Machine Learning.},
	author = {Marcellino, William and Helmus, Todd C. and Kerrigan, Joshua. and {International Security and Defense Policy Center.} and {Google (Firm)} and {Rand Corporation.}},
}

@incollection{du2007stance,
	title = {The stance triangle},
	booktitle = {Stancetaking in discourse: {Subjectivity}, evaluation, interaction},
	publisher = {Amsterdam: John Benjamins Publishing Company},
	author = {Du Bois, John W},
	editor = {Englebretson, Robert},
	year = {2007},
	pages = {139--182},
}

@article{kim_effects_2020,
	title = {Effects of social grooming on incivility in {COVID}-19},
	volume = {23},
	issn = {21522723},
	doi = {10.1089/cyber.2020.0201},
	abstract = {This study implements a computer-assisted content analysis to identify which social grooming factors reduce social media users' incivility when commenting or posting about the COVID-19 situation in South Korea. In addition, this study conducts semantic network analysis to interpret qualitatively how people express their thoughts. The findings suggest that social network size is a negative predictor of incivility. Moreover, Twitter users who have built larger networks and gained positive responses from others are less likely to use uncivil language. Lastly, linguistic choice among users is different depending on the size of their social network.},
	number = {8},
	journal = {Cyberpsychology, Behavior, and Social Networking},
	author = {Kim, Bumsoo},
	month = aug,
	year = {2020},
	pmid = {32267163},
	note = {Publisher: Mary Ann Liebert Inc.},
	keywords = {COVID-19, Corona19, big data, incivility, network analysis, social grooming},
	pages = {519--525},
}

@article{drieger_semantic_2013,
	title = {Semantic {Network} {Analysis} as a {Method} for {Visual} {Text} {Analytics}},
	volume = {79},
	issn = {18770428},
	doi = {10.1016/j.sbspro.2013.05.053},
	abstract = {This paper proposes an approach on a method for visual text analytics to support knowledge building, analytical reasoning and explorative analysis. For this purpose we use semantic network models that are automatically retrieved from unstructured text data using a parametric k-next-neighborhood model. Semantic networks are analyzed with methods of network analysis to gain quantitative and qualitative insights. Quantitative metrics can support the qualitative analysis and exploration of semantic structures. We discuss theoretical presuppositions regarding the text modeling with semantic networks to provide a basis for subsequent semantic network analysis. By presenting a systematic overview of basic network elements and their qualitative meaning for semantic network analysis, we describe exploration strategies that can support analysts to make sense of a given network. As a proof of concept, we illustrate the proposed method by an exemplary analysis of a wikipedia article using a visual text analytics system that leverages semantic network visualization for exploration and analysis.},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Drieger, Philipp},
	month = jun,
	year = {2013},
	note = {Publisher: Elsevier BV},
	pages = {4--17},
}

@article{carley_communication_1993,
	title = {Communication {Theory} {Three}: {Three} {Semantic} {Connectivity}: {An} {Approach} for {Analyzing} {Symbols} in {Semantic} {Networks}},
	abstract = {W e argue that the notions of "'symbol" and "symbolic connectivity" can be rigorously developed both from the point of view of the theoretical literature on the symbol and from the point of view of semantic network theory. The theoretical literature, inspired mainly by the literary metaphor, typically takes interpretive density as the chief dimension underlying symbolic expression. Density measures have also dominated the analysis of semantic networks. There is now a sizeable amount of work on the generation of such networks from linguistic data. The majority of that work locates the network and displays it. When attempts are made to analyze the network, the focus is typically on the density (i.e., the number of links) of particular concepts (which serve as the nodes) within the network and on the inferences that can be made about the communicative prominence of such concepts in light of their density. While density is a useful way of analyzing the communicative "connectivity" of a symbol in a message, it provides only one dimension for analyzing connectivity within a semantic network. In this article we offer two further dimensions-conductivity and consensus-with which to analyze semantic networks for connectivity. W e illustrate a typology based on these three dimensions. These dimensions and the associated typology form a useful conceptual device that enables the researcher both to specify and differentiate semantic objects within a rich typology for a given domain of analysis. W e show this device at work by applying the dimensions and typology to different communication contexts and by discussing other possible domains where they can be applied.},
	author = {Carley, Kathleen M and Kaufer, David S},
	year = {1993},
}

@article{eddington_communicative_2018,
	title = {The {Communicative} {Constitution} of {Hate} {Organizations} {Online}: {A} {Semantic} {Network} {Analysis} of “{Make} {America} {Great} {Again}”},
	volume = {4},
	issn = {20563051},
	doi = {10.1177/2056305118790763},
	abstract = {In the context of the 2016 U.S. Presidential Election, President Donald Trump’s use of Twitter to connect with followers and supporters created unprecedented access to Trump’s online political campaign. In using the campaign slogan, “Make America Great Again” (or its acronym “MAGA”), Trump communicatively organized and controlled media systems by offering his followers an opportunity to connect with his campaign through the discursive hashtag. In effect, the strategic use of these networks over time communicatively constituted an effective and winning political organization; however, Trump’s political organization was not without connections to far-right and hate groups that coalesced in and around the hashtag. Semantic network analyses uncovered how the textual nature of \#MAGA organized connections between hashtags, and, in doing so, exposed connections to overtly White supremacist groups within the United States and the United Kingdom throughout late November 2016. Cluster analyses further uncovered semantic connections to White supremacist and White nationalist groups throughout the hashtag networks connected to the central slogan of Trump’s presidential campaign. Theoretically, these findings contribute to the ways in which hashtag networks show how Trump’s support developed and united around particular organizing processes and White nationalist language, and provide insights into how these networks discursively create and connect White supremacists’ organizations to Trump’s campaign.},
	number = {3},
	journal = {Social Media and Society},
	author = {Eddington, Sean M.},
	month = jul,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Twitter, communicative constitutions of organizations, political discourse, semantic network analysis, social networking},
}

@inproceedings{beatty_graph-based_2020,
	title = {Graph-{Based} {Methods} to {Detect} {Hate} {Speech} {Diffusion} on {Twitter}},
	isbn = {978-1-72811-056-1},
	doi = {10.1109/ASONAM49781.2020.9381473},
	abstract = {In this paper, we investigate models to detect the spread of hate speech on Twitter based on its diffusion in the network graph. We experiment with a dataset of 10,000 tweets manually labelled as hate speech or not and show that classification based solely on the sharing graph yields strong F1 scores for our task and high hate speech detection precision. We also highlight the vulnerability of existing textual hate speech detection methods to adversarial attacks and demonstrate that while our methods do not outperform state-of-the-art text models, graph-based models provide robust detection mechanisms and are able to detect instances of hate speech that fool text classifiers. We find that graph convolutional networks produce the strongest hate speech F1 score of 0.58 and find other success with kernel methods. Finally, we also consider the effects of automated bots in the sharing of hate speech content and find they are insignificant in our experiments.},
	booktitle = {Proceedings of the 2020 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}, {ASONAM} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Beatty, Matthew},
	month = dec,
	year = {2020},
	keywords = {Twitter, graph classification, graph kernels, graph mining, hate speech},
	pages = {502--506},
}

@article{mathew_hate_2020,
	title = {Hate begets {Hate}: {A} {Temporal} {Study} of {Hate} {Speech}},
	volume = {4},
	issn = {25730142},
	doi = {10.1145/3415163},
	abstract = {With the ongoing debate on 'freedom of speech' vs. 'hate speech,' there is an urgent need to carefully understand the consequences of the inevitable culmination of the two, i.e., 'freedom of hate speech' over time. An ideal scenario to understand this would be to observe the effects of hate speech in an (almost) unrestricted environment. Hence, we perform the first temporal analysis of hate speech on Gab.com, a social media site with very loose moderation policy. We first generate temporal snapshots of Gab from millions of posts and users. Using these temporal snapshots, we compute an activity vector based on DeGroot model to identify hateful users. The amount of hate speech in Gab is steadily increasing and the new users are becoming hateful at an increased and faster rate. Further, our analysis analysis reveals that the hate users are occupying the prominent positions in the Gab network. Also, the language used by the community as a whole seem to correlate more with that of the hateful users as compared to the non-hateful ones. We discuss how, many crucial design questions in CSCW open up from our work.},
	number = {CSCW2},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Mathew, Binny and Illendula, Anurag and Saha, Punyajoy and Sarkar, Soumya and Goyal, Pawan and Mukherjee, Animesh},
	month = oct,
	year = {2020},
	note = {arXiv: 1909.10966
Publisher: Association for Computing Machinery},
	keywords = {degroot, freedom of speech, gab, hate speech, language analysis, moderation, temporal analysis},
}

@article{evkoski_retweet_2022,
	title = {Retweet communities reveal the main sources of hate speech},
	volume = {17},
	issn = {19326203},
	doi = {10.1371/journal.pone.0265602},
	abstract = {We address a challenging problem of identifying main sources of hate speech on Twitter. On one hand, we carefully annotate a large set of tweets for hate speech, and deploy advanced deep learning to produce high quality hate speech classification models. On the other hand, we create retweet networks, detect communities and monitor their evolution through time. This combined approach is applied to three years of Slovenian Twitter data. We report a number of interesting results. Hate speech is dominated by offensive tweets, related to political and ideological issues. The share of unacceptable tweets is moderately increasing with time, from the initial 20\% to 30\% by the end of 2020. Unacceptable tweets are retweeted significantly more often than acceptable tweets. About 60\% of unacceptable tweets are produced by a single right-wing community of only moderate size. Institutional Twitter accounts and media accounts post significantly less unacceptable tweets than individual accounts. In fact, the main sources of unacceptable tweets are anonymous accounts, and accounts that were suspended or closed during the years 2018-2020.},
	number = {3 March},
	journal = {PLoS ONE},
	author = {Evkoski, Bojan and Pelicon, Andraž and Mozetič, Igor and Ljubešić, Nikola and Novak, Petra Kralj},
	month = mar,
	year = {2022},
	pmid = {35298556},
	note = {arXiv: 2105.14898
Publisher: Public Library of Science},
}

@inproceedings{chatzakou_mean_2017,
	title = {Mean birds: {Detecting} aggression and bullying on {Twitter}},
	isbn = {978-1-4503-4896-6},
	doi = {10.1145/3091478.3091487},
	abstract = {In recent years, bullying and aggression against social media users have grown significantly, causing serious consequences to victims of all demographics. Nowadays, cyberbullying affects more than half of young social media users worldwide, suffering from prolonged and/or coordinated digital harassment. Also, tools and technologies geared to understand and mitigate it are scarce and mostly ineffective. In this paper, we present a principled and scalable approach to detect bullying and aggressive behavior on Twitter. We propose a robust methodology for extracting text, user, and network-based attributes, studying the properties of bullies and aggressors, and what features distinguish them from regular users. We find that bullies post less, participate in fewer online communities, and are less popular than normal users. Aggressors are relatively popular and tend to include more negativity in their posts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3 months, and show that machine learning classification algorithms can accurately detect users exhibiting bullying and aggressive behavior, with over 90\% AUC.},
	urldate = {2022-03-22},
	booktitle = {{WebSci} 2017 - {Proceedings} of the 2017 {ACM} {Web} {Science} {Conference}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Chatzakou, Despoina and Kourtellis, Nicolas and Blackburn, Jeremy and De Cristofaro, Emiliano and Stringhini, Gianluca and Vakali, Athena},
	month = jun,
	year = {2017},
	note = {arXiv: 1702.06877},
	pages = {13--22},
}

@inproceedings{papegnies_graph-based_2017,
	address = {Cham},
	title = {Graph-{Based} {Features} for {Automatic} {OnlineAbuse} {Detection}},
	volume = {10583},
	isbn = {978-3-319-68455-0},
	url = {http://link.springer.com/10.1007/978-3-319-68456-7},
	doi = {10.1007/978-3-319-68456-7},
	publisher = {Springer International Publishing},
	author = {Papegnies, Etienne and Labatut, Vincent and Dufour, Richard and Linarès, Georges},
	editor = {Camelin, Nathalie and Estève, Yannick and Martín-Vide, Carlos},
	year = {2017},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{hosseinmardi_detection_2015,
	title = {Detection of {Cyberbullying} {Incidents} on the {Instagram} {Social} {Network}},
	url = {http://arxiv.org/abs/1503.03909},
	abstract = {Cyberbullying is a growing problem affecting more than half of all American teens. The main goal of this paper is to investigate fundamentally new approaches to understand and automatically detect incidents of cyberbullying over images in Instagram, a media-based mobile social network. To this end, we have collected a sample Instagram data set consisting of images and their associated comments, and designed a labeling study for cyberbullying as well as image content using human labelers at the crowd-sourced Crowdflower Web site. An analysis of the labeled data is then presented, including a study of correlations between different features and cyberbullying as well as cyberaggression. Using the labeled data, we further design and evaluate the accuracy of a classifier to automatically detect incidents of cyberbullying.},
	author = {Hosseinmardi, Homa and Mattson, Sabrina Arredondo and Rafiq, Rahat Ibn and Han, Richard and Lv, Qin and Mishra, Shivakant},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.03909},
}

@inproceedings{hosseinmardi_towards_2014,
	title = {Towards understanding cyberbullying behavior in a semi-anonymous social network},
	isbn = {978-1-4799-5877-1},
	doi = {10.1109/ASONAM.2014.6921591},
	abstract = {Cyberbullying has emerged as an important and growing social problem, wherein people use online social networks and mobile phones to bully victims with offensive text, images, audio and video on a 24/7 basis. This paper studies negative user behavior in the Ask.fm social network, a popular new site that has led to many cases of cyberbullying, some leading to suicidal behavior.We examine the occurrence of negative words in Ask.fm's question+answer profiles along with the social network of 'likes' of questions+answers. We also examine properties of users with 'cutting' behavior in this social network.},
	booktitle = {{ASONAM} 2014 - {Proceedings} of the 2014 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Hosseinmardi, Homa and Han, Richard and Lv, Qin and Mishra, Shivakant and Ghasemianlangroodi, Amir},
	month = oct,
	year = {2014},
	note = {arXiv: 1404.3839},
	pages = {244--252},
}

@inproceedings{ribeiro_characterizing_2018,
	title = {Characterizing and {Detecting} {Hateful} {Users} on {Twitter}},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/download/17837/17079},
	abstract = {Current approaches to characterize and detect hate speech focus on content posted in Online Social Networks (OSNs). They face shortcomings to get the full picture of hate speech due to its subjectivity and the noisiness of OSN text. This work partially addresses these issues by shifting the focus towards users. We obtain a sample of Twitter's retweet graph with 100, 386 users and annotate 4, 972 as hateful or normal , and also find 668 users suspended after 4 months. Our analysis shows that hateful/suspended users differ from nor-mal/active ones in terms of their activity patterns, word usage and network structure. Exploiting Twitter's network of connections, we find that a node embedding algorithm out-performs content-based approaches for detecting both hateful and suspended users. Overall, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue.},
	urldate = {2022-03-22},
	booktitle = {Proceedings of the {Twelfth} {International} {AAAI} {Conference} on {Web} and {Social} {Media} ({ICWSM} 2018)},
	author = {Ribeiro, Manoel Horta and Calais, Pedro H and Santos, Yuri A and Almeida, Virgílio A F and Meira, Wagner},
	year = {2018},
	keywords = {Poster Papers},
	pages = {676--679},
}

@inproceedings{mishra_author_2018,
	title = {Author {Profiling} for {Abuse} {Detection}},
	abstract = {The rapid growth of social media in recent years has fed into some highly undesirable phenomena such as proliferation of hateful and offensive language on the Internet. Previous research suggests that such abusive content tends to come from users who share a set of common stereotypes and form communities around them. The current state-of-the-art approaches to abuse detection are oblivious to user and community information and rely entirely on textual (i.e., lexical and semantic) cues. In this paper, we propose a novel approach to this problem that incorporates community-based profiling features of Twitter users. Experimenting with a dataset of 16k tweets, we show that our methods significantly outperform the current state of the art in abuse detection. Further, we conduct a qualitative analysis of model characteristics. We release our code, pre-trained models and all the resources used in the public domain.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	author = {Mishra, Pushkar and Tredici, Marco Del and Yannakoudakis, Helen and Shutova, Ekaterina},
	year = {2018},
	pages = {1088--1098},
}

@inproceedings{del_tredici_you_2019,
	title = {You {Shall} {Know} a {User} by the {Company} {It} {Keeps}: {Dynamic} {Representations} for {Social} {Media} {Users} in {NLP}},
	abstract = {Information about individuals can help to better understand what they say, particularly in social media where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in sociolinguistics, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a model based on Graph Attention Networks that captures this observation. It dynamically explores the social graph of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processingand} the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Del Tredici, Marco and Marcheggiani, Diego and Schulte im Walde, Sabine and Fernández, Raquel},
	year = {2019},
	pages = {4707--4717},
}

@article{pereira-kohatsu_detecting_2019,
	title = {Detecting and monitoring hate speech in twitter},
	volume = {19},
	issn = {14248220},
	doi = {10.3390/s19214654},
	abstract = {Social Media are sensors in the real world that can be used to measure the pulse of societies. However, the massive and unfiltered feed of messages posted in social media is a phenomenon that nowadays raises social alarms, especially when these messages contain hate speech targeted to a specific individual or group. In this context, governments and non-governmental organizations (NGOs) are concerned about the possible negative impact that these messages can have on individuals or on the society. In this paper, we present HaterNet, an intelligent system currently being used by the Spanish National Office Against Hate Crimes of the Spanish State Secretariat for Security that identifies and monitors the evolution of hate speech in Twitter. The contributions of this research are many-fold: (1) It introduces the first intelligent system that monitors and visualizes, using social network analysis techniques, hate speech in Social Media. (2) It introduces a novel public dataset on hate speech in Spanish consisting of 6000 expert-labeled tweets. (3) It compares several classification approaches based on different document representation strategies and text classification models. (4) The best approach consists of a combination of a LTSM+MLP neural network that takes as input the tweet’s word, emoji, and expression tokens’ embeddings enriched by the tf-idf, and obtains an area under the curve (AUC) of 0.828 on our dataset, outperforming previous methods presented in the literature.},
	number = {21},
	journal = {Sensors (Switzerland)},
	author = {Pereira-Kohatsu, Juan Carlos and Quijano-Sánchez, Lara and Liberatore, Federico and Camacho-Collados, Miguel},
	month = nov,
	year = {2019},
	pmid = {31717760},
	note = {Publisher: MDPI AG},
	keywords = {Hate crime, Predictive policing, Sentiment analysis, Social network analysis, Text classification, Twitter},
}

@inproceedings{mathew_spread_2019,
	title = {Spread of {Hate} {Speech} in {Online} {Social} {Media}},
	isbn = {978-1-4503-6202-3},
	doi = {10.1145/3292522.3326034},
	abstract = {Hate speech is considered to be one of the major issues currently plaguing the online social media. With online hate speech culminating in gruesome scenarios like the Rohingya genocide in Myanmar, anti-Muslim mob violence in Sri Lanka, and the Pittsburgh synagogue shooting, there is a dire need to understand the dynamics of user interaction that facilitate the spread of such hateful content. In this paper, we perform the first study that looks into the diffusion dynamics of the posts made by hateful and non-hateful users on Gab1. We collect a massive dataset of 341K users with 21M posts and investigate the diffusion of the posts generated by hateful and non-hateful users. We observe that the content generated by the hateful users tend to spread faster, farther and reach a much wider audience as compared to the content generated by normal users. We further analyze the hateful and non-hateful users on the basis of their account and network characteristics. An important finding is that the hateful users are far more densely connected among themselves. Overall, our study provides the first cross-sectional view of how hateful users diffuse hate content in online social media.},
	booktitle = {{WebSci} 2019 - {Proceedings} of the 11th {ACM} {Conference} on {Web} {Science}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Mathew, Binny and Dutt, Ritam and Goyal, Pawan and Mukherjee, Animesh},
	month = jun,
	year = {2019},
	note = {arXiv: 1812.01693},
	keywords = {De-Groot Model, Gab, Hate Speech, Information Diffusion, Online Social Media},
	pages = {173--182},
}

@inproceedings{mossie_social_2018,
	title = {Social {Network} {Hate} {Speech} {Detection} for {Amharic} {Language}},
	doi = {10.5121/csit.2018.80604},
	abstract = {The anonymity of social networks makes it attractive for hate speech to mask their criminal activities online posing a challenge to the world and in particular Ethiopia. With this ever-increasing volume of social media data, hate speech identification becomes a challenge in aggravating conflict between citizens of nations. The high rate of production, has become difficult to collect, store and analyze such big data using traditional detection methods. This paper proposed the application of apache spark in hate speech detection to reduce the challenges. Authors developed an apache spark based model to classify Amharic Facebook posts and comments into hate and not hate. Authors employed Random forest and Naïve Bayes for learning and Word2Vec and TF-IDF for feature selection. Tested by 10-fold cross-validation, the model based on word2vec embedding performed best with 79.83\%accuracy. The proposed method achieve a promising result with unique feature of spark for big data. KEYWORDS Amharic Hate speech detection, Social networks and spark, Amharic posts and comments},
	publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
	author = {Mossie, Zewdie and Wang, Jenq-Haur},
	month = apr,
	year = {2018},
	pages = {41--55},
}

@article{Yoder2018,
	title = {Unpacking a political icon: ‘{Bike} lanes’ and orders of indexicality},
	volume = {12},
	copyright = {All rights reserved},
	url = {http://journals.sagepub.com/doi/full/10.1177/1750481317745753},
	doi = {10.1177/1750481317745753},
	number = {2},
	journal = {Discourse \& Communication},
	author = {Yoder, Michael Miller and Johnstone, Barbara},
	year = {2018},
	keywords = {bike lanes, discourse analysis, indexicality, mayoral election, media, political discourse analysis},
	pages = {192--208},
}

@article{jiang_empirical_2022,
	title = {An empirical analysis of high school students' practices of modelling with unstructured data},
	copyright = {All rights reserved},
	issn = {0007-1013},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13253},
	doi = {10.1111/bjet.13253},
	journal = {British Journal of Educational Technology},
	author = {Jiang, Shiyan and Nocera, Amato and Tatar, Cansu and Yoder, Michael Miller and Chao, Jie and Wiedemann, Kenia and Finzer, William and Rosé, Carolyn P.},
	month = jul,
	year = {2022},
}

@inproceedings{eisenstein_sparse_2011,
	title = {Sparse additive generative models of text},
	isbn = {978-1-4503-0619-5},
	abstract = {Generative models of text typically associate a multinomial with every class label or topic. Even in simple models this requires the estimation of thousands of parameters; in multifaceted latent variable models, standard approaches require additional latent "switching" variables for every token, complicating inference. In this paper, we propose an alternative generative model for text. The central idea is that each class label or latent topic is endowed with a model of the deviation in log-frequency from a constant background distribution. This approach has two key advantages: we can enforce sparsity to prevent overfitting, and we can combine generative facets through simple addition in log space, avoiding the need for latent switching variables. We demonstrate the applicability of this idea to a range of scenarios: classification, topic modeling, and more complex multifaceted generative models. Copyright 2011 by the author(s)/owner(s).},
	urldate = {2022-07-06},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Machine} {Learning}, {ICML} 2011},
	author = {Eisenstein, Jacob and Ahmed, Amr and Xing, Eric P.},
	year = {2011},
	pages = {1041--1048},
}

@inproceedings{almerekhi_detecting_2019,
	title = {Detecting toxicity triggers in online discussions},
	isbn = {978-1-4503-6885-8},
	doi = {10.1145/3342220.3344933},
	abstract = {Despite the considerable interest in the detection of toxic comments, there has been little research investigating the causes - i.e., triggers - of toxicity. In this work, we first propose a formal definition of triggers of toxicity in online communities. We proceed to build an LSTM neural network model using textual features of comments, and then, based on a comprehensive review of previous literature, we incorporate topical and sentiment shift in interactions as features. Our model achieves an average accuracy of 82.5\% of detecting toxicity triggers from diverse Reddit communities.},
	booktitle = {{HT} 2019 - {Proceedings} of the 30th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Almerekhi, Hind and Jansen, Bernard J. and Kwak, Haewoon and Salminen, Joni},
	month = sep,
	year = {2019},
	keywords = {Neural networks, Reddit, Social media, Toxicity, Trigger detection},
	pages = {291--292},
}

@inproceedings{pavlopoulos_toxicity_2020,
	title = {Toxicity {Detection}: {Does} {Context} {Really} {Matter}?},
	abstract = {Moderation is crucial to promoting healthy on-line discussions. Although several 'toxicity' detection datasets and models have been published , most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5\% in one of our experiments) end up having the opposite toxicity labels if the an-notators are not provided with context. Surprisingly , we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Pavlopoulos, John and Sorensen, Jeffrey and Dixon, Lucas and Thain, Nithum},
	year = {2020},
	pages = {4296--4305},
}

@inproceedings{caselli_i_2020,
	address = {Marseille, France},
	title = {I {Feel} {Offended}, {Don}'t {Be} {Abusive}! {Implicit}/{Explicit} {Messages} in {Offensive} and {Abusive} {Language}},
	abstract = {Abusive language detection is an unsolved and challenging problem for the NLP community. Recent literature suggests various approaches to distinguish between different language phenomena (e.g., hate speech vs. cyberbullying vs. offensive language) and factors (degree of explicitness and target) that may help to classify different abusive language phenomena. There are data sets that annotate the target of abusive messages (i.e.OLID/OffensEval (Zampieri et al., 2019a)). However, there is a lack of data sets that take into account the degree of explicitness. In this paper, we propose annotation guidelines to distinguish between explicit and implicit abuse in English and apply them to OLID/OffensEval. The outcome is a newly created resource, AbuseEval v1.0, which aims to address some of the existing issues in the annotation of offensive and abusive language (e.g., explicitness of the message, presence of a target, need of context, and interaction across different phenomena).},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association (ELRA},
	author = {Caselli, Tommaso and Basile, Valerio and Mitrovi´cmitrovi´c, Jelena and Kartoziya, Inga and Granitzer, Michael},
	year = {2020},
	keywords = {Corpus Annotation, Social Media Processing, Statistical and Machine Learning Methods},
	pages = {11--16},
}

@inproceedings{wiegand_detection_2019,
	title = {Detection of {Abusive} {Language}: the {Problem} of {Biased} {Datasets}},
	abstract = {We discuss the impact of data bias on abusive language detection. We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.},
	booktitle = {Proceedings of {NAACL}-{HLT} 2019},
	author = {Wiegand, Michael and Ruppenhofer, Josef and Kleinbauer, Thomas},
	year = {2019},
	pages = {602--608},
}

@inproceedings{Shen2018,
	title = {Perceptions of {Censorship} and {Moderation} {Bias} in {Political} {Debate} {Forums}},
	copyright = {All rights reserved},
	url = {https://aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17809/17026},
	booktitle = {Proceedings of the {Twelfth} {International} {AAAI} {Conference} on {Web} and {Social} {Media} ({ICWSM} 2018)},
	author = {Shen, Qinlan and Yoder, Michael Miller and Jo, Yohan and Rosé, Carolyn P},
	year = {2018},
	pages = {350--359},
}

@article{menini_abuse_2021,
	title = {Abuse is {Contextual}, {What} about {NLP}? {The} {Role} of {Context} in {Abusive} {Language} {Annotation} and {Detection}},
	url = {http://arxiv.org/abs/2103.14916},
	abstract = {The datasets most widely used for abusive language detection contain lists of messages, usually tweets, that have been manually judged as abusive or not by one or more annotators, with the annotation performed at message level. In this paper, we investigate what happens when the hateful content of a message is judged also based on the context, given that messages are often ambiguous and need to be interpreted in the context of occurrence. We first re-annotate part of a widely used dataset for abusive language detection in English in two conditions, i.e. with and without context. Then, we compare the performance of three classification algorithms obtained on these two types of dataset, arguing that a context-aware classification is more challenging but also more similar to a real application scenario.},
	author = {Menini, Stefano and Aprosio, Alessio Palmero and Tonelli, Sara},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14916},
}

@article{adams_dominance_1995,
	title = {Dominance and entitlement: the rhetoric men use to discuss their violence towards women},
	volume = {6},
	number = {3},
	journal = {Discourse \& Society},
	author = {Adams, Peter J. and Towns, Alison and Gavey, Nicola},
	year = {1995},
	pages = {387--406},
}

@incollection{sidanius_social_1999,
	title = {Social {Dominance} {Theory}: {A} {New} {Synthesis}},
	abstract = {As we saw in the preceding chapter, a number of classical and contemporary theories of social attitudes and intergroup relations have given us some important insights into the nature and dynamics of intergroup conflict, stereotyping, and group oppression. However, there has yet to be a serious effort to integrate these insights into one coherent and comprehensive theoretical model. In an effort to accomplish this and gain a firmer purchase on the almost boringly repetitive nature of human oppression, we have developed social dominance theory (SDT). While this approach is new in many ways, its primary virtue is that it ties together the most critical and useful components and models reviewed in Chapter 1. The most important sources for our new synthesis can be found in the ideas of (a) authoritarian personality theory, (b) Rokeach's two-value theory of political behavior, (c) Blumer's group positions theory, (d) Marxism and neoclassical elite theories, (e) results from political attitude and public opinion research, (f) social identity theory (SIT), and (g) modern thinking within evolutionary psychology While SDT has been influenced by models within personality psychology, social psychology, and political sociology, it is neither strictly a psychological nor a sociological theory, but rather an attempt to connect the worlds of individual personality and attitudes with the domains of institutional behavior and social structure. Thus, SDT is an attempt to integrate several levels of analysis into one coherent theoretical framework.},
	booktitle = {Social {Dominance}},
	publisher = {Cambridge University Press},
	author = {Sidanius, Jim and Pratto, Felicia},
	year = {1999},
	doi = {10.1017/cbo9781139175043.002},
	pages = {31--58},
}

@article{newman_social_2014,
	title = {Social dominance and the cultural politics of immigration},
	volume = {35},
	issn = {14679221},
	doi = {10.1111/pops.12047},
	abstract = {We argue that conflict over immigration largely concerns who bears the burden of cultural transaction costs, which we define as the costs associated with overcoming cultural barriers (e.g., language) to social exchange. Our framework suggests that the ability of native-born citizens to push cultural transaction costs onto immigrant out-groups serves as an important expression of social dominance. In two novel studies, we demonstrate that social dominance motives condition emotional responses to encountering cultural transaction costs, shape engagement in cultural accommodation behavior toward immigrants, and affect immigration attitudes and policy preferences. © 2013 International Society of Political Psychology.},
	number = {2},
	journal = {Political Psychology},
	author = {Newman, Benjamin J. and Hartman, Todd K. and Taber, Charles S.},
	year = {2014},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {Cultural threat, Immigration, Social dominance orientation, Transaction costs},
	pages = {165--186},
}

@inproceedings{novikova_speech_2021,
	title = {Speech aggression as a manifestation of dominance: {Pragmalinguistic} aspects},
	volume = {273},
	doi = {10.1051/e3sconf/202127311047},
	abstract = {The authors investigate the phenomenon of speech aggression in texts of various directions. The destruction of the cultural system, built exclusively on religious unity, marks the end of the Middle Ages, coming towards the end of the fourteenth century. It was then that the formation of a new principle begins, which is that objective reality and its meaning are sensual. Reality is comprehended not with the help of revelation, transposed by selected religious practices, but through the senses. The role of anthropocentrism is growing both at the philosophical and cultural, and at the speech-thinking (linguo-cognitive) levels. A cultural system based on such a premise idealistic. In it, feeling is balanced by intellect, faith by science, empirical perception by intuition. Its values belong to both the Earth and Heaven. Her world is both sensual and supersensible.},
	booktitle = {{E3S} {Web} of {Conferences}},
	publisher = {EDP Sciences},
	author = {Novikova, Lidia and Shuraeva, Fotina},
	month = jun,
	year = {2021},
	note = {ISSN: 22671242},
}

@inproceedings{lu_subverting_2022,
	title = {Subverting machines, fluctuating identities: {Re}-learning human categorization},
	url = {http://arxiv.org/abs/2205.13740},
	doi = {10.1145/3531146.3533161},
	abstract = {Most machine learning systems that interact with humans construct some notion of a person's "identity," yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co\${\textbackslash}unicode\{x2010\}\$occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.},
	booktitle = {{FAccT} '22: 2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Lu, Christina and Kay, Jackie and McKee, Kevin R.},
	month = may,
	year = {2022},
	note = {arXiv: 2205.13740},
	pages = {1005--1014},
}

@techreport{sharma_degree_2018,
	title = {Degree based {Classification} of {Harmful} {Speech} using {Twitter} {Data}},
	url = {https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data},
	abstract = {Harmful speech has various forms and it has been plaguing the social media in different ways. If we need to crackdown different degrees of hate speech and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent, and used it to annotate twitter data accordingly. The key contribution of this paper is the new dataset of tweets we created based on ontological classes and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it's automatic detection more robust and efficient.},
	author = {Sharma, Sanjana and Shrivastava, Manish},
	year = {2018},
	pages = {106--112},
}

@inproceedings{jurgens_just_2019,
	address = {Florence, Italy},
	title = {A just and comprehensive strategy for using {NLP} to address online abuse},
	abstract = {Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Lingustics},
	author = {Jurgens, David and Chandrasekharan, Eshwar and Hemphill, Libby},
	month = jul,
	year = {2019},
	note = {ISSN: 23318422},
	pages = {3658--3666},
}

@inproceedings{Dadvar2012,
	address = {Ghent},
	title = {Improved {Cyberbullying} {Detection} {Using} {Gender} {Information}},
	abstract = {As a result of the invention of social networks, friendships, relationships and social communication are all undergoing changes and new definitions seem to be applicable. One may have hundreds of " friends " without even seeing their faces. Meanwhile, alongside this transition there is increasing evidence that online social applications are used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the characteristics of the actors involved in cyberbullying. Social studies on cyberbullying reveal that the written language used by a harasser varies with the author " s features including gender. In this study we used a support vector machine model to train a gender-specific text classifier. We demonstrated that taking gender-specific language features into account improves the discrimination capacity of a classifier to detect cyberbullying.},
	booktitle = {Proceedings of the {Twelfth} {Dutch}-{Belgian} {Information} {Retrieval} {Workshop} ({DIR} 2012)},
	publisher = {Ghent University},
	author = {Dadvar, Maral and De Jong, Franciska and Ordelman, Roeland and Trieschnigg, Dolf},
	year = {2012},
	keywords = {Experimentation, Gender distinction, Human Factors, Languages Keywords Cyberharassment, Security, Social networks, Support vector machine, Text mining},
	pages = {22--25},
}

@inproceedings{salminen_anatomy_2018,
	title = {Anatomy of {Online} {Hate}: {Developing} a {Taxonomy} and {Machine} {Learning} {Models} for {Identifying} and {Classifying} {Hate} in {Online} {News} {Media}},
	url = {www.aaai.org},
	abstract = {Online social media platforms generally attempt to mitigate hateful expressions, as these comments can be detrimental to the health of the community. However, automatically identifying hateful comments can be challenging. We manually label 5,143 hateful expressions posted to YouTube and Facebook videos among a dataset of 137,098 comments from an online news media. We then create a granular tax-onomy of different types and targets of online hate and train machine learning models to automatically detect and classify the hateful comments in the full dataset. Our contribution is twofold: 1) creating a granular taxonomy for hateful online comments that includes both types and targets of hateful comments, and 2) experimenting with machine learning, including Logistic Regression, Decision Tree, Random Forest, Adaboost, and Linear SVM, to generate a multiclass, multilabel classification model that automatically detects and categorizes hateful comments in the context of online news media. We find that the best performing model is Linear SVM, with an average F1 score of 0.79 using TF-IDF features. We validate the model by testing its predictive ability, and, relatedly, provide insights on distinct types of hate speech taking place on social media.},
	author = {Salminen, Joni and Almerekhi, † and Milenković, Milica and Jung, Soon-Gyo and An, Jisun and Kwak, Haewoon and Jansen, Bernard J},
	year = {2018},
	keywords = {Full Papers},
	pages = {330--339},
}

@inproceedings{mozafari_bert-based_2019,
	title = {A {BERT}-{Based} {Transfer} {Learning} {Approach} for {Hate} {Speech} {Detection} in {Online} {Social} {Media}},
	abstract = {Generated hateful and toxic content by a portion of users in social media is a rising phenomenon that motivated researchers to dedicate substantial efforts to the challenging direction of hateful content identification. We not only need an efficient automatic hate speech detection model based on advanced machine learning and natural language processing, but also a sufficiently large amount of annotated data to train a model. The lack of a sufficient amount of labelled hate speech data, along with the existing biases, has been the main issue in this domain of research. To address these needs, in this study we introduce a novel transfer learning approach based on an existing pre-trained language model called BERT (Bidirectional Encoder Representations from Transformers). More specifically, we investigate the ability of BERT at capturing hateful context within social media content by using new fine-tuning methods based on transfer learning. To evaluate our proposed approach , we use two publicly available datasets that have been annotated for racism, sexism, hate, or offensive content on Twitter. The results show that our solution obtains considerable performance on these datasets in terms of precision and recall in comparison to existing approaches. Consequently , our model can capture some biases in data annotation and collection process and can potentially lead us to a more accurate model.},
	booktitle = {International {Conference} on {Complex} {Networks} and {Their} {Applications}.},
	author = {Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noël},
	year = {2019},
	keywords = {BERT, NLP, fine-tuning, hate speech detection, language model-ing, social media, transfer learning},
	pages = {928--940},
}

@inproceedings{Kwok2013,
	title = {Locate the {Hate}: {Detecting} {Tweets} against {Blacks}},
	isbn = {978-1-57735-615-8},
	url = {http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CC0QFjAA&amp;url=http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6419/6821&amp;ei=e7hJUq2EAtKq4AOB04HoDg&amp;usg=AFQjCNEi9mX0w71lUCo8tdxTnQJkR74MLg&am},
	abstract = {Although the social medium Twitter grants users freedom of speech, its instantaneous nature and retweeting features also amplify hate speech. Because Twitter has a sizeable black constituency, racist tweets against blacks are especially det- rimental in the Twitter community, though this effect may not be obvious against a backdrop of half a billion tweets a day.1 We apply a supervised machine learning approach, employing inexpensively acquired labeled data from diverse Twitter accounts to learn a binary classifier for the labels “racist” and “nonracist.” The classifier has a 76\% average accuracy on individual tweets, suggesting that with further improvements, our work can contribute data on the sources of anti-black hate speech.},
	booktitle = {Twenty-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Kwok, Irene and Wang, Yuzhou},
	year = {2013},
	keywords = {Student Abstract and Poster Program},
	pages = {1621--1622},
}

@inproceedings{grondahl_all_2018,
	title = {All you need is “love”: {Evading} hate speech detection},
	isbn = {978-1-4503-6004-3},
	doi = {10.1145/3270101.3270103},
	abstract = {With the spread of social networks and their unfortunate use for hate speech, automatic detection of the latter has become a pressing problem. In this paper, we reproduce seven state-of-the-art hate speech detection models from prior work, and show that they perform well only when tested on the same type of data they were trained on. Based on these results, we argue that for successful hate speech detection, model architecture is less important than the type of data and labeling criteria. We further show that all proposed detection techniques are brittle against adversaries who can (automatically) insert typos, change word boundaries or add innocuous words to the original hate speech. A combination of these methods is also effective against Google Perspective – a cutting-edge solution from industry. Our experiments demonstrate that adversarial training does not completely mitigate the attacks, and using character-level features makes the models systematically more attack-resistant than using word-level features.},
	booktitle = {Proceedings of the 11th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security} ({AISec} ’18)},
	publisher = {Association for Computing Machinery},
	author = {Gröndahl, Tommi and Pajola, Luca and Juuti, Mika and Conti, Mauro and Asokan, N.},
	month = oct,
	year = {2018},
	note = {arXiv: 1808.09115
ISSN: 15437221},
	pages = {2--12},
}

@inproceedings{borkan_nuanced_2019,
	title = {Nuanced metrics for measuring unintended bias with real data for text classification},
	isbn = {978-1-4503-6675-5},
	doi = {10.1145/3308560.3317593},
	abstract = {Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.},
	booktitle = {Companion {Proceedings} of the 2019 {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	month = may,
	year = {2019},
	note = {arXiv: 1903.04561},
	pages = {491--500},
}

@inproceedings{basile_semeval-2019_2019,
	title = {{SemEval}-2019 {Task} 5: {Multilingual} {Detection} of {Hate} {Speech} {Against} {Immigrants} and {Women} in {Twitter}},
	abstract = {The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.},
	booktitle = {Proceedings of the 13th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2019)},
	author = {Basile, Valerio and Bosco, Cristina and Fersini, Elisabetta and Nozza, Debora and Patti, Viviana and Rangel, Francisco and Rosso, Paolo and Sanguinetti, Manuela},
	year = {2019},
	pages = {54--63},
}

@inproceedings{swamy_studying_2019,
	address = {Hong Kong, China},
	title = {Studying {Generalisability} {Across} {Abusive} {Language} {Detection} {Datasets}},
	abstract = {Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.},
	urldate = {2022-06-29},
	booktitle = {Proceedings of the 23rd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Swamy, Steve Durairaj and Jamatia, Anupam and Gambäck, Björn},
	month = nov,
	year = {2019},
	pages = {940--950},
}

@article{berlet_overview_2006,
	title = {Overview of {U}.{S}. {White} {Supremacist} {Groups}},
	volume = {34},
	number = {1},
	journal = {Journal of Political and Military Sociology},
	author = {Berlet, Chip and Vysotsky, Stanislav},
	year = {2006},
	pages = {11--48},
}

@article{burris_white_2000,
	title = {White {Supremacist} {Networks} on the {Internet}},
	volume = {33},
	number = {2},
	journal = {Source: Sociological Focus},
	author = {Burris, Val and Smith, Emery and Strahm, Ann},
	year = {2000},
	pages = {215--235},
}

@inproceedings{safi_samghabadi_aggression_2020,
	title = {Aggression and {Misogyny} {Detection} using {BERT}: {A} {Multi}-{Task} {Approach}},
	isbn = {7/2/20678231},
	url = {https://www.theverge.com/interface/2019/},
	abstract = {In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection. This paper presents our system for TRAC-2 shared task on "Aggression Identification" (sub-task A) and "Misogynistic Aggression Identification" (sub-task B). The data for this shared task is provided in three different languages-English, Hindi, and Bengali. Each data instance is annotated into one of the three aggression classes-Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes-Gendered and Non-Gendered. We propose an end-to-end neural model using attention on top of BERT that incorporates a multi-task learning paradigm to address both sub-tasks simultaneously. Our team, "na14", scored 0.8579 weighted F1-measure on the English sub-task B and secured 3 rd rank out of 15 teams for the task. The code and the model weights are publicly available at https://github.com/NiloofarSafi/TRAC-2.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Trolling}, {Aggression} and {Cyberbullying}},
	author = {Safi Samghabadi, Niloofar and Patwa, Parth and Pykl, Srinivas and Mukherjee, Prerana and Das, Amitava and Solorio, Thamar},
	year = {2020},
	keywords = {Abusive Language, Aggression, BERT, Hate-Speech Detection, Misogyny, NLP, Neural Networks, Social Media},
	pages = {11--16},
}

@article{ng_pro_2022,
	title = {Pro or {Anti}? a {Social} {Influence} {Model} of {Online} {Stance} {Flipping}},
	issn = {2327-4697},
	url = {https://ieeexplore.ieee.org/document/9806336/},
	doi = {10.1109/TNSE.2022.3185785},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Ng, Lynnette Hui Xian and Carley, Kathleen M.},
	year = {2022},
	pages = {1--18},
}

@article{magno_measuring_2022,
	title = {Measuring {International} {Online} {Human} {Values} with {Word} {Embeddings}},
	volume = {16},
	issn = {1559-1131},
	doi = {10.1145/3501306},
	abstract = {As the Internet grows in number of users and in the diversity of services, it becomes more influential on peoples lives. It has the potential of constructing or modifying the opinion, the mental perception, and the values of individuals. What is being created and published online is a reflection of people's values and beliefs. As a global platform, the Internet is a great source of information for researching the online culture of many different countries. In this work we develop a methodology for measuring data from textual online sources using word embedding models, to create a country-based online human values index that captures cultural traits and values worldwide. Our methodology is applied with a dataset of 1.7 billion tweets, and then we identify their location among 59 countries. We create a list of 22 Online Values Inquiries (OVI), each one capturing different questions from the World Values Survey, related to several values such as religion, science, and abortion. We observe that our methodology is indeed capable of capturing human values online for different counties and different topics. We also show that some online values are highly correlated (up to c = 0.69, p {\textless} 0.05) with the corresponding offline values, especially religion-related ones. Our method is generic, and we believe it is useful for social sciences specialists, such as demographers and sociologists, that can use their domain knowledge and expertise to create their own Online Values Inquiries, allowing them to analyze human values in the online environment.},
	number = {2},
	journal = {ACM Transactions on the Web},
	author = {Magno, Gabriel and Almeida, Virgilio},
	month = may,
	year = {2022},
	note = {Publisher: Association for Computing Machinery (ACM)},
	pages = {1--38},
}

@techreport{ali_analyzing_2022,
	title = {Analyzing {Antisemitism} and {Islamophobia} using a {Lexicon}-based {Approach}},
	url = {www.aaai.org},
	abstract = {The spread of Antisemitic and Islamophobic content in a longstanding problem, in particular within fringe Web communities. In this work, we attempt to analyze the spread of Antisemitic and Islamophobic content on 4chan's Politically Incorrect board (/pol/) using a lexicon-based approach. We use an openly-accessible knowledge graph, word embedding techniques that allow us to assess semantic similarity between terms, as well as manual annotations to create 2 lexicons. A lexicon of 48 Antisemitic terms and another lexicon of 135 Islamophobic terms. Then, by extracting all posts containing these terms from /pol/, we assess the popularity and veracity (i.e., what percentage of posts that contain these terms are actually Antisemitic/Islamophobic). We find that 93\% and 81\% of posts that contain terms from our lexicons are Antisemitic and Islamophobic, respectively. Also, we find that the veracity and frequency of these terms greatly varies on 4chan's /pol/. Finally, using topic modeling, we provide an overview of how popular Antisemitic and Islamophobic terms are used on 4chan's /pol/. To conclude, we make publicly available our lexicons for Antisemitic and Islamophobic terms, which are likely to be useful for researchers working on Antisemitism/Islamophobia or hate speech in general. Disclaimer. In this paper we analyze content that we consider as derogatory, offensive or racist. We thus warn the reader(s) that in the rest of this paper, we do not censor any of the language and suggest reader discretion.},
	author = {Ali, Moonis and Zannettou, Savvas},
	year = {2022},
}

@techreport{yoder_research_2022,
	title = {Research {Needs} for {Countering} {Extremist} {Hate}},
	copyright = {All rights reserved},
	url = {https://www.collabagainsthate.org/papers-presentations/research-needs},
	abstract = {This white paper identifies open research needs that practitioners outside academia face in combating extremist hate. We group these research needs into six themes and present a list of specific projects for future research. Our goal with this document is to orient researchers across disciplines toward research questions with the potential for translational impact in countering extremist hate.},
	urldate = {2022-06-17},
	institution = {Collaboratory Against Hate},
	author = {Yoder, Michael Miller and Habib, Hana},
	year = {2022},
}

@inproceedings{qian_benchmark_2019,
	title = {A {Benchmark} {Dataset} for {Learning} to {Intervene} in {Online} {Hate} {Speech}},
	abstract = {Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily fo-cused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets 1 collected from Gab 2 and Reddit 3. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk 4 Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Qian, Jing and Bethke, Anna and Liu, Yinyin and Belding, Elizabeth and Wang, William Yang},
	year = {2019},
	pages = {4754--4763},
}

@inproceedings{Dixon2017,
	title = {Measuring and {Mitigating} {Unintended} {Bias} in {Text} {Classification}},
	abstract = {Jigsaw Abstract We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique per-mits analysis in the common scenario where demographic in-formation on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compro-mising overall model quality.},
	booktitle = {{AAAI}/{ACM} {Conference} on {Artificial} {Intelligence}, {Ethics}, and {Society} ({AIES})},
	author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	year = {2017},
}

@article{williams_towards_2017,
	title = {Towards an {Ethical} {Framework} for {Publishing} {Twitter} {Data} in {Social} {Research}: {Taking} into {Account} {Users}’ {Views}, {Online} {Context} and {Algorithmic} {Estimation}},
	volume = {51},
	issn = {14698684},
	doi = {10.1177/0038038517708140},
	abstract = {New and emerging forms of data, including posts harvested from social media sites such as Twitter, have become part of the sociologist’s data diet. In particular, some researchers see an advantage in the perceived ‘public’ nature of Twitter posts, representing them in publications without seeking informed consent. While such practice may not be at odds with Twitter’s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in ‘legal’ accounts of the permissible use of these data in research publications. To challenge some existing practice in Twitter-based research, this article brings to the fore: (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.},
	number = {6},
	journal = {Sociology},
	author = {Williams, Matthew L. and Burnap, Pete and Sloan, Luke},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Twitter, algorithms, computational social science, context collapse, ethics, social data science, social media},
	pages = {1149--1168},
}

@incollection{talat_bridging_2018,
	title = {Bridging the {Gaps}: {Multi} {Task} {Learning} for {Domain} {Transfer} of {Hate} {Speech} {Detection}},
	abstract = {@incollection\{waseem2018bridging, title=\{Bridging the gaps: Multi task learning for domain transfer of hate speech detection\}, author=\{Waseem, Zeerak and Thorne, James and Bingel, Joachim\}, booktitle=\{Online Harassment\}, pages=\{29--55\}, year=\{2018\}, publisher=\{Springer\} \}},
	author = {Talat, Zeerak and Thorne, James and Bingel, Joachim},
	year = {2018},
	doi = {10.1007/978-3-319-78583-7_3},
	pages = {29--55},
}

@techreport{benesch_dangerous_2013,
	title = {Dangerous {Speech}: {A} {Proposal} to {Prevent} {Group} {Violence}},
	author = {Benesch, Susan},
	year = {2013},
}

@techreport{sanguinetti_haspeede_2020,
	title = {{HaSpeeDe} 2 @ {EVALITA2020}: {Overview} of the {EVALITA} 2020 {Hate} {Speech} {Detection} {Task}},
	url = {https://www.},
	abstract = {The Hate Speech Detection (HaSpeeDe 2) task is the second edition of a shared task on the detection of hateful content in Ital-ian Twitter messages. HaSpeeDe 2 is composed of a Main task (hate speech detection) and two Pilot tasks, (stereotype and nominal utterance detection). Systems were challenged along two dimensions: (i) time, with test data coming from a different time period than the training data, and (ii) domain, with test data coming from the news domain (i.e., news headlines). Overall, 14 teams participated in the Main task, the best systems achieved a macro F1-score of 0.8088 and 0.7744 on the in-domain in the out-of-domain test sets, respectively ; 6 teams submitted their results for Pilot task 1 (stereotype detection), the best systems achieved a macro F1-score of 0.7719 and 0.7203 on in-domain and out-of-domain test sets. We did not receive any submission for Pilot task 2.},
	author = {Sanguinetti, Manuela and Comandini, Gloria and Di Nuovo, Elisa and Frenda, Simona and Stranisci, Marco and Bosco, Cristina and Caselli, Tommaso and Patti, Viviana and Russo, Irene},
	year = {2020},
}

@article{fortuna_survey_2018,
	title = {A survey on automatic detection of hate speech in text},
	volume = {51},
	issn = {15577341},
	doi = {10.1145/3232676},
	abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Fortuna, Paula and Nunes, Sérgio},
	month = jul,
	year = {2018},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Hate speech, Literature review, Natural language processing, Opinion mining, Text mining},
}

@inproceedings{cao_deephate_2020,
	title = {{DeepHate}: {Hate} {Speech} {Detection} via {Multi}-{Faceted} {Text} {Representations}},
	isbn = {978-1-4503-7989-2},
	doi = {10.1145/3394231.3397890},
	abstract = {Online hate speech is an important issue that breaks the cohesiveness of online social communities and even raises public safety concerns in our societies. Motivated by this rising issue, researchers have developed many traditional machine learning and deep learning methods to detect hate speech in online social platforms automatically. However, most of these methods have only considered single type textual feature, e.g., term frequency, or using word embeddings. Such approaches neglect the other rich textual information that could be utilized to improve hate speech detection. In this paper, we propose DeepHate, a novel deep learning model that combines multi-faceted text representations such as word embeddings, sentiments, and topical information, to detect hate speech in online social platforms. We conduct extensive experiments and evaluate DeepHate on three large publicly available real-world datasets. Our experiment results show that DeepHate outperforms the state-of-the-art baselines on the hate speech detection task. We also perform case studies to provide insights into the salient features that best aid in detecting hate speech in online social platforms.},
	booktitle = {{WebSci} 2020 - {Proceedings} of the 12th {ACM} {Conference} on {Web} {Science}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Cao, Rui and Lee, Roy Ka Wei and Hoang, Tuan Anh},
	month = jul,
	year = {2020},
	note = {arXiv: 2103.11799},
	keywords = {Hate Speech Detection, Online Toxic Content, Social Media},
	pages = {11--20},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://aclanthology.org/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	note = {arXiv: 1810.04805},
	pages = {4171--4186},
}

@inproceedings{fortuna_toxic_2020,
	title = {Toxic, {Hateful}, {Offensive} or {Abusive}? {What} {Are} {We} {Really} {Classifying}? {An} {Empirical} {Analysis} of {Hate} {Speech} {Datasets}},
	abstract = {The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation.},
	booktitle = {Proceedings of the 12th {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2020)},
	author = {Fortuna, Paula and Soler-Company, Juan and Wanner, Leo},
	year = {2020},
	keywords = {aggression, dataset comparison, hate speech, offensive, toxicity},
	pages = {6786--6794},
}

@article{pamungkas_misogyny_2020,
	title = {Misogyny {Detection} in {Twitter}: a {Multilingual} and {Cross}-{Domain} {Study}},
	volume = {57},
	issn = {03064573},
	doi = {10.1016/j.ipm.2020.102360},
	abstract = {The freedom of expression given by social media has a dark side: the growing proliferation of abusive contents on these platforms. Misogynistic speech is a kind of abusive language, which can be simplified as hate speech targeting women, and it is becoming a more and more relevant issue in recent years. AMI IberEval 2018 and AMI EVALITA 2018 were two shared tasks which mainly focused on tackling the problem of misogyny in Twitter, in three different languages, namely English, Italian, and Spanish. In this paper, we present an in-depth study on the phenomena of misogyny in those three languages, by focusing on three main objectives. Firstly, we investigate the most important features to detect misogyny and the issues which contribute to the difficulty of misogyny detection, by proposing a novel system and conducting a broad evaluation on this task. Secondly, we study the relationship between misogyny and other abusive language phenomena, by conducting a series of cross-domain classification experiments. Finally, we explore the feasibility of detecting misogyny in a multilingual environment, by carrying out cross-lingual classification experiments. Our system succeeded to outperform all state of the art systems in all benchmark AMI datasets both subtask A and subtask B. Moreover, intriguing insights emerged from error analysis, in particular about the interaction between different but related abusive phenomena. Based on our cross-domain experiment, we conclude that misogyny is quite a specific kind of abusive language, while we experimentally found that it is different from sexism. Lastly, our cross-lingual experiments show promising results. Our proposed joint-learning architecture obtained a robust performance across languages, worth to be explored in further investigation.},
	number = {6},
	journal = {Information Processing and Management},
	author = {Pamungkas, Endang Wahyu and Basile, Valerio and Patti, Viviana},
	month = nov,
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Abusive language online, Automatic misogyny identification, Cross-domain classification, Cross-lingual classification, Social media},
}

@inproceedings{Waseem2016,
	title = {Are {You} a {Racist} or {Am} {I} {Seeing} {Things}? {Annotator} {Influence} on {Hate} {Speech} {Detection} on {Twitter}},
	url = {http://aclweb.org/anthology/W16-5618},
	doi = {10.18653/v1/W16-5618},
	abstract = {Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encour- ages a use of crowd sourcing for the annota- tion efforts. In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We},
	booktitle = {Proceedings of the {First} {Workshop} on {NLP} and {Computational} {Social} {Science}},
	author = {Waseem, Zeerak},
	year = {2016},
	pages = {138--142},
}

@inproceedings{elsherief_peer_2018,
	title = {Peer to {Peer} {Hate}: {Hate} {Speech} {Instigators} and {Their} {Targets}},
	abstract = {While social media has become an empowering agent to individual voices and freedom of expression, it also facilitates antisocial behaviors including online harassment, cyberbul-lying, and hate speech. In this paper, we present the first comparative study of hate speech instigators and target users on Twitter. Through a multi-step classification process, we curate a comprehensive hate speech dataset capturing various types of hate. We study the distinctive characteristics of hate instigators and targets in terms of their profile self-presentation, activities, and online visibility. We find that hate instigators target more popular and high profile Twitter users, and that participating in hate speech can result in greater on-line visibility. We conduct a personality analysis of hate insti-gators and targets and show that both groups have eccentric personality facets that differ from the general Twitter population. Our results advance the state of the art of understanding online hate speech engagement.},
	booktitle = {Proceedings of the {Twelfth} {International} {AAAI} {Conference} on {Web} and {Social} {Media} ({ICWSM} 2018)},
	author = {ElSherief, Mai and Nilizadeh, Shirin and Nguyen, Dana and Vigna, Giovanni and Belding, Elizabeth},
	year = {2018},
	keywords = {Full Papers},
	pages = {52--61},
}

@techreport{elsherief_hate_2018,
	title = {Hate {Lingo}: {A} {Target}-{Based} {Linguistic} {Analysis} of {Hate} {Speech} in {Social} {Media}},
	url = {https://www.hatebase.org/},
	abstract = {While social media empowers freedom of expression and individual voices, it also enables antisocial behavior, online harassment , cyberbullying, and hate speech. In this paper, we deepen our understanding of online hate speech by focus-ing on a largely neglected but crucial aspect of hate speech-its target: either directed towards a specific person or entity, or generalized towards a group of people sharing a common protected characteristic. We perform the first linguistic and psycholinguistic analysis of these two forms of hate speech and reveal the presence of interesting markers that distinguish these types of hate speech. Our analysis reveals that Directed hate speech, in addition to being more personal and directed, is more informal, angrier, and often explicitly attacks the target (via name calling) with fewer analytic words and more words suggesting authority and influence. Generalized hate speech, on the other hand, is dominated by religious hate, is characterized by the use of lethal words such as murder, exterminate , and kill; and quantity words such as million and many. Altogether, our work provides a data-driven analysis of the nuances of online-hate speech that enables not only a deepened understanding of hate speech and its social implications , but also its detection.},
	author = {ElSherief, Mai and Kulkarni, Vivek and Nguyen, Dana and Wang, William Yang and Belding, Elizabeth},
	year = {2018},
	keywords = {Full Papers},
}

@phdthesis{marsters_when_2019,
	title = {When hate speech leads to hateful actions: {A} corpus and discourse analytic approach to linguistic threat assessment of hate speech},
	school = {Georgetown University},
	author = {Marsters, Alexandria},
	year = {2019},
}

@inproceedings{poletto_resources_2021,
	title = {Resources and benchmark corpora for hate speech detection: a systematic review},
	volume = {55},
	doi = {10.1007/s10579-020-09502-8},
	abstract = {Hate Speech in social media is a complex phenomenon, whose detection has recently gained significant traction in the Natural Language Processing community, as attested by several recent review works. Annotated corpora and benchmarks are key resources, considering the vast number of supervised approaches that have been proposed. Lexica play an important role as well for the development of hate speech detection systems. In this review, we systematically analyze the resources made available by the community at large, including their development methodology, topical focus, language coverage, and other factors. The results of our analysis highlight a heterogeneous, growing landscape, marked by several issues and venues for improvement.},
	booktitle = {Language {Resources} and {Evaluation}},
	publisher = {Springer Science and Business Media B.V.},
	author = {Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
	month = jun,
	year = {2021},
	note = {Issue: 2
ISSN: 15728412},
	keywords = {Benchmark corpora, Hate speech detection, Natural Language Processing shared tasks, Systematic review},
	pages = {477--523},
}

@article{pohjonen_extreme_2017,
	title = {Extreme {Speech} {Online}: {An} {Anthropological} {Critique} of {Hate} {Speech} {Debates}},
	volume = {11},
	url = {https://ijoc.org/index.php/ijoc/article/download/5843/1965},
	abstract = {Exploring the cases of India and Ethiopia, this article develops the concept of "extreme speech" to critically analyze the cultures of vitriolic exchange on Internet-enabled media. While online abuse is largely understood as "hate speech," we make two interventions to problematize the presuppositions of this widely invoked concept. First, extreme speech emphasizes the need to contextualize online debate with an attention to user practices and particular histories of speech cultures. Second, related to context, is the ambiguity of online vitriol, which defies a simple antonymous conception of hate speech versus acceptable speech. The article advances this analysis using the approach of "comparative practice," which, we suggest, complicates the discourse of Internet "risk" increasingly invoked to legitimate online speech restrictions. The recent electoral victories for conservative groups with aggressive online presence have brought the political stakes of digital speech into sharp public focus, unsettling euphoric pronouncements on new media as a radical enabler of citizen participation and open society. Whether online Islamist radicalization or hate messages on social media during the 2016 refugee crisis, current developments have reinvigorated political debates on the limits of free speech online. The discourse on digital technologies has tilted toward the "dark side" of new media as a platform for promoting hate speech, fake news, right-wing nationalist mobilization, terrorism, misogyny, and intergroup conflict (Lovink, 2013; Morozov, 2012). Such negative forms of online speech, it is argued, threaten many of the taken-for-granted freedoms commonly associated with digital media cultures around the world. The discourse of online speech as a form of "risk" and "threat" is also used increasingly by governments to rhetorically legitimize securitization and control over their citizens' communicative practices (Amoore \& Goede, 2008).},
	urldate = {2022-01-19},
	journal = {International Journal of Communication},
	author = {Pohjonen, Matti and Udupa, Sahana},
	year = {2017},
	keywords = {Ethiopia, India, Internet risk, comparative practice, hate speech, online abuse},
	pages = {1173--1191},
}

@article{sap_risk_2019,
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://www.aclweb.org/anthology/P19-1163},
	abstract = {We investigate how annotators\{'\} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet\{'\}s dialect they are significantly less likely to label the tweet as offensive.},
	journal = {Proceedings of the 57th Conference of the Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A},
	year = {2019},
	pages = {1668--1678},
}

@article{hoover_moral_2020,
	title = {Moral {Foundations} {Twitter} {Corpus}: {A} {Collection} of 35k {Tweets} {Annotated} for {Moral} {Sentiment}},
	volume = {11},
	issn = {19485514},
	url = {https://journals.sagepub.com/doi/10.1177/1948550619876629},
	doi = {10.1177/1948550619876629},
	abstract = {Research has shown that accounting for moral sentiment in natural language can yield insight into a variety of on- and off-line phenomena such as message diffusion, protest dynamics, and social distancing. However, measuring moral sentiment in natural language is challenging, and the difficulty of this task is exacerbated by the limited availability of annotated data. To address this issue, we introduce the Moral Foundations Twitter Corpus, a collection of 35,108 tweets that have been curated from seven distinct domains of discourse and hand annotated by at least three trained annotators for 10 categories of moral sentiment. To facilitate investigations of annotator response dynamics, we also provide psychological and demographic metadata for each annotator. Finally, we report moral sentiment classification baselines for this corpus using a range of popular methodologies.},
	number = {8},
	urldate = {2022-05-31},
	journal = {Social Psychological and Personality Science},
	author = {Hoover, Joe and Portillo-Wightman, Gwenyth and Yeh, Leigh and Havaldar, Shreya and Davani, Aida Mostafazadeh and Lin, Ying and Kennedy, Brendan and Atari, Mohammad and Kamel, Zahra and Mendlen, Madelyn and Moreno, Gabriela and Park, Christina and Chang, Tingyee E. and Chin, Jenna and Leong, Christian and Leung, Jun Yen and Mirinjian, Arineh and Dehghani, Morteza},
	month = nov,
	year = {2020},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {Moral Foundations Theory, NLP, morality, sentiment, text analysis},
	pages = {1057--1071},
}

@techreport{hackenburg_mapping_2022,
	title = {Mapping moral language on {U}.{S}. presidential primary campaigns reveals rhetorical networks of political division and unity},
	abstract = {During political campaigns, candidates use rhetoric to advance competing visions and assessments of their country. Research reveals that the moral language used in this rhetoric can significantly influence citizens' political attitudes and behaviors; however, the moral language actually used in the rhetoric of elites during political campaigns remains understudied. Using a dataset of every tweet (N = 139,412) published by 39 U.S. presidential candidates during the 2016 and 2020 primary elections, we extracted moral language and constructed network models illustrating how candi-dates' rhetoric is semantically connected. These network models yielded two key discoveries. First, we find that party affiliation clusters can be reconstructed solely based on the moral words used in candidates' rhetoric. Within each party, popular moral values are expressed in highly similar ways, with Democrats emphasizing careful and just treatment of individuals and Republicans emphasizing in-group loyalty and respect for social hierarchies. Second, we illustrate the ways in which outsider candidates like Donald Trump can separate themselves during primaries by using moral rhetoric that differs from their parties' common language. Our findings demonstrate the functional use of strategic moral rhetoric in a campaign context and show that unique methods of text network analysis are broadly applicable to the study of campaigns and social movements. moral language {\textbar} political campaigns {\textbar} moral foundations theory {\textbar} network analysis {\textbar} natural language processing A t the very heart of elections in a representative democracy lies the art of rhetoric. As Aristotle observed, effective rhetoric can offer political advocates significant electoral influence. Moral rhetoric-emphasizing notions of right and wrong-is among the most powerful and widely used form of rhetoric. Surprisingly, twenty-five years into a political era characterized by moral emotion (1-3) it remains unclear exactly how moral rhetoric shapes our electoral landscape. Of particular concern is whether ideologically opposing candidates emphasize different moral values in their rhetoric, thereby entrenching voters in their existing views and exacerbating political polarization (4). Similarly unclear is the extent to which the use of unique moral rhetoric separates candidates from their competition, potentially increasing their persuasive appeal and shaping electoral outcomes (5, 6). However, since campaigns generate vast amounts of textual data and develop nuanced, overlapping vocabularies, empirically mapping the moral language used by competing candidates poses a unique challenge. Here, we develop a methodology combining natural language processing and network analysis to reveal the extent to which the use of specific moral words connected or differentiated political candidates during recent elections in the U.S. Focusing on five moral foundations (7, 8)-care, fairness, loyalty, authority, and sanctity-and leveraging a complete dataset of 139,412 tweets published by 39 candidates during the 2016 and 2020 U.S. presidential primaries, we illustrate how moral word choice organizes candidates in rhetorical space. This methodology is then applied to answer two key theoretical questions: (1) To what extent do political candidates within and between party lines naturally converge or diverge based on their use of moral words, and what moral-rhetorical dynamics drive these patterns? (2) To what extent can the use of unique moral rhetoric separate candidates from their competition? Our work differs from prior scholarship on elite moral rhetoric in two main ways. First, we expand the study of rhetorical positioning during elections. By empirically illustrating how candidates are connected by shared vocabularies or differentiated through the use of language uniquely their own, our approach can reveal not only the moral-rhetorical norms of a given primary, but the extent and manner in which outsider candidates deviate from those norms. Our approach is distinct from those treating the rhetoric of candidates as discrete objects of study (9) and provides important context: candidates argue, debate, and respond to one another as part of a connected discourse, and the moral language a candidate chooses to use may depend on the utterances of their peers. Voters may also select candidates based on their proximity to a moral-rhetorical ideal point, making electoral outcomes for a given campaign context-dependent. It is therefore important to understand not just what a candidate says during a campaign, but rather what a candidate says, given what other candidates are saying. We argue here that by choosing to use some words and not to use others, competing candidates create a sociolinguistic map which can be reconstructed and analyzed (10, 11), making visible the ways in which moral language structures inter-and intra-party dynamics, ideological shifts, and electoral outcomes. Second, our study comprehensively examines discrete moral values-operationalized here as moral foundations (12)-in campaign rhetoric, bridging the gap between lab-based work on moral foundations and rhetoric in a campaigning context. Previous work on elite moral rhetoric has instead emphasized either discourse from already-elected leaders-analyzing "offi-K.H.},
	author = {Hackenburg, Kobi and Brady, William J and Tsakiris, Manos},
	year = {2022},
}

@inproceedings{thavareesan_sentiment_2020,
	title = {Sentiment {Lexicon} {Expansion} using {Word2vec} {andfastText} for {Sentiment} {Prediction} in {Tamil} texts},
	isbn = {978-1-72819-975-7},
	abstract = {Conference held online due to COVID-19. "Part Number: CFP20B72-ART"--PDF copyright page},
	booktitle = {6th {International} {Moratuwa} {Engineering} {Research} {Conference}},
	author = {Thavareesan, Sajeetha and Mahesan, Sinnathamby},
	year = {2020},
}

@article{kim_exploring_2022,
	title = {Exploring incivility and moral foundations toward {Asians} in {English}-speaking tweets in hate crime-reporting cities during the {COVID}-19 pandemic},
	volume = {32},
	issn = {10662243},
	doi = {10.1108/INTR-11-2020-0678},
	abstract = {Purpose: This study aims to explore the extent to which Twitter users engaged in uncivil and morally questionable expressions in their comments about specific Asian countries and citizens. The integrated threat theory (ITT) was used to formulate questions surrounding incivility and moral foundations within Twitter discourses related to the COVID-19 pandemic. Design/methodology/approach: The authors collected tweets and retweets posted by English-speaking Twitter users in the United States (US) across the following three phases: (1) initial discovery of COVID-19 in China, (2) high US mortality rate from COVID-19 and (3) the announcement that a vaccine would soon be available in the US. Findings: The authors found a significant difference in uncivil tweets posted in cities with higher levels of reported hate crimes against Asians than cities with low levels. Lastly, English-speaking Twitter users tended to employ moral virtue words and moral vice words when discussing China and Chinese culture/populations. Research limitations/implications: The bags-of-words employed are limited in capturing nuanced and metaphorical terms. In addition, the analysis focused solely on Tweets composed in English and thus did not capture the thoughts and opinions of non-English speakers. Lastly, this study did not address all Asian countries. In this sense, the findings of this study might not be applicable to Tweets about other nations. Practical implications: Given that many Twitter users tend to use terms of moral virtue in support of Asians and Asian communities, the authors suggest that non-governmental organization administrators provide morally supportive social media campaigns that encourage users to engage in civil discourse. Social implications: These findings have theoretical implications as the frameworks of integrated threats and moral foundations were used to offer group-level explanations for online behavior. Additional research is needed to explore whether these frameworks can be used to explain negativity in other communication environments. Originality/value: This study expands the findings of prior studies that identified the extent to which Twitter users express hate speech, focusing on general Twitter discourse across three specific periods of the pandemic: degrees of incivility and moral foundations, and comparison of incivility based on the prevalence of reported hate crimes.},
	number = {1},
	journal = {Internet Research},
	author = {Kim, Bumsoo and Cooks, Eric and Kim, Seong Kyu},
	month = jan,
	year = {2022},
	note = {Publisher: Emerald Group Holdings Ltd.},
	keywords = {Big data, COVID-19, Incivility, Integrated threat theory, Moral foundations theory, Semantic network analysis, Twitter},
	pages = {362--378},
}

@article{wilhelm_gendered_2019,
	title = {Gendered {Morality} and {Backlash} {Effects} in {Online} {Discussions}: {An} {Experimental} {Study} on {How} {Users} {Respond} to {Hate} {Speech} {Comments} {Against} {Women} and {Sexual} {Minorities}},
	volume = {80},
	issn = {15732762},
	doi = {10.1007/s11199-018-0941-5},
	abstract = {Hate speech in online users’ comments is often targeted toward underprivileged social groups such as immigrants, sexual minorities, and women. Besides the general severity of such offenses, social media users’ personal characteristics influence the evaluation of hate comments. We focus on the flagging of hate comments aimed toward women and sexual minorities (i.e., the intention to report such comments as inappropriate to a moderator or platform provider of an online discussion forum). We investigate the influence of user’s morality on the intention to flag of such comments. Relying on social role and backlash theory, we scrutinize in how far gender plays a role in flagging intention and in how far people perceive hate comments by women as an act of double deviance. Therefore, we conducted a 2 × 2 online experiment with 457 participants (51\% female) recruited through political interest groups and a German news magazine site on Facebook. Results indicate that moral judgments are to some extent gendered as women are more concerned about fairness and avoiding harm to others than men are. Deviant and agentic online behavior by women is judged more strictly than such behavior by men. Results implicate that moderators of online discussions and platform providers should be sensitive to how gender stereotypes influence online discussions.},
	number = {7-8},
	journal = {Sex Roles},
	author = {Wilhelm, Claudia and Joeckel, Sven},
	month = apr,
	year = {2019},
	note = {Publisher: Springer New York LLC},
	keywords = {Antisocial behavior, Backlash, Gender roles, Gender stereotypes, Hate speech, Minority groups, Moral foundation, Moral identity, Morality, Online comments, Sexual minority},
	pages = {381--392},
}

@techreport{lai_hamor_2021,
	title = {{HaMor} at the {Profiling} {Hate} {Speech} {Spreaders} on {Twitter} {Notebook} for {PAN} at {CLEF} 2021},
	url = {https://www.noswearing.com/},
	abstract = {In this paper we describe the Hate and Morality (HaMor) submission for the Profiling Hate Speech Spreaders on Twitter task at PAN 2021. We ranked as the 19th position-over 66 participating teams-according to the averaged accuracy value of 73\% reached by our proposed models over the two languages. We obtained the 43th higher accuracy for English (62\%) and the 2nd higher accuracy for Spanish (84\%). We proposed four types of features for inferring users attitudes just from the text in their messages: HS detection, users morality, named entities, and communicative behaviour. The results of our experiments are promising and will lead to future investigations of these features in a finer grained perspective.},
	author = {Lai, Mirko and Stranisci, Marco Antonio and Bosco, Cristina and Damiano, Rossana and Patti, Viviana},
	year = {2021},
	keywords = {Communicative Behaviour, Hate Speech, Moral Values, Named Entities},
}

@article{armstrong_framing_2019,
	title = {Framing hate: {Moral} foundations, party cues, and ({In})tolerance of offensive speech},
	volume = {7},
	issn = {21953325},
	doi = {10.5964/jspp.v7i2.1006},
	abstract = {One of the most controversial elements of political tolerance concerns support for hate speech. We argue that there are two factors that can reduce tolerance for hate speech: 1) moral foundations and 2) party cues. U.S. citizens’ tolerance of hate speech will be reduced when it is framed as a violation of a specific moral foundation, opposed by a political party, or when the morality violation is utilized by party elites. Using two survey experiments, we manipulated the target of hate speech (i.e. Muslims or the American flag), whether the speech violated a moral foundation (i.e. harm or loyalty), and which political party supported or opposed the hate speech in question. For flag burning, moral frames and party cues on their own reduced U.S. citizens’ tolerance relative to a non-political control, while moral frames and party cues were successful in reducing tolerance of anti-Muslim speech compared to a free speech appeal. Partisans were generally responsive to cues from the in-party. We also found instances of moral repackaging, where morally incongruent appeals from the in-party reduced tolerance of flag burning among Democrats. Among Republicans, harm morality decreased tolerance of anti-Muslim speech when invoked by the in-party, but increased tolerance when used by the out-party – an indication of the power of party cues to repackage moral arguments and to trigger backlash. These results provide a better understanding of what factors can affect tolerance for hate speech, providing political leaders and social justice advocates with a roadmap to alleviate this problem.},
	number = {2},
	journal = {Journal of Social and Political Psychology},
	author = {Armstrong, Grant M. and Wronski, Julie},
	year = {2019},
	note = {Publisher: PsychOpen},
	keywords = {Hate speech, Moral foundations, Party cues, Political tolerance},
	pages = {695--725},
}

@article{dwoskin_facebook_2020,
	title = {Facebook to start policing anti-{Black} hate speech more aggressively than anti-{White} comments, documents show},
	url = {https://www.washingtonpost.com/technology/2020/12/03/facebook-hate-speech/},
	urldate = {2022-05-27},
	journal = {The Washington Post},
	author = {Dwoskin, Elizabeth and Tiku, Nitasha and Kelly, Heather},
	year = {2020},
}

@incollection{lakoff_political_2000,
	title = {"{Political} {Correctness}" and hate speech: {The} {Word} as {Sword}},
	isbn = {202220:12:48},
	abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
	booktitle = {The {Language} {War}},
	publisher = {University of California Press},
	author = {Lakoff, Robin Tolmach},
	year = {2000},
	doi = {10.1525/j.ctt1pp38b.7},
}

@book{butler_excitable_1997,
	title = {Excitable {Speech}},
	isbn = {978-1-135-23980-0},
	publisher = {Routledge},
	author = {Butler, Judith},
	year = {1997},
	doi = {10.4324/9780203948682},
}

@inproceedings{uyheng_identity-based_2021,
	title = {An {Identity}-{Based} {Framework} for {Generalizable} {Hate} {Speech} {Detection}},
	doi = {10.1007/978-3-030-80387-2_12},
	abstract = {This paper explores the viability of leveraging an identity-based framework for generalizable hate speech detection. Across a corpus of seven benchmark datasets, we find that hate speech consistently features higher levels of abusive and identity terms, robust to social …},
	booktitle = {International {Conference} on {Social} {Computing}, {Behavioral}-{Cultural} {Modeling} and {Prediction} and {Behavior} {Representation} in {Modeling} and {Simulation}},
	author = {Uyheng, Joshua and Carley, Kathleen M.},
	year = {2021},
	pages = {121--130},
}

@inproceedings{sanh_distilbert_2019,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {https://github.com/huggingface/transformers},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	booktitle = {5th {Workshop} on {Energy} {Efficient} {Machine} {Learning} and {Cognitive} {Computing}},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	year = {2019},
}

@inproceedings{mathew_hatexplain_2021,
	title = {{HateXplain}: {A} {Benchmark} {Dataset} for {Explainable} {Hate} {Speech} {Detection}},
	url = {https://github.com/punyajoy/HateXplain},
	abstract = {Hate speech is a challenging issue plaguing the online social media. While better models for hate speech detection are continuously being developed, there is little research on the bias and interpretability aspects of hate speech. In this paper, we introduce HateXplain, the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and observe that even models that perform very well in classification do not score high on explainability metrics like model plau-sibility and faithfulness. We also observe that models, which utilize the human rationales for training, perform better in reducing unintended bias towards target communities. We have made our code and dataset public 1 for other researchers 2 .},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
	year = {2021},
	pages = {14867--14875},
}

@article{Mendelsohn2020,
	title = {A framework for the computational linguistic analysis of dehumanization},
	volume = {3},
	issn = {2624-8212},
	url = {https://doi.org/10.3389/frai.2020.00055},
	doi = {10.3389/frai.2020.00055},
	abstract = {Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online.},
	number = {55},
	urldate = {2022-05-17},
	journal = {Frontiers in Artificial Intelligence},
	author = {Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},
	year = {2020},
	note = {arXiv: 2003.03014},
	keywords = {Computational sociolinguistics, Dehumanization, LGBTQ, Language change, Lexical variation, Media, New York Times},
	pages = {1--24},
}

@article{campbell_thousands_2016,
	title = {Thousands of positive reviews: {Distributed} mentoring in online fan communities},
	volume = {27},
	doi = {10.1145/2818048.2819934},
	abstract = {Young people worldwide are participating in ever-increasing numbers in online fan communities. Far from mere shallow repositories of pop culture, these sites are accumulating significant evidence that sophisticated informal learning is taking place online in novel and unexpected ways. In order to understand and analyze in more detail how learning might be occurring, we conducted an in-depth nine-month ethnographic investigation of online fanfiction communities, including participant observation and fanfiction author interviews. Our observations led to the development of a theory we term distributed mentoring, which we present in detail in this paper. Distributed mentoring exemplifies one instance of how networked technology affords new extensions of behaviors that were previously bounded by time and space. Distributed mentoring holds potential for application beyond the spontaneous mentoring observed in this investigation and may help students receive diverse, thoughtful feedback in formal learning environments as well.},
	journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
	author = {Campbell, Julie Ann and Aragon, Cecilia and Davis, Katie and Evans, Sarah and Evans, Abigail and Randall, David P.},
	year = {2016},
	note = {ISBN: 9781450335928},
	keywords = {Digital youth, Distributed mentoring, Fanfiction, Informal learning, Mentoring, Online communities},
	pages = {691--704},
}

@incollection{graham_moral_2013,
	title = {Moral {Foundations} {Theory}: {The} {Pragmatic} {Validity} of {Moral} {Pluralism}},
	volume = {47},
	abstract = {Where does morality come from? Why are moral judgments often so similar across cultures, yet sometimes so variable? Is morality one thing, or many? Moral Foundations Theory (MFT) was created to answer these questions. In this chapter, we describe the origins, assumptions, and current conceptualization of the theory and detail the empirical findings that MFT has made possible, both within social psychology and beyond. Looking toward the future, we embrace several critiques of the theory and specify five criteria for determining what should be considered a foundation of human morality. Finally, we suggest a variety of future directions for MFT and moral psychology. © 2013 Elsevier Inc.},
	booktitle = {Advances in {Experimental} {Social} {Psychology}},
	publisher = {Academic Press Inc.},
	author = {Graham, Jesse and Haidt, Jonathan and Koleva, Sena and Motyl, Matt and Iyer, Ravi and Wojcik, Sean P. and Ditto, Peter H.},
	year = {2013},
	doi = {10.1016/B978-0-12-407236-7.00002-4},
	note = {ISSN: 00652601},
	keywords = {Cultural learning, Intuition, Method-theory coevolution, Morality, Nativism, Pluralism},
	pages = {55--130},
}

@article{graham_liberals_2009,
	title = {Liberals and {Conservatives} {Rely} on {Different} {Sets} of {Moral} {Foundations}},
	volume = {96},
	issn = {00223514},
	doi = {10.1037/a0015141},
	abstract = {How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt \& J. Graham, 2007; J. Haidt \& C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), "sacredness" reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American "culture war.". © 2009 American Psychological Association.},
	number = {5},
	journal = {Journal of Personality and Social Psychology},
	author = {Graham, Jesse and Haidt, Jonathan and Nosek, Brian A.},
	month = may,
	year = {2009},
	pmid = {19379034},
	keywords = {conservative, ideology, liberal, morality},
	pages = {1029--1046},
}

@article{yin_towards_2021,
	title = {Towards generalisable hate speech detection: a review on obstacles and solutions},
	volume = {7},
	issn = {23765992},
	doi = {10.7717/PEERJ-CS.598},
	abstract = {Hate speech is one type of harmful online content which directly attacks or promotes hate towards a group or an individual member based on their actual or perceived aspects of identity, such as ethnicity, religion, and sexual orientation. With online hate speech on the rise, its automatic detection as a natural language processing task is gaining increasing interest. However, it is only recently that it has been shown that existing models generalise poorly to unseen data. This survey paper attempts to summarise how generalisable existing hate speech detection models are and the reasons why hate speech models struggle to generalise, sums up existing attempts at addressing the main obstacles, and then proposes directions of future research to improve generalisation in hate speech detection.},
	journal = {PeerJ Computer Science},
	author = {Yin, Wenjie and Zubiaga, Arkaitz},
	year = {2021},
	note = {arXiv: 2102.08886
Publisher: PeerJ Inc.},
	keywords = {Abusive language, Artificial Intelligence, Computational Linguistics, Data Mining and Machine Learning, Generalisation, Hate speech, Literature review, Natural Language and Speech, Social Computing, Social media, Text classification},
	pages = {1--38},
}

@article{pathak_method_2021,
	title = {A {Method} to {Analyze} {Multiple} {Social} {Identities} in {Twitter} {Bios}},
	volume = {5},
	issn = {25730142},
	doi = {10.1145/3479502},
	abstract = {Twitter users signal social identity in their profile descriptions, or bios, in a number of important but complex ways that are not well-captured by existing characterizations of how identity is expressed in language. Better ways of defining and measuring these expressions may therefore be useful both in understanding how social identity is expressed in text, and how the self is presented on Twitter. To this end, the present work makes three contributions. First, using qualitative methods, we identify and define the concept of a personal identifier, which is more representative of the ways in which identity is signaled in Twitter bios. Second, we propose a method to extract all personal identifiers expressed in a given bio. Finally, we present a series of validation analyses that explore the strengths and limitations of our proposed method. Our work opens up exciting new opportunities at the intersection between the social psychological study of social identity and the study of how we compose the self through markers of identity on Twitter and in social media more generally.},
	number = {CSCW2},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Pathak, Arjunil and Madani, Navid and Joseph, Kenneth},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.14120
Publisher: Association for Computing Machinery},
	keywords = {computational social science, self-presentation, social identity, twitter},
}

@article{kinnvall_exploring_2022,
	title = {Exploring the populist ‘mind’: {Anxiety}, fantasy, and everyday populism},
	issn = {1369-1481},
	url = {http://journals.sagepub.com/doi/10.1177/13691481221075925},
	doi = {10.1177/13691481221075925},
	abstract = {{\textless}p{\textgreater}This article is focused on the appeal of far-right populist politics in the everyday and how this appeal is related to continuity and change in the global order. Contemporary societies have witnessed an upsurge of populist movements and groups set on filling a political space by appealing to a population in search of solutions to an ever-changing political and economic landscape. Here, we specifically highlight the role of ontological insecurity, fantasy narratives, and emotional governance as critical for understanding far-right populist politics. The analysis consequently attends to the centrality of gendered and racialised narratives and to how these are fuelled by feelings of pride, shame, vulnerability, and insecurity. The aim is to show how structures and emotions work in tandem to create far-right support and how these developments are similar across Western and non-Western contexts. Particular attention is paid to far-right narratives that pertain to the Covid-19 pandemic.{\textless}/p{\textgreater}},
	journal = {The British Journal of Politics and International Relations},
	author = {Kinnvall, Catarina and Svensson, Ted},
	month = feb,
	year = {2022},
	pages = {1--17},
}

@inproceedings{mondal_measurement_2017,
	title = {A measurement study of hate speech in social media},
	isbn = {978-1-4503-4708-2},
	doi = {10.1145/3078714.3078723},
	abstract = {Social media platforms provide an inexpensive communication medium that allows anyone to quickly reach millions of users. Consequently, in these platforms anyone can publish content and anyone interested in the content can obtain it, representing a transformative revolution in our society. However, this same potential of social media systems brings together an important challenge - these systems provide space for discourses that are harmful to certain groups of people. This challenge manifests itself with a number of variations, including bullying, offensive content, and hate speech. Specifically, authorities of many countries today are rapidly recognizing hate speech as a serious problem, specially because it is hard to create barriers on the Internet to prevent the dissemination of hate across countries or minorities. In this paper, we provide the first of a kind systematic large scale measurement and analysis study of hate speech in online social media. We aim to understand the abundance of hate speech in online social media, the most common hate expressions, the effect of anonymity on hate speech and the most hated groups across regions. In order to achieve our objectives, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both of these systems. Our results identify hate speech forms and unveil a set of important patterns, providing not only a broader understanding of online hate speech, but also offering directions for detection and prevention approaches.},
	booktitle = {{HT} 2017 - {Proceedings} of the 28th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Mondal, Mainack and Silva, Leandro Araújo and Benevenuto, Fabrício},
	month = jul,
	year = {2017},
	keywords = {Anonymity, Hate speech, Pattern recognition, Social media, Twitter, Whisper},
	pages = {85--94},
}

@article{kennedy_constructing_2020,
	title = {Constructing interval variables via faceted {Rasch} measurement and multitask deep learning: a hate speech application},
	url = {http://arxiv.org/abs/2009.10277},
	abstract = {We propose a general method for measuring complex variables on a continuous, interval spectrum by combining supervised deep learning with the Constructing Measures approach to faceted Rasch item response theory (IRT). We decompose the target construct, hate speech in our case, into multiple constituent components that are labeled as ordinal survey items. Those survey responses are transformed via IRT into a debiased, continuous outcome measure. Our method estimates the survey interpretation bias of the human labelers and eliminates that influence on the generated continuous measure. We further estimate the response quality of each labeler using faceted IRT, allowing responses from low-quality labelers to be removed. Our faceted Rasch scaling procedure integrates naturally with a multitask deep learning architecture for automated prediction on new data. The ratings on the theorized components of the target outcome are used as supervised, ordinal variables for the neural networks' internal concept learning. We test the use of an activation function (ordinal softmax) and loss function (ordinal cross-entropy) designed to exploit the structure of ordinal outcome variables. Our multitask architecture leads to a new form of model interpretation because each continuous prediction can be directly explained by the constituent components in the penultimate layer. We demonstrate this new method on a dataset of 50,000 social media comments sourced from YouTube, Twitter, and Reddit and labeled by 11,000 U.S.-based Amazon Mechanical Turk workers to measure a continuous spectrum from hate speech to counterspeech. We evaluate Universal Sentence Encoders, BERT, and RoBERTa as language representation models for the comment text, and compare our predictive accuracy to Google Jigsaw's Perspective API models, showing significant improvement over this standard benchmark.},
	author = {Kennedy, Chris J. and Bacon, Geoff and Sahn, Alexander and von Vacano, Claudia},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.10277},
}

@article{voigt_language_2017,
	title = {Language from police body camera footage shows racial disparities in officer respect},
	volume = {114},
	issn = {10916490},
	doi = {10.1073/pnas.1702413114},
	abstract = {Using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. We develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. We find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. Such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police-community trust.},
	number = {25},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Voigt, Rob and Camp, Nicholas P. and Prabhakaran, Vinodkumar and Hamilton, William L. and Hetey, Rebecca C. and Griffiths, Camilla M. and Jurgens, David and Jurafsky, Dan and Eberhardt, Jennifer L.},
	month = jun,
	year = {2017},
	pmid = {28584085},
	note = {Publisher: National Academy of Sciences},
	pages = {6521--6526},
}

@techreport{fersini_overview_2018,
	title = {Overview of the {Evalita} 2018 {Task} on {Automatic} {Misogyny} {Identification} ({AMI})},
	url = {https://figure-eight.com/},
	abstract = {English. Automatic Misogyny Identification (AMI) is a new shared task proposed for the first time at the Evalita 2018 evaluation campaign. The AMI challenge , based on both Italian and English tweets, is distinguished into two subtasks, i.e. Subtask A on misogyny identification and Subtask B about misogynistic behaviour categorization and target classification. Regarding the Italian language, we have received a total of 13 runs for Sub-task A and 11 runs for Subtask B. Concerning the English language, we received 26 submissions for Subtask A and 23 runs for Subtask B. The participating systems have been distinguished according to the language, counting 6 teams for Italian and 10 teams for English. We present here an overview of the AMI shared task, the datasets, the evaluation methodology, the results obtained by the participants and a discussion of the methodology adopted by the teams. Finally, we draw some conclusions and discuss future work.},
	author = {Fersini, Elisabetta and Nozza, Debora and Rosso, Paolo},
	year = {2018},
}

@inproceedings{mathew_thou_2019,
	title = {Thou {Shalt} {Not} {Hate}: {Countering} {Online} {Hate} {Speech}},
	url = {https://goo.gl/zEu4aX,},
	abstract = {Hate content in social media is ever increasing.},
	booktitle = {Proceedings of the {Thirteenth} {International} {AAAI} {Conference} on {Web} and {Social} {Media} ({ICWSM} 2019)},
	author = {Mathew, Binny and Saha, Punyajoy and Tharad, Hardik and Rajgaria, Subham and Singhania, Prajwal and Kalyan Maity, Suman and Goyal, Pawan and Mukherjee, Animesh},
	year = {2019},
}

@article{Shen2018,
	title = {Practical text phylogeny for real-world settings},
	volume = {6},
	issn = {21693536},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412174},
	doi = {10.1109/ACCESS.2018.2856865},
	abstract = {The ease with which one can edit and redistribute digital documents on the Internet is one of modernity's great achievements, but it also leads to some vexing problems. With growing academic interest in the study of the evolution of digital writing on the one hand and the rise of disinformation on the other, the problem of identifying the relationship between texts with similar content is becoming more important. Traditional vector space representations of texts have made progress in solving this problem when it is cast as a reconstruction task that organizes related texts into a tree expressing relationships - this is dubbed text phylogeny in the information forensics literature. However, as new text representation methods have been successfully applied to many other text analysis problems, it is worth investigating if they too are used in text phylogeny tree reconstruction. In this paper, we explore the use of word embeddings as a text representation method, with the aim of trying to improve the accuracy of reconstructed phylogeny trees for real-world data and compare it with other widely used text representation methods. We evaluate the performance on established benchmarks for this task: a synthetic data set and data collected from Wikipedia. We also apply our framework to a new data set of fan fiction based on some famous fairy tales. Experimental results show that word embeddings are competitive with other feature sets for the published benchmarks, and are highly effective for creative writing. © 2013 IEEE.},
	urldate = {2022-02-23},
	journal = {IEEE Access},
	author = {Shen, Bingyu and Forstall, Christopher W. and Rocha, Anderson De Rezende and Scheirer, Walter J.},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Digital humanities, Forensics, Machine learning, Natural language processing, Text phylogeny, Word embeddings},
	pages = {41002--41012},
}

@inproceedings{elsherief_latent_2021,
	title = {Latent {Hatred}: {A} {Benchmark} for {Understanding} {Implicit} {Hate} {Speech}},
	abstract = {Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxon-omy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue. To down-load the data, see https://github.com/ GT-SALT/implicit-hate},
	urldate = {2022-02-23},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
	year = {2021},
	pages = {345--363},
}

@article{nurik_men_2019,
	title = {"{Men} {Are} {Scum}": {Self}-{Regulation}, {Hate} {Speech}, and {Gender}-{Based} {Censorship} on {Facebook}},
	volume = {13},
	url = {http://ijoc.org.},
	abstract = {Because social media sites are self-regulating, each site has developed its own community standards, which serve as regulatory tools. However, the processes of content moderation are often unclear, subjective, and discriminatory. Drawing from a series of interviews with individuals in the "Men Are Scum" movement, this article describes the experiences of women who have been censored on Facebook and explores whether self-regulatory processes on this platform are distinctly gendered. It asserts that both explicit censorship (e.g., limited displays of the body) and implicit censorship (e.g., rampant and unchecked hate speech silencing women's voices) are operative on Facebook, limiting women's expressive potentiality. Thus, this article proposes the term "gender-based censorship" as a lens through which to understand women's experiences on Facebook. These findings help reveal the pitfalls of industry self-regulation in which profit motives are prioritized over protection of users (especially those who may be marginalized offline).},
	journal = {International Journal of Communication},
	author = {Nurik, Chloé},
	year = {2019},
	keywords = {Facebook, content moderation, gender-based censorship, self-regulation, social media},
	pages = {2878--2898},
}

@article{Roberts2014,
	title = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	volume = {58},
	issn = {00925853},
	url = {http://doi.wiley.com/10.1111/ajps.12103},
	doi = {10.1111/ajps.12103},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	year = {2014},
	pages = {1064--1082},
}

@article{griffiths_topics_2007,
	title = {Topics in semantic representation},
	volume = {114},
	issn = {0033295X},
	doi = {10.1037/0033-295X.114.2.211},
	abstract = {Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure. PsycINFO Database Record (c) 2007 APA, all rights reserved.},
	number = {2},
	journal = {Psychological Review},
	author = {Griffiths, Thomas L. and Steyvers, Mark and Tenenbaum, Joshua B.},
	month = apr,
	year = {2007},
	pmid = {17500626},
	keywords = {Bayesian models, Computational models, Probabilistic models, Semantic memory, Semantic representation},
	pages = {211--244},
}

@article{miao_language_2016,
	title = {Language as a {Latent} {Variable}: {Discrete} {Generative} {Models} for {Sentence} {Compression}},
	issn = {978-3-319-10589-5},
	url = {http://arxiv.org/abs/1609.07317},
	doi = {10.1007/978-3-319-10590-1_53},
	abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
	author = {Miao, Yishu and Blunsom, Phil},
	year = {2016},
	pmid = {26353135},
	note = {arXiv: 1609.07317
ISBN: 978-3-319-10589-5},
	pages = {319--328},
}

@article{hill_junk_1995,
	title = {{JUNK} {SPANISH}, {COYERT} {RACISM}, {AND} {THE} ({LEAKY}) {BOUNDARY} {BETWEEN} {PUBLTC} {AND} {PRIVATE} {SPHERES}},
	volume = {5},
	number = {2},
	journal = {Pragmatics},
	author = {Hill, Jane H},
	year = {1995},
	pages = {197--212},
}

@book{hill_everyday_2008,
	title = {The {Everyday} {Language} of {White} {Racism}},
	publisher = {Wiley-Blackwell},
	author = {Hill, Jane H.},
	year = {2008},
}

@article{ohanlon_what_2019,
	title = {What kills us and what moves us: {A} comparative discourse analysis of heart disease and breast cancer},
	volume = {5},
	issn = {20552076},
	doi = {10.1177/2055207619844865},
	abstract = {Introduction: Heart disease kills nearly 300,000 US women annually, while approximately 40,000 US women die of breast cancer. Breast cancer online patient communities are well known for their high engagement and emotional support. This exploratory study compared social media discourse on breast cancer with discourse related to heart disease. Methods: Computer-assisted text analysis of two corpora composed of Twitter posts using \#BreastCancer and \#HeartDisease hashtags from December 2013 to December 2014. Lexical analysis (word and hashtag level) used AntConc software and lexicogrammatical analysis (style and stance) was conducted with DocuScope. Results: The \#BreastCancer corpus consisted of 592,046 posts, 57\% of which were not original to the user (retweets). \#HeartDisease had 269,769 posts (13\% retweets). Social media discourse about \#BreastCancer and \#HeartDisease drew attention to women, new developments, appeals for help and disease risks. The \#BreastCancer corpus incorporates gendered language and associations with art and activism, while posts about \#HeartDisease were discussed scientifically in concert with other diseases. The \#BreastCancer corpus uniquely included community-specific initialism hashtags. Stance analysis of the \#BreastCancer corpus revealed more socially oriented posts, marked by language of constructive reasoning, inclusive language and abstract thought, while \#HeartDisease corpus posts were more scholarly, used contingent and oppositional reasoning, language from institutional and academic registers, citations and meta-discourse. Conclusion: The \#HeartDisease social media community is less engaged, and content is less specific to both the disease and individual experience than \#BreastCancer. Cultivating a women-focused heart disease online community might replicate some of the \#BreastCancer community’s successes.},
	journal = {Digital Health},
	author = {O’Hanlon, Claire E.},
	month = apr,
	year = {2019},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {Social media, breast cancer, breast neoplasms, chronic disease, discourse analysis, female, health communication, heart diseases, lexical analysis, lexicogrammatical analysis},
}

@article{cheng_problematic_2021,
	title = {The {Problematic} {Concept} of {Native} {Speaker} in {Psycholinguistics}: {Replacing} {Vague} and {Harmful} {Terminology} {With} {Inclusive} and {Accurate} {Measures}},
	volume = {12},
	issn = {16641078},
	doi = {10.3389/fpsyg.2021.715843},
	abstract = {Though the term NATIVE SPEAKER/SIGNER is frequently used in language research, it is inconsistently conceptualized. Factors, such as age, order, and context of acquisition, in addition to social/cultural identity, are often differentially conflated. While the ambiguity and harmful consequences of the term NATIVE SPEAKER have been problematized across disciplines, much of this literature attempts to repurpose the term in order to include and/or exclude certain populations. This paper problematizes NATIVE SPEAKER within psycholinguistics, arguing that the term is both unhelpful to rigorous theory construction and harmful to marginalized populations by reproducing normative assumptions about behavior, experience, and identity. We propose that language researchers avoid NATIVE SPEAKER altogether, and we suggest alternate ways of characterizing language experience/use. The vagueness of NATIVE SPEAKER can create problems in research design (e.g., through systematically excluding certain populations), recruitment (as participants’ definitions might diverge from researchers’), and analysis (by distilling continuous factors into under-specified binary categories). This can result in barriers to cross-study comparison, which is particularly concerning for theory construction and replicability. From a research ethics perspective, it matters how participants are characterized and included: Excluding participants based on binary/essentialist conceptualizations of nativeness upholds deficit perspectives toward multilingualism and non-hegemonic modes of language acquisition. Finally, by implicitly assuming the existence of a critical period, NATIVE SPEAKER brings with it theoretical baggage which not all researchers may want to carry. Given the issues above and how ‘nativeness’ is racialized (particularly in European and North American contexts), we ask that researchers consider carefully whether exclusion of marginalized/minoritized populations is necessary or justified—particularly when NATIVE SPEAKER is used only as a way to achieve linguistic homogeneity. Instead, we urge psycholinguists to explicitly state the specific axes traditionally implied by NATIVENESS that they wish to target. We outline several of these (e.g., order of acquisition, allegiance, and comfort with providing intuitions) and give examples of how to recruit and describe participants while eschewing NATIVE SPEAKER. Shifting away from harmful conventions, such as NATIVE SPEAKER, will not only improve research design and analysis, but also is one way we can co-create a more just and inclusive field.},
	journal = {Frontiers in Psychology},
	author = {Cheng, Lauretta S.P. and Burgess, Danielle and Vernooij, Natasha and Solís-Barroso, Cecilia and McDermott, Ashley and Namboodiripad, Savithry},
	month = sep,
	year = {2021},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {language experience, multilingualism, native speaker, psycholinguistics, research methods},
}

@article{vidgen_directions_2020,
	title = {Directions in abusive language training data, a systematic review: {Garbage} in, garbage out},
	volume = {15},
	issn = {19326203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243300#references},
	doi = {10.1371/journal.pone.0243300},
	abstract = {Data-driven and machine learning based approaches for detecting, categorising and measuring abusive content such as hate speech and harassment have gained traction due to their scalability, robustness and increasingly high performance. Making effective detection systems for abusive content relies on having the right training datasets, reflecting a widely accepted mantra in computer science: Garbage In, Garbage Out. However, creating training datasets which are large, varied, theoretically-informed and that minimize biases is difficult, laborious and requires deep expertise. This paper systematically reviews 63 publicly available training datasets which have been created to train abusive language classifiers. It also reports on creation of a dedicated website for cataloguing abusive language data hatespeechdata.com. We discuss the challenges and opportunities of open science in this field, and argue that although more dataset sharing would bring many benefits it also poses social and ethical risks which need careful consideration. Finally, we provide evidencebased recommendations for practitioners creating new abusive content training datasets.},
	number = {12 December},
	urldate = {2022-01-09},
	journal = {PLoS ONE},
	author = {Vidgen, Bertie and Derczynski, Leon},
	month = dec,
	year = {2020},
	pmid = {33370298},
	note = {Publisher: Public Library of Science},
}

@inproceedings{wiegand_implicitly_2021,
	title = {Implicitly {Abusive} {Comparisons}-{A} {New} {Dataset} and {Linguistic} {Analysis}},
	url = {http://thelawdictionary.org/},
	abstract = {We examine the task of detecting implicitly abusive comparisons (e.g. Your hair looks like you have been electrocuted). Implicitly abusive comparisons are abusive comparisons in which abusive words (e.g. dumbass or scum) are absent. We detail the process of creating a novel dataset for this task via crowdsourcing that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present classification experiments that include a range of linguistic features that help us better understand the mechanisms underlying abusive comparisons.},
	author = {Wiegand, Michael and Geulig, Maja and Ruppenhofer, Josef},
	year = {2021},
	pages = {358--368},
}

@article{ng_coordinating_2021,
	title = {Coordinating {Narratives} and the {Capitol} {Riots} on {Parler}},
	url = {http://arxiv.org/abs/2109.00945},
	abstract = {Coordinated disinformation campaigns are used to influence social media users, potentially leading to offline violence. In this study, we introduce a general methodology to uncover coordinated messaging through analysis of user parleys on Parler. The proposed method constructs a user-to-user coordination network graph induced by a user-to-text graph and a text-to-text similarity graph. The text-to-text graph is constructed based on the textual similarity of Parler posts. We study three influential groups of users in the 6 January 2020 Capitol riots and detect networks of coordinated user clusters that are all posting similar textual content in support of different disinformation narratives related to the U.S. 2020 elections.},
	author = {Ng, Lynnette Hui Xian and Cruickshank, Iain and Carley, Kathleen M.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.00945},
}

@incollection{costello_hate_2020,
	title = {Hate speech in online spaces},
	isbn = {978-3-319-78440-3},
	abstract = {The ever-increasing centrality of the Internet, especially social media, in people’s everyday lives has led to heightened fears over the growth of hateful material online. Indeed, online hate, or cyberhate, is rapidly proliferating on the Internet, resulting in more people encountering it. In turn, mounting concerns over the effects of exposure to hateful online content have led to a swell in research on the topic. This chapter offers a summary of several key studies that examine cyberhate, beginning with a brief overview of the emergence of online hate. A detailing of the various types of hate currently permeating cyberspace follows. This segues into a discussion of factors that lead online users to come into contact with online hate, be directly victimized by cyberhate, as well as produce this type of content. The chapter concludes with a discussion of the future of cyberhate, emphasizing some of the challenges associated with reducing hateful online material in an increasingly expansive and anonymous online universe.},
	booktitle = {The {Palgrave} {Handbook} of {International} {Cybercrime} and {Cyberdeviance}},
	publisher = {Palgrave Macmillan},
	author = {Costello, Matthew and Hawdon, James},
	month = jan,
	year = {2020},
	doi = {10.1007/978-3-319-78440-3_60},
	keywords = {Cyberhate, Online extremism, Online hate},
	pages = {1397--1416},
}

@article{rieger_assessing_2021,
	title = {Assessing the {Extent} and {Types} of {Hate} {Speech} in {Fringe} {Communities}: {A} {Case} {Study} of {Alt}-{Right} {Communities} on 8chan, 4chan, and {Reddit}},
	volume = {7},
	issn = {20563051},
	doi = {10.1177/20563051211052906},
	abstract = {Recent right-wing extremist terrorists were active in online fringe communities connected to the alt-right movement. Although these are commonly considered as distinctly hateful, racist, and misogynistic, the prevalence of hate speech in these communities has not been comprehensively investigated yet, particularly regarding more implicit and covert forms of hate. This study exploratively investigates the extent, nature, and clusters of different forms of hate speech in political fringe communities on Reddit, 4chan, and 8chan. To do so, a manual quantitative content analysis of user comments (N = 6,000) was combined with an automated topic modeling approach. The findings of the study not only show that hate is prevalent in all three communities (24\% of comments contained explicit or implicit hate speech), but also provide insights into common types of hate speech expression, targets, and differences between the studied communities.},
	number = {4},
	urldate = {2022-02-09},
	journal = {Social Media and Society},
	author = {Rieger, Diana and Kümpel, Anna Sophie and Wich, Maximilian and Kiening, Toni and Groh, Georg},
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {4chan, 8chan, Reddit, alt-right, content analysis, fringe communities, hate speech, topic modeling},
}

@inproceedings{kennedy_introducing_2022,
	title = {Introducing the {Gab} {Hate} {Corpus}: defining and applying hate-based rhetoric to social media posts at scale},
	url = {https://link.springer.com/10.1007/s10579-021-09569-x},
	doi = {10.1007/s10579-021-09569-x},
	booktitle = {Language {Resources} and {Evaluation}},
	author = {Kennedy, Brendan and Atari, Mohammad and Davani, Aida Mostafazadeh and Yeh, Leigh and Omrani, Ali and Kim, Yehsong and Coombs, Kris and Havaldar, Shreya and Portillo-Wightman, Gwenyth and Gonzalez, Elaine and Hoover, Joe and Azatian, Aida and Hussain, Alyzeh and Lara, Austin and Cardenas, Gabriel and Omary, Adam and Park, Christina and Wang, Xin and Wijaya, Clarisa and Zhang, Yong and Meyerowitz, Beth and Dehghani, Morteza},
	month = jan,
	year = {2022},
	note = {ISSN: 1574-020X},
}

@article{uyheng_characterizing_2021,
	title = {Characterizing network dynamics of online hate communities around the {COVID}-19 pandemic},
	volume = {6},
	issn = {23648228},
	url = {https://doi.org/10.1007/s41109-021-00362-x},
	doi = {10.1007/s41109-021-00362-x},
	abstract = {Hate speech has long posed a serious problem for the integrity of digital platforms. Although significant progress has been made in identifying hate speech in its various forms, prevailing computational approaches have tended to consider it in isolation from the community-based contexts in which it spreads. In this paper, we propose a dynamic network framework to characterize hate communities, focusing on Twitter conversations related to COVID-19 in the United States and the Philippines. While average hate scores remain fairly consistent over time, hate communities grow increasingly organized in March, then slowly disperse in the succeeding months. This pattern is robust to fluctuations in the number of network clusters and average cluster size. Infodemiological analysis demonstrates that in both countries, the spread of hate speech around COVID-19 features similar reproduction rates as other COVID-19 information on Twitter, with spikes in hate speech generation at time points with highest community-level organization of hate speech. Identity analysis further reveals that hate in the US initially targets political figures, then grows predominantly racially charged; in the Philippines, targets of hate consistently remain political over time. Finally, we demonstrate that higher levels of community hate are consistently associated with smaller, more isolated, and highly hierarchical network clusters across both contexts. This suggests potentially shared structural conditions for the effective spread of hate speech in online communities even when functionally targeting distinct identity groups. Our findings bear theoretical and methodological implications for the scientific study of hate speech and understanding the pandemic’s broader societal impacts both online and offline.},
	number = {1},
	journal = {Applied Network Science},
	author = {Uyheng, Joshua and Carley, Kathleen M.},
	year = {2021},
	note = {Publisher: Springer International Publishing},
	keywords = {COVID-19 pandemic, Constructural theory, Dynamic network analysis, Hate speech, Infodemic},
}

@article{col_david_beskow_social_2019,
	title = {Social {Cybersecurity} {An} {Emerging} {National} {Security} {Requirement}},
	journal = {Military Review},
	author = {Col David Beskow, Lt M and Army Kathleen Carley, Us M},
	year = {2019},
	keywords = {English Military Review March-April 2019 Beskow},
}

@inproceedings{vidgen_introducing_2021,
	title = {Introducing {CAD}: the {Contextual} {Abuse} {Dataset}},
	url = {https://github.com/dongpng/cad_},
	abstract = {Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Vidgen, Bertie and Nguyen, Dong and Margetts, Helen and Rossini, Patricia and Tromble, Rebekah},
	year = {2021},
	pages = {2289--2303},
}

@inproceedings{sanguinetti_italian_2018,
	title = {An {Italian} {Twitter} {Corpus} of {Hate} {Speech} against {Immigrants}},
	url = {http://hatespeech.di.unito.},
	abstract = {The paper describes a recently-created Twitter corpus of about 6,000 tweets, annotated for hate speech against immigrants, and developed to be a reference dataset for an automatic system of hate speech monitoring. The annotation scheme was therefore specifically designed to account for the multiplicity of factors that can contribute to the definition of a hate speech notion, and to offer a broader tagset capable of better representing all those factors, which may increase, or rather mitigate, the impact of the message. This resulted in a scheme that includes, besides hate speech, the following categories: aggressiveness, offensiveness, irony, stereotype, and (on an experimental basis) intensity. The paper hereby presented namely focuses on how this annotation scheme was designed and applied to the corpus. In particular, also comparing the annotation produced by CrowdFlower contributors and by expert annotators, we make some remarks about the value of the novel resource as gold standard, which stems from a preliminary qualitative analysis of the annotated data and on future corpus development.},
	booktitle = {Proceedings  of  the  eleventh  international  conference  on  language resources  and  evaluation  ({LREC}’18).},
	author = {Sanguinetti, Manuela and Poletto, Fabio and Bosco, Cristina and Patti, Viviana and Stranisci, Marco},
	year = {2018},
	keywords = {Italian, hate speech, immigrants, social media},
	pages = {2798--2895},
}

@article{reyes_are_2002,
	title = {“{Are} you losing your culture?”: poetics, indexicality and {Asian} {American} identity},
	volume = {4},
	number = {2},
	journal = {Discourse Studies},
	author = {Reyes, Angela},
	year = {2002},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {183--199},
}

@book{johnstone2007discourse,
	title = {Discourse {Analysis}},
	isbn = {978-1-4051-4427-8},
	url = {https://books.google.com/books?id=Cf4vWV-uqTIC},
	publisher = {Wiley},
	author = {Johnstone, Barbara},
	year = {2018},
	note = {Series Title: Introducing Linguistics},
}

@inproceedings{Prabhakaran2014,
	title = {Gender and {Power}: {How} {Gender} and {Gender} {Environment} {Affect} {Manifestations} of {Power}},
	isbn = {978-1-937284-96-1},
	abstract = {We investigate the interaction of power, gender, and language use in the Enron email corpus. We present a freely available extension to the Enron corpus, with the gender of senders of 87\% messages reliably identified. Using this data, we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power: women managers use face-saving communicative strategies, and women use language more explicitly than men to create and maintain social relations. We introduce the notion of “gender environment” to the computational study of written conversations; we interpret this notion as the gender makeup of an email thread, and show that some manifestations of power differ significantly between gender environments. Finally, we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions.},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2014)},
	author = {Prabhakaran, Vinodkumar and Reid, Emily E and Rambow, Owen},
	year = {2014},
	pages = {1965--1976},
}

@techreport{etta_comparing_2021,
	title = {Comparing the impact of social media regulations on news consumption {Journal}: {IEEE} {Transactions} on {Computational} {Social} {Systems} {IEEE} {Transactions} on {Computational} {Social} {Systems} {Comparing} the impact of social media regulations on news consumption},
	url = {http://www.michaelshell.org/contact.html},
	abstract = {Users online tend to consume information adhering to their system of beliefs and to ignore dissenting information. During the COVID-19 pandemic, users get exposed to a massive amount of information about a new topic having a high level of uncertainty. In this paper, we analyze two social media that enforced opposite moderation methods, Twitter and Gab, to assess the interplay between news consumption and content regulation concerning COVID-19. We compare the two platforms on about three million pieces of content analyzing user interaction with respect to news articles. We first describe users' consumption patterns on the two platforms focusing on the political leaning of news outlets. Finally, we characterize the echo chamber effect by modeling the dynamics of users' interaction networks. Our results show that the presence of moderation pursued by Twitter produces a significant reduction of questionable content, with a consequent affiliation towards reliable sources in terms of engagement and comments. Conversely, the lack of clear regulation on Gab results in the tendency of the user to engage with both types of content, showing a slight preference for the questionable ones which may account for a dissing/endorsement behavior. Twitter users show segregation towards reliable content with a uniform narrative. Gab, instead, offers a more heterogeneous structure where users, independently of their leaning, follow people who are slightly polarized towards questionable news.},
	author = {Etta, Gabriele and Cinelli, Matteo and Galeazzi, Alessandro and Michele Valensise, Carlo and Quattrociocchi, Walter and Conti, Mauro and Member, Senior},
	year = {2021},
	note = {Publication Title: IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS
Volume: 14
Issue: 8},
	keywords = {COVID-19 Index Terms-COVID-19, Echo Chambers, Fake news, News Consumption, Social Media},
}

@article{macavaney_hate_2019,
	title = {Hate speech detection: {Challenges} and solutions},
	volume = {14},
	issn = {19326203},
	doi = {10.1371/journal.pone.0221152},
	abstract = {As online content continues to grow, so does the spread of hate speech. We identify and examine challenges faced by online automatic approaches for hate speech detection in text. Among these difficulties are subtleties in language, differing definitions on what constitutes hate speech, and limitations of data availability for training and testing of these systems. Furthermore, many recent approaches suffer from an interpretability problem—that is, it can be difficult to understand why the systems make the decisions that they do. We propose a multi-view SVM approach that achieves near state-of-the-art performance, while being simpler and producing more easily interpretable decisions than neural methods. We also discuss both technical and practical challenges that remain for this task.},
	number = {8},
	journal = {PLoS ONE},
	author = {MacAvaney, Sean and Yao, Hao Ren and Yang, Eugene and Russell, Katina and Goharian, Nazli and Frieder, Ophir},
	month = aug,
	year = {2019},
	pmid = {31430308},
	note = {Publisher: Public Library of Science},
}

@inproceedings{ousidhoum_comparative_2020,
	title = {Comparative {Evaluation} of {Label}-{Agnostic} {Selection} {Bias} in {Multilingual} {Hate} {Speech} {Datasets}},
	url = {https://hatebase.org/.},
	abstract = {Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.},
	author = {Ousidhoum, Nedjma and Song, Yangqiu and Yeung, Dit-Yan},
	year = {2020},
	pages = {2532},
}

@inproceedings{sinem_generating_2020,
	title = {Generating {Counter} {Narratives} against {Online} {Hate} {Speech}: {Data} and {Strategies}},
	url = {http://stoppinghate.getthetrollsout.org/},
	abstract = {Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred on-line. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses. Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively , employing large scale unsupervised language models such as GPT-2},
	publisher = {Association for Computational Linguistics},
	author = {Sinem, Serra and Glu, Tekiro˘ and Chung, Yi-Ling and Guerini, Marco},
	year = {2020},
	pages = {1177--1190},
}

@book{lavrakas_encyclopedia_2012,
	title = {Encyclopedia of {Survey} {Research} {Methods}},
	abstract = {Non-Probability Sampling},
	publisher = {Sage Publications, Inc.},
	author = {Lavrakas, Paul},
	month = may,
	year = {2012},
	doi = {10.4135/9781412963947},
	note = {Publication Title: Encyclopedia of Survey Research Methods},
}

@inproceedings{holstein_improving_2019,
	title = {Improving {Fairness} in {Machine} {Learning} {Systems}: {What} {Do} {Industry} {Practitioners} {Need}?},
	isbn = {978-1-4503-5970-2},
	doi = {10.1145/3290605.3300830},
	abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.},
	author = {Holstein, Kenneth and Wortman Vaughan, Jennifer and Daumé, Hal and Dudik, Miro and Wallach, Hanna},
	year = {2019},
	keywords = {acm reference format, algorithmic bias, algorithmic bias, fair machine learning, product t, empirical study, fair machine learning, finding, need-, product teams, ux of machine learning},
	pages = {1--16},
}

@inproceedings{zampieri_predicting_2019,
	title = {Predicting the {Type} and {Target} of {Offensive} {Posts} in {Social} {Media}},
	url = {http://bit.ly/2FhLMVz},
	abstract = {As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather fo-cused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.},
	publisher = {Association for Computational Linguistics},
	author = {Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh},
	year = {2019},
	pages = {1415--1420},
}

@techreport{pohjonen_comparative_2019,
	title = {A {Comparative} {Approach} to {Social} {Media} {Extreme} {Speech}: {Online} {Hate} {Speech} as {Media} {Commentary}},
	url = {http://ijoc.org.},
	abstract = {By exploring lessons learned from Ethiopia and Finland, this article challenges two assumptions about online hate speech research. First, it challenges the assumption that the best way to understand controversial concepts such as online hate speech is to determine how closely they represent or mirror some underlying set of facts or state of affairs online or in social media. Second, it challenges the assumption that academic research should be seen as separate from the many controversies that surround online hate speech debates globally. In its place, the article proposes the theory of "commentary" as a comparative research framework aimed at explaining how the messy and complex world of online and social media practices is articulated as hate speech over other ways of imagining this growing problem in global digital media environments.},
	author = {Pohjonen, Matti},
	year = {2019},
	note = {Publication Title: International Journal of Communication
Volume: 13},
	keywords = {Ethiopia, Finland, commentary, comparative research, extreme speech, online hate speech},
	pages = {3088--3103},
}

@article{gaudette_upvoting_2021,
	title = {Upvoting extremism: {Collective} identity formation and the extreme right on {Reddit}},
	volume = {23},
	issn = {14617315},
	doi = {10.1177/1461444820958123},
	abstract = {Since the advent of the Internet, right-wing extremists and those who subscribe to extreme right views have exploited online platforms to build a collective identity among the like-minded. Research in this area has largely focused on extremists’ use of websites, forums, and mainstream social media sites, but overlooked in this research has been an exploration of the popular social news aggregation site Reddit. The current study explores the role of Reddit’s unique voting algorithm in facilitating “othering” discourse and, by extension, collective identity formation among members of a notoriously hateful subreddit community, r/The\_Donald. The results of the thematic analysis indicate that those who post extreme-right content on r/The\_Donald use Reddit’s voting algorithm as a tool to mobilize like-minded members by promoting extreme discourses against two prominent out-groups: Muslims and the Left. Overall, r/The\_Donald’s “sense of community” facilitates identity work among its members by creating an environment wherein extreme right views are continuously validated.},
	number = {12},
	journal = {New Media and Society},
	author = {Gaudette, Tiana and Scrivens, Ryan and Davies, Garth and Frank, Richard},
	month = dec,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Collective identity, Extreme right discourse, r/The\_Donald, social movement theory, social news aggregators},
	pages = {3491--3508},
}

@article{wong_reappropriation_2005,
	title = {The reappropriation of tongzhi},
	volume = {34},
	issn = {00474045},
	doi = {10.1017/S0047404505050281},
	abstract = {A general address term in Communist China, the Chinese word tongzhi 'comrade' was appropriated by gay rights activists in Hong Kong to refer to members of sexual minorities. It has positive connotations of respect, equality, and resistance. This article focuses on the reappropriation of this word by a mainstream newspaper in Hong Kong. The parodic use of tongzhi allows journalists to ridicule gay rights activists so as to increase the entertainment value of news stories. At the same time, it mocks activists' demand for equality and may lead to the pejoration of the term. This study provides synchronic evidence for sociolinguistic accounts that explain how lexical items may undergo pejoration because of the context of their use. It shows that because the meaning potential of a word is not bounded by the intentions of its users, words that marginalized groups have appropriated can be resignified yet again in hateful contexts. © 2005 Cambridge University Press Hong Kong, parody, appropriation, semantic change, meaning contestation.},
	number = {5},
	journal = {Language in Society},
	author = {Wong, Andrew D.},
	month = nov,
	year = {2005},
	pages = {763--793},
}

@article{erjavec_you_2012,
	title = {"{You} {Don}'t {Understand}, {This} is a {New} {War}!" {Analysis} of {Hate} {Speech} in {News} {Web} {Sites}' {Comments}},
	volume = {15},
	issn = {15205436},
	doi = {10.1080/15205436.2011.619679},
	abstract = {Because news websites' comments have become an important space of spreading hate speech, this article tries to contribute to uncovering the characteristics of Internet hate speech by combining discourse analyses of comments on Slovenian news websites with online in-depth interviews with producers of hate speech comments, researching their values, beliefs, and motives for production. Producers of hate speech use different strategies, mostly rearticulating the meaning of news items. The producers either are organized or act on their own initiative. The main motive of soldiers and believers is the mission; they share characteristics of an authoritarian personality. The key motives of the players are thrill and fun. The watchdogs are motivated by drawing attention to social injustice. The last two groups share the characteristics of a libertarian personality. © 2012 Copyright Mass Communication \& Society Division of the Association for Education in Journalism and Mass Communication.},
	number = {6},
	journal = {Mass Communication and Society},
	author = {Erjavec, Karmen and Kovačič, Melita Poler},
	month = nov,
	year = {2012},
	pages = {899--920},
}

@article{silverstein_dialectics_2021,
	title = {The dialectics of indexical semiosis: {Scaling} up and out from the "actual" to the "virtual"},
	volume = {272},
	issn = {16133668},
	doi = {10.1515/ijsl-2021-2124},
	abstract = {Conventional indexicality is semiotically effective when regimented by its meta-indexical (or "metapragmatic") interpretant, a conceptual scheme presumed upon by participants in communication that determines the categories of possibility for a relevant "here-and-now"of indexically signaled co-presence, just as, conversely, such an interpretant is an emergent consequence of the sign's pointing to its object. In the more general case of non-denotational indexicality - forms indicating everything from perduring demographic characteristics of participants in interaction to their role incumbencies, voicings of identity, and momentary relational attitudes and affects (loosely termed "stances") - the culture - and thus group-specific metapragmatics (or "ethno-metapragmatics") is central to how indexicals entail the mutual (il)legibility of interlocutors and the (in)coherence of interactional projects in which they are engaged, the "interactional text"of what is happening. This inherent metapragmatic functionality of models of indexical signs and their contexts is, in general, itself influenced by genres of metapragmatic discourse about social life that "circulate"among networks of people who participate in certain sites of sociality. Such "circulation"is a virtual reality that comes into being via chains of interdiscursivity, allowing us to imagine an "ideological"plane with its own order of virtual semiotic dialectic that, notwithstanding, we experience in actual interactional context by its effects on the ever-changing what and how of indexicality.},
	number = {1},
	journal = {International Journal of the Sociology of Language},
	author = {Silverstein, Michael},
	month = nov,
	year = {2021},
	note = {Publisher: De Gruyter Mouton},
	keywords = {circulation, deixis, indexicality, language ideology, metapragmatics, metricalization, register, reported speech},
	pages = {13--45},
}

@article{josey_hate_2010,
	title = {Hate speech and identity: {An} analysis of neo racism and the indexing of identity},
	volume = {21},
	issn = {09579265},
	doi = {10.1177/0957926509345071},
	abstract = {In the academy and society at large, there remains an area of discourse largely deemed too marginal to analyze at any length: openly racist speech. It remains unexamined, in part, because much attention has been given to 'covert' racism. Recently, technology has allowed openly racist groups to shift strategies for creating and maintaining their own identity. Conventional wisdom would assume that these groups use referential or direct means of indexing identity. Using theories of discourse, this analysis demonstrates that even the most 'traditional' racists employ a complex pattern of voicing to indirectly index a 'neo-traditional' racist identity. These findings illustrate that within these communities, there is not only a sense of whiteness, but also a set of practices delineating 'good' and 'bad' white identity. Implications of these findings are discussed in the light of political and identity practices. © The Author(s) 2010.},
	number = {1},
	journal = {Discourse and Society},
	author = {Josey, Christopher S.},
	month = jan,
	year = {2010},
	keywords = {Hate speech, Identity, Indexing, Race, Racism, Register, Speech, Voicing, Whiteness},
	pages = {27--39},
}

@article{buyse_words_2014,
	title = {Words of {Violence}: "{Fear} {Speech}," or {How} {Violent} {Conflict} {Escalation} {Relates} to the {Freedom} of {Expression}},
	url = {https://heinonline.org/HOL/License},
	abstract = {The limits of the freedom of expression are a perennial discussion in human rights discourse. This article focuses on identifying yardsticks to establish the boundaries of freedom of expression in cases where violence is a risk. It does so by using insights from the social sciences on the escalation of violent conflict. By emphasizing the interaction between violence and discourse, and its effect on antagonisms between groups, it offers an interdisciplinary perspective on an ongoing legal debate. It introduces the notion of "fear speech" and argues that it may be much more salient in this context than hate speech.},
	author = {Buyse, Antoine},
	year = {2014},
}

@techreport{johnson_bag--words_2022,
	title = {Bag-of-{Words} {Algorithms} {Can} {Supplement} {Transformer} {Sequence} {Classification} \& {Improve} {Model} {Interpretability}},
	url = {www.rand.org},
	author = {Johnson, Christian and Marcellino, William},
	year = {2022},
}

@article{gelber_evidencing_2016,
	title = {Evidencing the harms of hate speech},
	url = {https://ro.uow.edu.au/lhapapers/2280/},
	urldate = {2022-01-19},
	author = {Gelber, Katharine and Mcnamara, Luke J},
	year = {2016},
}

@techreport{mittos_and_2020,
	title = {``{And} {We} {Will} {Fight} for {Our} {Race}!'' {A} {Measurement} {Study} of {Genetic} {Testing} {Conversations} on {Reddit} and 4chan},
	url = {www.aaai.org},
	abstract = {Progress in genomics has enabled the emergence of a booming market for "direct-to-consumer" genetic testing. Nowadays , companies like 23andMe and AncestryDNA provide affordable health, genealogy, and ancestry reports, and have already tested tens of millions of customers. At the same time, alt-and far-right groups have also taken an interest in genetic testing, using them to attack minorities and prove their genetic "purity." In this paper, we present a measurement study shedding light on how genetic testing is being discussed on Web communities in Reddit and 4chan. We collect 1.3M comments posted over 27 months on the two platforms, using a set of 280 keywords related to genetic testing. We then use NLP and computer vision tools to identify trends, themes, and topics of discussion. Our analysis shows that genetic testing attracts a lot of attention on Reddit and 4chan, with discussions often including highly toxic language expressed through hateful, racist, and misogynistic comments. In particular , on 4chan's politically incorrect board (/pol/), content from genetic testing conversations involves several alt-right personalities and openly antisemitic rhetoric, often conveyed through memes. Finally, we find that discussions build around user groups, from technology enthusiasts to communities promoting fringe political views.},
	author = {Mittos, Alexandros and Zannettou, Savvas and Blackburn, Jeremy and De Cristofaro, Emiliano},
	year = {2020},
	note = {Volume: 2020},
	keywords = {ICWSM Full Papers},
}

@inproceedings{hine_kek_2017,
	title = {Kek, {Cucks}, and {God} {Emperor} {Trump}: {A} {Measurement} {Study} of 4chan's {Politically} {Incorrect} {Forum} and {Its} {Effects} on the {Web}},
	isbn = {89,7361,418,5},
	url = {http://www.internetlivestats.com/internet-users/},
	abstract = {The discussion-board site 4chan has been part of the Internet's dark underbelly since its inception, and recent political events have put it increasingly in the spotlight. In particular, /pol/, the "Politically Incorrect" board, has been a central figure in the outlandish 2016 US election season, as it has often been linked to the alt-right movement and its rhetoric of hate and racism. However, 4chan remains relatively unstudied by the scientific community: little is known about its user base, the content it generates, and how it affects other parts of the Web. In this paper, we start addressing this gap by analyzing /pol/ along several axes, using a dataset of over 8M posts we collected over two and a half months. First, we perform a general characterization , showing that /pol/ users are well distributed around the world and that 4chan's unique features encourage fresh discussions. We also analyze content, finding, for instance, that YouTube links and hate speech are predominant on /pol/. Overall, our analysis not only provides the first measurement study of /pol/, but also insight into online harassment and hate speech trends in social media.},
	author = {Hine, Gabriel Emile and Onaolapo, Jeremiah and De Cristofaro, Emiliano and Kourtellis, Nicolas and Leontiadis, Ilias and Samaras, Riginos and Stringhini, Gianluca and Blackburn, Jeremy},
	year = {2017},
	keywords = {Full Papers},
}

@techreport{calvert_hate_1997,
	title = {Hate {Speech} and {Its} {Harms}: {A} {Communication} {Theory} {Perspective}},
	abstract = {In this article I analyze harms caused by hate speech using Carey's (1989) contrasting transmission and ritual models of communication. Adoption of the transmission model directs the attention of courts and legislative bodies to effects of hate speech, such as emotional and behavioral changes in the recipients of the speech. In contrast, the ritual model illustrates the reinforcement of racist attitudes and disparate treatment of minorities that occurs with the repetitive use of hate speech. Although the ritual model serves a heuristic function by providing courts with a framework for understanding harm caused by hate speech, its judicial adoption to determine liability for individual instances of racist or sexist communication raises troublesome First Amendment issues.},
	author = {Calvert, Clay},
	year = {1997},
	note = {Publication Title: Journal of Communication},
	pages = {4},
}

@article{lillian_thorn_2007,
	title = {A thorn by any other name: {Sexist} discourse as hate speech},
	volume = {18},
	issn = {09579265},
	doi = {10.1177/0957926507082193},
	abstract = {Scholarship on hate speech usually addresses racist and ethnicist discourses, and less often homophobic discourses. This article opens a conversation about sexist discourse as hate speech. In arguing that sexist discourse should be considered hate speech, I review several definitions of hate speech, one of which I use in analyzing the texts of neoconservative author William D. Gairdner. I argue that, although Gairdner's sexist discourse does not meet the legal definitions of hate speech, it is consistent with linguistic criteria for hate speech and that, since Gairdner's discourse is representative of mainstream sexist discourse, all such sexist discourse counts as hate speech. I conclude by asking why, amid all the published works on hate speech, the question of sexist discourse as hate speech is rarely even addressed. Since society still operates as if 'male' and 'female' were simple, self-evident categories, we, as feminists, must still respond to and challenge the sexist discourses that perpetuate and reproduce such dichotomies. One way to do that is to recognize sexist discourse as a form of hate speech and to challenge it on that basis. Copyright © 2007 SAGE Publications.},
	number = {6},
	journal = {Discourse and Society},
	author = {Lillian, Donna L.},
	month = nov,
	year = {2007},
	keywords = {Canada, Critical discourse analysis, Feminism, Hate speech, Sexism},
	pages = {719--740},
}

@book{assimakopoulos_springer_2017,
	title = {{SPRINGER} {BRIEFS} {IN} {LINGUISTICS} {Online} {Hate} {Speech} in the {European} {Union} {A} {Discourse}-{Analytic} {Perspective}},
	url = {http://www.springer.com/series/11940},
	author = {Assimakopoulos, Stavros and Baider, Fabienne H and Millar, Sharon},
	year = {2017},
}

@article{burch_you_2018,
	title = {‘{You} are a parasite on the productive classes’: online disablist hate speech in austere times},
	volume = {33},
	issn = {13600508},
	doi = {10.1080/09687599.2017.1411250},
	abstract = {In response to a climate of austerity, disability has been keyed to a meta-narrative that has a political purpose, to justify extensive welfare cuts by positioning disability as a drain on so-called ‘hard-working taxpayers’. This paper explores how this meta-narrative is articulated on the online bulletin board, Reddit, to show how disablist hate speech may emerge as an attempt to secure one’s sense of self during austere times. A critical discourse analysis (CDA) is employed to reveal these instances and highlights the normalisation of disablist hate speech within the wider context of welfare dependency. Based on these findings, this paper makes recommendations for future research and makes a call for policy change.},
	number = {3},
	journal = {Disability and Society},
	author = {Burch, Leah},
	month = mar,
	year = {2018},
	note = {Publisher: Routledge},
	keywords = {Online hate speech, austerity, disablism, welfare dependency},
	pages = {392--415},
}

@inproceedings{liu_fuzzy_2019,
	title = {Fuzzy multi-task learning for hate speech type identification},
	isbn = {978-1-4503-6674-8},
	doi = {10.1145/3308558.3313546},
	abstract = {In traditional machine learning, classifiers training is typically undertaken in the setting of single-task learning, so the trained classifier can discriminate between different classes. However, this must be based on the assumption that different classes are mutually exclusive. In real applications, the above assumption does not always hold. For example, the same book may belong to multiple subjects. From this point of view, researchers were motivated to formulate multi-label learning problems. In this context, each instance can be assigned multiple labels but the classifiers training is still typically undertaken in the setting of single-task learning. When probabilistic approaches are adopted for classifiers training, multi-task learning can be enabled through transformation of a multi-labelled data set into several binary data sets. The above data transformation could usually result in the class imbalance issue. Without the above data transformation, multi-labelling of data results in an exponential increase of the number of classes, leading to fewer instances for each class and a higher difficulty for identifying each class. In addition, multi-labelling of data is very time consuming and expensive in some application areas, such as hate speech detection. In this paper, we introduce a novel formulation of the hate speech type identification problem in the setting of multi-task learning through our proposed fuzzy ensemble approach. In this setting, single-labelled data can be used for semi-supervised multi-label learning and two new metrics (detection rate and irrelevance rate) are thus proposed to measure more effectively the performance for this kind of learning tasks. We report an experimental study on identification of four types of hate speech, namely: religion, race, disability and sexual orientation. The experimental results show that our proposed fuzzy ensemble approach outperforms other popular probabilistic approaches, with an overall detection rate of 0.93.},
	booktitle = {The {Web} {Conference} 2019 - {Proceedings} of the {World} {Wide} {Web} {Conference}, {WWW} 2019},
	publisher = {Association for Computing Machinery, Inc},
	author = {Liu, Han and Alorainy, Wafa and Burnap, Pete and Williams, Matthew L.},
	month = may,
	year = {2019},
	keywords = {Cyberhate detection, Fuzzy classification, Machine learning, Multi-task learning, Text classification},
	pages = {3006--3012},
}

@article{zhang_hate_2018,
	title = {Hate {Speech} {Detection}: {A} {Solved} {Problem}? {The} {Challenging} {Case} of {Long} {Tail} on {Twitter}},
	url = {http://arxiv.org/abs/1803.03662},
	abstract = {In recent years, the increasing propagation of hate speech on social media and the urgent need for effective counter-measures have drawn significant investment from governments, companies, and researchers. A large number of methods have been developed for automated hate speech detection online. This aims to classify textual content into non-hate or hate speech, in which case the method may also identify the targeting characteristics (i.e., types of hate, such as race, and religion) in the hate speech. However, we notice significant difference between the performance of the two (i.e., non-hate v.s. hate). In this work, we argue for a focus on the latter problem for practical reasons. We show that it is a much more challenging task, as our analysis of the language in the typical datasets shows that hate speech lacks unique, discriminative features and therefore is found in the 'long tail' in a dataset that is difficult to discover. We then propose Deep Neural Network structures serving as feature extractors that are particularly effective for capturing the semantics of hate speech. Our methods are evaluated on the largest collection of hate speech datasets based on Twitter, and are shown to be able to outperform the best performing method by up to 5 percentage points in macro-average F1, or 8 percentage points in the more challenging case of identifying hateful content.},
	author = {Zhang, Ziqi and Luo, Lei},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.03662},
}

@techreport{vigna_hate_nodate,
	title = {Hate me, hate me not: {Hate} speech detection on {Facebook}},
	url = {https://curl.haxx.se},
	abstract = {While favouring communications and easing information sharing, Social Network Sites are also used to launch harmful campaigns against specific groups and individuals. Cyberbullism, incitement to self-harm practices, sexual predation are just some of the severe effects of massive online offensives. Moreover, attacks can be carried out against groups of victims and can degenerate in physical violence. In this work, we aim at containing and preventing the alarming diffusion of such hate campaigns. Using Facebook as a benchmark, we consider the textual content of comments appeared on a set of public Italian pages. We first propose a variety of hate categories to distinguish the kind of hate. Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy. Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, we design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM). We test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition. The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.},
	author = {Vigna, Fabio Del and Cimino, Andrea and Dell'orletta, Felice and Petrocchi, Marinella and Tesconi, Maurizio},
}

@article{knechtle_when_nodate,
	title = {When to {Regulate} {Hate} {Speech}},
	url = {https://heinonline.org/HOL/License},
	author = {Knechtle, John C},
}

@techreport{altman_ora_2020,
	title = {{ORA} {User}'s {Guide} 2020},
	url = {www.netanomics.com.},
	author = {Altman, Neal and Carley, Kathleen M and Reminga, Jeffrey},
	year = {2020},
}

@techreport{carley_studying_nodate,
	title = {Studying {Social} {Networks}},
	author = {Carley, Kathleen M},
}

@techreport{carley_ora_nodate,
	title = {{ORA}: {A} {Toolkit} for {Dynamic} {Network} {Analysis} and {Visualization}},
	author = {Carley, Kathleen M},
}

@techreport{blane_social-cyber_nodate,
	title = {Social-{Cyber} {Maneuvers} {Analysis} {During} the {COVID}-19 {Vaccine} {Initial} {Rollout}},
	abstract = {Before and after the release of Pfizer's COVID-19 vaccine, the first COVID-19 vaccine to be approved and distributed in the western world, many users took to social media to discuss getting vaccinated or to persuade others not to vaccinate. The methods of persuasion or manipulation users employ on social media can be characterized under the BEND maneuver framework. In this study, we examine Twitter data from the time periods before, during, and after the rollout of the Pfizer vaccine and separate users into pro-vaccine and anti-vaccine communities. We then conduct a network analysis for these time periods and communities to find the important members and see how the different groups used BEND maneuvers to influence their target audiences and the network as a whole. Our analysis shows how each community attempts to build their own communities while simultaneously narrowing the opposing community. Pro-vaccine groups used excite and explain messages to encourage vaccination, while anti-vaccine groups relied on dismaying messages about side effects and death. Furthermore, nuking through platform policies showed to be effective in reducing the size of the anti-vaccine online community and the quantity of anti-vaccine messages.},
	author = {Blane, Janice T},
}

@article{tsantarliotis_defining_2017,
	title = {Defining and predicting troll vulnerability in online social media},
	volume = {7},
	issn = {18695469},
	doi = {10.1007/s13278-017-0445-2},
	abstract = {Trolling describes a range of antisocial online behaviors that aim at disrupting the normal operation of online social networks and media. Existing approaches to combating trolling rely on human-based or automatic mechanisms for identifying trolls and troll posts. In this paper, we take a novel approach to the problem: our goal is to identify troll vulnerable posts, that is, posts that are potential targets of trolls, so as to prevent trolling before it happens. To this end, we define three natural axioms that a troll vulnerability metric must satisfy and introduce metrics that satisfy them. We then define the troll vulnerability prediction problem, where given a post we aim at predicting whether it is vulnerable to trolling. We construct models that use features from the content and the history of the post for the prediction. Our experiments with real data from Reddit demonstrate that our approach is successful in identifying a large fraction of the troll vulnerable posts.},
	number = {1},
	journal = {Social Network Analysis and Mining},
	author = {Tsantarliotis, Paraskevas and Pitoura, Evaggelia and Tsaparas, Panayiotis},
	month = dec,
	year = {2017},
	note = {Publisher: Springer-Verlag Wien},
}

@article{brown_stylistic_2021,
	title = {Stylistic variation in email},
	issn = {2542-9477},
	url = {http://www.jbe-platform.com/content/journals/10.1075/rs.20023.bro},
	doi = {10.1075/rs.20023.bro},
	abstract = {{\textless}p{\textgreater}This study explores how email is partly shaped by writers’ positions within a corporate structure. This stylistic variation is measurable at scale and can be described by messages’ rhetorical organizations and orientations. The modeling was carried out on a subset of the Enron email corpus, which was processed using the dictionary-based tagger DocuScope. The results identify four stylistic variants (Trained/Technical Support, Decision-Making, Everyday Workplace Interaction, and Engaged Planning), each realizing distinctive combinations of features reflective of their communicative functions. In Trained/Technical Support emails, for example, constellations of words and phrases associated with informational production and facilitation are marshaled in fulfilling routine guidance-seeking and guidance-giving tasks. While writers’ positions motivate stylistic tendencies (e.g., members of upper-level management compose a majority of their messages in the Decision-Making style), all writers avail themselves of a variety of styles, depending on audience and purpose, suggesting that learners might benefit from developing adaptable communicative repertoires.{\textless}/p{\textgreater}},
	journal = {Register Studies},
	author = {Brown, David West and Laudenbach, Michael},
	month = dec,
	year = {2021},
}

@inproceedings{raisi_cyberbullying_2017,
	title = {Cyberbullying detection with weakly supervised machine learning},
	isbn = {978-1-4503-4993-2},
	doi = {10.1145/3110025.3110049},
	abstract = {Detrimental online behavior such as harassment and cyberbullying is becoming a serious, large-scale problem damaging people’s lives. This phenomenon is creating a need for automated, data-driven techniques for analyzing and detecting such behaviors. We propose a machine learning method for simultaneously inferring user roles in harassment-based bullying and new vocabulary indicators of bullying. The learning algorithm considers social structure and infers which users tend to bully and which tend to be victimized. To address the elusive nature of cyberbullying, the learning algorithm only requires weak supervision. Experts provide a small seed vocabulary of bullying indicators, and the algorithm uses a large, unlabeled corpus of social media interactions to extract bullying roles of users and additional vocabulary indicators of bullying. The model estimates whether each social interaction is bullying based on who participates and based on what language is used, and it tries to maximize the agreement between these estimates, i.e., participant-vocabulary consistency (PVC). We evaluate PVC on three social media data sets, demonstrating quantitatively and qualitatively its effectiveness in cyberbullying detection.},
	booktitle = {Proceedings of the 2017 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}, {ASONAM} 2017},
	publisher = {Association for Computing Machinery, Inc},
	author = {Raisi, Elaheh and Huang, Bert},
	month = jul,
	year = {2017},
	pages = {409--416},
}

@inproceedings{zhang_detecting_2018,
	title = {Detecting {Hate} {Speech} on {Twitter} {Using} a {Convolution}-{GRU} {Based} {Deep} {Neural} {Network}},
	volume = {10843 LNCS},
	isbn = {978-3-319-93416-7},
	doi = {10.1007/978-3-319-93417-4_48},
	abstract = {In recent years, the increasing propagation of hate speech on social media and the urgent need for effective counter-measures have drawn significant investment from governments, companies, and empirical research. Despite a large number of emerging scientific studies to address the problem, a major limitation of existing work is the lack of comparative evaluations, which makes it difficult to assess the contribution of individual works. This paper introduces a new method based on a deep neural network combining convolutional and gated recurrent networks. We conduct an extensive evaluation of the method against several baselines and state of the art on the largest collection of publicly available Twitter datasets to date, and show that compared to previously reported results on these datasets, our proposed method is able to capture both word sequence and order information in short texts, and it sets new benchmark by outperforming on 6 out of 7 datasets by between 1 and 13\% in F1. We also extend the existing dataset collection on this task by creating a new dataset covering different topics.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	publisher = {Springer Verlag},
	author = {Zhang, Ziqi and Robinson, David and Tepper, Jonathan},
	year = {2018},
	note = {ISSN: 16113349},
	pages = {745--760},
}

@article{alorainy_enemy_2019,
	title = {“{The} {Enemy} {Among} {Us}”},
	volume = {13},
	issn = {1559-1131},
	doi = {10.1145/3324997},
	abstract = {Offensive or antagonistic language targeted at individuals and social groups based on their personal characteristics (also known as cyber hate speech or cyberhate) has been frequently posted and widely circulated via the World Wide Web. This can be considered as a key risk factor for individual and societal tension surrounding regional instability. Automated Web-based cyberhate detection is important for observing and understanding community and regional societal tension—especially in online social networks where posts can be rapidly and widely viewed and disseminated. While previous work has involved using lexicons, bags-of-words, or probabilistic language parsing approaches, they often suffer from a similar issue, which is that cyberhate can be subtle and indirect—thus, depending on the occurrence of individual words or phrases, can lead to a significant number of false negatives, providing inaccurate representation of the trends in cyberhate. This problem motivated us to challenge thinking around the representation of subtle language use, such as references to perceived threats from “the other” including immigration or job prosperity in a hateful context. We propose a novel “othering” feature set that utilizes language use around the concept of “othering” and intergroup threat theory to identify these subtleties, and we implement a wide range of classification methods using embedding learning to compute semantic distances between parts of speech considered to be part of an “othering” narrative. To validate our approach, we conducted two sets of experiments. The first involved comparing the results of our novel method with state-of-the-art baseline models from the literature. Our approach outperformed all existing methods. The second tested the best performing models from the first phase on unseen datasets for different types of cyberhate, namely religion, disability, race, and sexual orientation. The results showed F-measure scores for classifying hateful instances obtained through applying our model of 0.81, 0.71, 0.89, and 0.72, respectively, demonstrating the ability of the “othering” narrative to be an important part of model generalization.},
	number = {3},
	journal = {ACM Transactions on the Web},
	author = {Alorainy, Wafa and Burnap, Pete and Liu, Han and Williams, Matthew L.},
	month = nov,
	year = {2019},
	note = {Publisher: Association for Computing Machinery (ACM)},
	pages = {1--26},
}

@article{uyheng_bots_2020,
	title = {Bots and online hate during the {COVID}-19 pandemic: case studies in the {United} {States} and the {Philippines}},
	volume = {3},
	issn = {2432-2717},
	doi = {10.1007/s42001-020-00087-4},
	abstract = {Online hate speech represents a serious problem exacerbated by the ongoing COVID-19 pandemic. Although often anchored in real-world social divisions, hate speech in cyberspace may also be fueled inorganically by inauthentic actors like social bots. This work presents and employs a methodological pipeline for assessing the links between hate speech and bot-driven activity through the lens of social cybersecurity. Using a combination of machine learning and network science tools, we empirically characterize Twitter conversations about the pandemic in the United States and the Philippines. Our integrated analysis reveals idiosyncratic relationships between bots and hate speech across datasets, highlighting different network dynamics of racially charged toxicity in the US and political conflicts in the Philippines. Most crucially, we discover that bot activity is linked to higher hate in both countries, especially in communities which are denser and more isolated from others. We discuss several insights for probing issues of online hate speech and coordinated disinformation, especially through a global approach to computational social science.},
	number = {2},
	journal = {Journal of Computational Social Science},
	author = {Uyheng, Joshua and Carley, Kathleen M.},
	month = nov,
	year = {2020},
	pmid = {33102925},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {445--468},
}

@inproceedings{vidgen_challenges_2019,
	address = {Florence, Italy},
	title = {Challenges and frontiers in abusive content detection},
	url = {https://aclanthology.org/W19-3509},
	doi = {10.18653/v1/W19-3509},
	abstract = {Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.},
	booktitle = {Proceedings of the {Third} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Vidgen, Bertie and Harris, Alex and Nguyen, Dong and Tromble, Rebekah and Hale, Scott and Margetts, Helen},
	month = aug,
	year = {2019},
	pages = {80--93},
}

@inproceedings{qian_lifelong_2021,
	title = {Lifelong {Learning} of {Hate} {Speech} {Classification} on {Social} {Media}},
	url = {https://l1ght.com/Toxicity_during_coronavirus_Report-},
	abstract = {Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting , we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining vari-ational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.},
	author = {Qian, Jing and Wang, Hong and Elsherief, Mai and Yan, Xifeng},
	year = {2021},
	pages = {2304--2314},
}

@techreport{schieb_governing_nodate,
	title = {{GOVERNING} {HATE} {SPEECH} {BY} {MEANS} {OF} {COUNTER} {SPEECH} 1 {Governing} {Hate} {Speech} by {Means} of {Counter} {Speech} on {Facebook}},
	url = {http://t3n.de/news/facebook-hatespeech-fehler-gemacht-637860/},
	abstract = {Counter speech is currently advocated by social networks as a measure for delimiting the effects of hate speech. Conveniently, counter speech is left to the dedicated user so that internet companies do not have to come up with new technologies or invest into manual treatment. But how efficient is counter speech? Our approach is twofold: Firstly, we review existing literature in order to find examples where counter speech worked well. Secondly, due to the missing availability of data, we set up a computational simulation model that is used to answer general questions concerning the effects that hinder or support the impact of counter speech. On the basis of our findings, we argue that the defining factors for the success of counter speech are the proportion of the hate speech faction and the type of influence the counter speakers can exert on the undecided.},
	author = {Schieb, Carla and Preuss, Mike},
}

@techreport{chung_conan-counter_nodate,
	title = {{CONAN}-{COunter} {NArratives} through {Nichesourcing}: a {Multilingual} {Dataset} of {Responses} to {Fight} {Online} {Hate} {Speech}},
	url = {https://github.com/},
	abstract = {Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed tex-tual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.},
	institution = {Association for Computational Linguistics},
	author = {Chung, Yi-Ling and Kuzmenko, Elizaveta and Sinem Tekiro˘ Glu, Serra and Guerini, Marco},
	pages = {2819--2829},
}

@inproceedings{pavlopoulos_deep_2017,
	title = {Deep {Learning} for {User} {Comment} {Moderation}},
	url = {http://www.perspectiveapi.com/},
	abstract = {Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outper-forms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	author = {Pavlopoulos, John and Malakasiotis, Prodromos and Androutsopoulos, Ion},
	year = {2017},
	pages = {25--35},
}

@techreport{ljubesic_datasets_2018,
	title = {Datasets of {Slovene} and {Croatian} {Moderated} {News} {Comments}},
	url = {https://figshare.com/projects/},
	abstract = {This paper presents two large newly constructed datasets of moderated news comments from two highly popular online news portals in the respective countries: the Slovene RTV MCC and the Croatian 24sata. The datasets are analyzed by performing manual annotation of the types of the content which have been deleted by moderators and by investigating deletion trends among users and threads. Next, initial experiments on automatically detecting the deleted content in the datasets are presented. Both datasets are published in encrypted form, to enable others to perform experiments on detecting content to be deleted without revealing potentially inappropriate content. Finally, the baseline classification models trained on the non-encrypted datasets are disseminated as well to enable real-world use.},
	author = {Ljubeši´c, Nikola Ljubeši´ and Erjavec, Tomaž and Fišer, Darja},
	year = {2018},
	pages = {124--131},
}

@techreport{francesconi_error_2019,
	title = {Error {Analysis} in a {Hate} {Speech} {Detection} {Task}: the {Case} of {HaSpeeDe}-{TW} at {EVALITA} 2018},
	url = {https://coling2018.org/},
	abstract = {Taking as a case study the Hate Speech Detection task at EVALITA 2018, the paper discusses the distribution and typol-ogy of the errors made by the five best-scoring systems. The focus is on the sub-task where Twitter data was used both for training and testing (HaSpeeDe-TW). In order to highlight the complexity of hate speech and the reasons beyond the failures in its automatic detection, the annotation provided for the task is enriched with orthogonal categories annotated in the original reference corpus, such as aggressiveness , offensiveness, irony and the presence of stereotypes.},
	author = {Francesconi, Chiara and Bosco, Cristina and Poletto, Fabio and Sanguinetti, Manuela},
	year = {2019},
}

@inproceedings{wulczyn_ex_2017,
	title = {Ex machina: {Personal} attacks seen at scale},
	isbn = {978-1-4503-4913-0},
	doi = {10.1145/3038912.3052591},
	abstract = {The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.},
	booktitle = {26th {International} {World} {Wide} {Web} {Conference}, {WWW} 2017},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
	year = {2017},
	note = {arXiv: 1610.08914},
	pages = {1391--1399},
}

@inproceedings{djuric_hate_2015,
	title = {Hate speech detection with comment embeddings},
	isbn = {978-1-4503-3473-0},
	doi = {10.1145/2740908.2742760},
	abstract = {We address the problem of hate speech detection in online user comments. Hate speech, defined as an abusive speech targeting specific group characteristics, such as ethnicity, religion, or gender, is an important problem plaguing websites that allow users to leave feedback, having a negative impact on their online business and overall user experience. We propose to learn distributed low-dimensional representations of comments using recently proposed neural language models, that can then be fed as inputs to a classification algorithm. Our approach addresses issues of high-dimensionality and sparsity that impact the current state-of-the-art, resulting in highly efficient and effiective hate speech detectors.},
	booktitle = {{WWW} 2015 {Companion} - {Proceedings} of the 24th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Djuric, Nemanja and Zhou, Jing and Morris, Robin and Grbovic, Mihajlo and Radosavljevic, Vladan and Bhamidipati, Narayan},
	month = may,
	year = {2015},
	pages = {29--30},
}

@inproceedings{gao_detecting_2017,
	title = {Detecting {Online} {Hate} {Speech} {Using} {Context} {Aware} {Models}},
	isbn = {978-954-452-049-6},
	doi = {10.26615/978-954-452-049-6_036},
	booktitle = {{RANLP} 2017 - {Recent} {Advances} in {Natural} {Language} {Processing} {Meet} {Deep} {Learning}},
	publisher = {Incoma Ltd. Shoumen, Bulgaria},
	author = {Gao, Lei and Huang, Ruihong},
	month = nov,
	year = {2017},
	pages = {260--266},
}

@article{schafer_i_nodate,
	title = {I {N} {T} {R} {O} {D} {U} {C} {T} {I} {O} {N} {T} {O} {W} {A} {R} {D} {S} {A} {N} {N} {O} {TAT} {I} {N} {G} {I} {L} {L} {E} {G} {A} {L} {H} {AT} {E} {S} {P} {E} {E} {C} {H} : {A} {C} {O} {M} {P} {U} {TAT} {I} {O} {N} {A} {L} {L} {I} {N} {G} {U} {I} {S} {T} {I} {C} {A} {P} {P} {R} {O} {A} {C} {H}},
	issn = {2736-6391},
	url = {https://www.bmjv.de/Shared-},
	abstract = {In this report we describe our method of creating annotation guidelines for potentially illegal hate speech and the empirical analysis of Twitter data. While no unified definition of hate speech exists, scientific research has to follow clear specifications when actual data instances are to be categorized. The usual approach is to describe an own definition which is often based on linguistic cues and substantiated by examples. On the other hand, legal prosecution of severe cases follows different paths to make decisions with the entire contextual environment taken into account. Therefore we can identify a gap of hate speech research directions to 1 real consequences of identified cases. Nevertheless, we can point out an article by the German Federal Agency for Civic Education where the phenomenon is also described by short text examples. In our research we aim to find a compromise approach and developed our understanding of hate speech on the basis of laws and court rulings applied to example instances. Furthermore, in this report we outline our development steps and discuss several issues of the process. Finally, we provide our guidelines and introduce a small gold standard data set.},
	author = {Schäfer, Johannes and Boguslu, Kübra},
}

@inproceedings{schafer_offence_2019,
	title = {Offence in dialogues: {A} corpus-based study},
	volume = {2019-September},
	isbn = {978-954-452-055-7},
	doi = {10.26615/978-954-452-056-4_125},
	abstract = {In recent years an increasing number of analyses of offensive language has been published, however, dealing mainly with the automatic detection and classification of isolated instances. In this paper we aim to understand the impact of offensive messages in online conversations diachronically, and in particular the change in offensiveness of dialogue turns. In turn, we aim to measure the progression of offence level as well as its direction - For example, whether a conversation is escalating or declining in offence. We present our method of extracting linear dialogues from tree-structured conversations in social media data and make our code publicly available.1 Furthermore, we discuss methods to analyse this dataset through changes in discourse offensiveness. Our paper includes two main contributions; first, using a neural network to measure the level of offensiveness in conversations; and second, the analysis of conversations around offensive comments using decoupling functions.},
	booktitle = {International {Conference} {Recent} {Advances} in {Natural} {Language} {Processing}, {RANLP}},
	publisher = {Incoma Ltd},
	author = {Schäfer, Johannes and Burtenshaw, Ben},
	year = {2019},
	note = {ISSN: 13138502},
	pages = {1085--1093},
}

@techreport{olteanu_effect_2018,
	title = {The {Effect} of {Extremist} {Violence} on {Hateful} {Speech} {Online}},
	url = {http://bbc.com/news/technology-41442958},
	abstract = {User-generated content online is shaped by many factors, including endogenous elements such as platform affordances and norms, as well as exogenous elements, in particular significant events. These impact what users say, how they say it, and when they say it. In this paper, we focus on quantifying the impact of violent events on various types of hate speech, from offensive and derogatory to intimidation and explicit calls for violence. We anchor this study in a series of attacks involving Arabs and Muslims as perpetrators or victims , occurring in Western countries, that have been covered extensively by news media. These attacks have fueled intense policy debates around immigration in various fora, including online media, which have been marred by racist prejudice and hateful speech. The focus of our research is to model the effect of the attacks on the volume and type of hateful speech on two social media platforms, Twitter and Reddit. Among other findings, we observe that extremist violence tends to lead to an increase in online hate speech, particularly on messages directly advocating violence. Our research has implications for the way in which hate speech online is monitored and suggests ways in which it could be fought.},
	author = {Olteanu, Alexandra and Castillo, Carlos and Boy, Jeremy and Varshney, Kush R},
	year = {2018},
	keywords = {Full Papers},
}

@techreport{fiser_legal_nodate,
	title = {Legal {Framework}, {Dataset} and {Annotation} {Schema} for {Socially} {Unacceptable} {Online} {Discourse} {Practices} in {Slovene}},
	url = {http://www.spletno-oko.si/},
	abstract = {In this paper we present the legal framework , dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slove-nia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multi-cultural information society.},
	author = {Fišer, Darja and Ljubeši´c, Nikola Ljubeši´ and Erjavec, Tomaž},
	pages = {46--51},
}

@book{institute_of_electrical_and_electronics_engineers_2018_nodate,
	title = {2018 {Fifth} {International} {Conference} on {Social} {Networks} {Analysis}, {Management} and {Security} ({SNAMS}) : 15-18 {October}, 2018 {Valencia}, {Spain}.},
	isbn = {978-1-5386-9588-3},
	abstract = {Organizers and sponsors: Staffordshire University, Jordan University of Science and Technology, IEEE Sección España, Universidad Politécnica de Valencia. "IEEE Catalog Number: CFP18R39-ART".},
	author = {{Institute of Electrical and Electronics Engineers} and {Institute of Electrical and Electronics Engineers. Spanish Section} and {Staffordshire University} and {Jordan University of Science \& Technology} and {Universidad Politécnica de Valencia}},
}

@article{bender_data_nodate,
	title = {Data {Statements} for {Natural} {Language} {Processing}: {Toward} {Mitigating} {System} {Bias} and {Enabling} {Better} {Science}},
	doi = {10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf},
	abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists , in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results , protect companies from public embarrassment , and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
	author = {Bender, Emily M and Friedman, Batya},
}

@article{chetty_hate_2018,
	title = {Hate speech review in the context of online social networks},
	volume = {40},
	issn = {18736335},
	doi = {10.1016/j.avb.2018.05.003},
	abstract = {Advances in Internet Technologies (ITs) and online social networks have made more benefits to humanity. At the same time, the dark side of this growth/benefit has led to increased hate speech and terrorism as most common and powerful threats globally. Hate speech is an offensive kind of communication mechanism that expresses an ideology of hate using stereotypes. Hate speech targets different protected characteristics such as gender, religion, race, and disability. Control of hate speech can be made using different national and international legal frameworks. Any intentional act directed against life or related entities causing a common danger is known as terrorism. There is a common practice of discussing or debating hate speech and terrorism separately. In the recent past, most of the research articles have discussed either hate speech or terrorism. Hate speech is a type of terrorism and follows an incident or trigger event of terrorism. Online social networks are the result of ITs and evolved rapidly through the popularity among youth. As both the activities are near to close and makes use of online social networks, the collective discussion is appropriate. Therefore we have a review on hate speech with different classes and terrorism with cyber use in the framework of online social networks. With the help of combined effort from the government, the Internet Service Providers (ISPs) and online social networks, the proper policies can be framed to counter both hate speech and terrorism efficiently and effectively.},
	journal = {Aggression and Violent Behavior},
	author = {Chetty, Naganna and Alathur, Sreejith},
	month = may,
	year = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Cyberterrorism, Extremism, Free speech, Hate crime, Hate speech, Online social networks, Terrorism, Twitter and Facebook},
	pages = {108--118},
}

@article{waqas_mapping_2019,
	title = {Mapping online hate: {A} scientometric analysis on research trends and hotspots in research on online hate},
	volume = {14},
	issn = {19326203},
	doi = {10.1371/journal.pone.0222194},
	abstract = {Internet and social media participation open doors to a plethora of positive opportunities for the general public. However, in addition to these positive aspects, digital technology also provides an effective medium for spreading hateful content in the form of cyberbullying, bigotry, hateful ideologies, and harassment of individuals and groups. This research aims to investigate the growing body of online hate research (OHR) by mapping general research indices, prevalent themes of research, research hotspots, and influential stakeholders such as organizations and contributing regions. For this, we use scientometric techniques and collect research papers from the Web of Science core database published through March 2019. We apply a predefined search strategy to retrieve peer-reviewed OHR and analyze the data using CiteSpace software by identifying influential papers, themes of research, and collaborating institutions. Our results show that higher-income countries contribute most to OHR, with Western countries accounting for most of the publications, funded by North American and European funding agencies. We also observed increased research activity post-2005, starting from more than 50 publications to more than 550 in 2018. This applies to a number of publications as well as citations. The hotbeds of OHR focus on cyberbullying, social media platforms, co-morbid mental disorders, and profiling of aggressors and victims. Moreover, we identified four main clusters of OHR: (1) Cyberbullying, (2) Sexual solicitation and intimate partner violence, (3) Deep learning and automation, and (4) Extremist and online hate groups, which highlight the cross-disciplinary and multifaceted nature of OHR as a field of research. The research has implications for researchers and policymakers engaged in OHR and its associated problems for individuals and society.},
	number = {9},
	journal = {PLoS ONE},
	author = {Waqas, Ahmed and Salminen, Joni and Jung, Soon gyo and Almerekhi, Hind and Jansen, Bernard J.},
	month = sep,
	year = {2019},
	pmid = {31557227},
	note = {Publisher: Public Library of Science},
}

@article{rahman_addressing_nodate,
	title = {Addressing {Content} {Selection} {Bias} in {Creating} {Datasets} for {Hate} {Speech} {Detection}},
	author = {Rahman, Md Mustafizur and Balakrishnan, Dinesh and Murthy, Dhiraj and Kutlu, Mucahid and Lease, Matthew},
}

@article{castelle_linguistic_2018,
	title = {The {Linguistic} {Ideologies} of {Deep} {Abusive} {Language} {Classification}},
	abstract = {This paper brings together theories from so-ciolinguistics and linguistic anthropology to critically evaluate the so-called "language ideologies"-the set of beliefs and ways of speaking about language-in the practices of abusive language classification in modern machine learning-based NLP. This argument is made at both a conceptual and empirical level, as we review approaches to abusive language from different fields, and use two neural network methods to analyze three datasets developed for abusive language classification tasks (drawn from Wikipedia, Facebook, and Stack-Overflow). By evaluating and comparing these results, we argue for the importance of incorporating theories of pragmatics and metaprag-matics into both the design of classification tasks as well as in ML architectures.},
	journal = {Proceedings of the Second Workshop on Abusive Language Online},
	author = {Castelle, Michael},
	year = {2018},
	keywords = {★},
	pages = {160--170},
}

@inproceedings{tatar_modeling_nodate,
	title = {Modeling {Unstructured} {Data}: {Teachers} as {Learners} and {Designers} of {Technology}-enhanced {Artificial} {Intelligence} {Curriculum}},
	copyright = {All rights reserved},
	abstract = {In this paper, we present a co-design study with teachers to contribute towards development of a technology-enhanced Artificial Intelligence (AI) curriculum, focusing on modeling unstructured data. We created an initial design of a learning activity prototype and explored ways to incorporate the design into high school classes. Specifically, teachers explored text classification models with the prototype and reflected on the exploration as a user, learner, and teacher. They provided insights about learning opportunities in the activity and feedback for integrating it into their teaching. Findings from qualitative analysis demonstrate that exploring text classification models provided an accessible and comprehensive approach for integrated learning of mathematics, language arts, and computing with the potential of supporting the understanding of core AI concepts including identifying structure within unstructured data and reasoning about the roles of human insight in developing AI technologies.},
	urldate = {2022-01-06},
	author = {Tatar, Cansu and Yoder, Michael Miller and Coven, Madeline and Jiang, Shiyan and Rosé, Carolyn P},
}

@article{lum_predict_2016,
	title = {To predict and serve?},
	volume = {13},
	issn = {17409713},
	doi = {10.1111/j.1740-9713.2016.00960.x},
	abstract = {Predictive policing systems are used increasingly by law enforcement to try to prevent crime before it occurs. But what happens when these systems are trained using biased data? Kristian Lum and William Isaac consider the evidence – and the social consequences.},
	number = {5},
	journal = {Significance},
	author = {Lum, Kristian and Isaac, William},
	year = {2016},
	pages = {14--19},
}

@book{stryker2008transgender,
	title = {Transgender {History}},
	isbn = {978-1-58005-224-5},
	url = {https://books.google.com/books?id=XcQ%5C_BAAAQBAJ},
	publisher = {Seal Press},
	author = {Stryker, S},
	year = {2008},
	note = {Series Title: Seal Studies},
}

@article{Stryker2004,
	title = {Transgender {Studies}: {Queer} {Theory}'s {Evil} {Twin}},
	volume = {10},
	issn = {1064-2684},
	doi = {10.1215/10642684-10-2-212},
	abstract = {If queer theory was born of the union of sexuality studies and feminism, transgen- der studies can be considered queer theory’s evil twin: it has the same parentage but willfully disrupts the privileged family narratives that favor sexual identity labels (like gay, lesbian, bisexual, and heterosexual) over the gender categories (like man and woman) that enable desire to take shape and find its aim.},
	number = {2},
	journal = {GLQ: A Journal of Lesbian and Gay Studies},
	author = {Stryker, S.},
	year = {2004},
	pages = {212--215},
}

@book{Barry2002,
	title = {Beginning {Theory}: {An} {Introduction} to {Literary} and {Cultural} {Theory}},
	publisher = {Manchester University Press},
	author = {Barry, Peter},
	year = {2002},
}

@misc{Atkin2013,
	title = {Peirce's {Theory} of {Signs}},
	url = {https://plato.stanford.edu/archives/sum2013/entries/peirce-semiotics},
	journal = {The Stanford Encyclopedia of Philosophy},
	author = {Atkin, Albert},
	year = {2013},
}

@incollection{Barrett2014,
	title = {The {Emergence} of the {Unmarked}},
	isbn = {978-0-19-968267-6},
	booktitle = {Queer {Excursions}: {Retheorizing} {Binaries} in {Language}, {Gender}, and {Sexuality}},
	author = {Barrett, Rusty},
	editor = {Zimman, Lal and Davis, Jenny and Raclaw, Joshua},
	year = {2014},
	doi = {10.1093/acprof},
	note = {ISSN: 18255167},
}

@book{austin1962things,
	title = {How to {Do} {Things} with {Words}},
	publisher = {Harvard University Press},
	author = {Austin, JL},
	year = {1962},
}

@article{bucholtz2005identity,
	title = {Identity and interaction: {A} sociocultural linguistic approach},
	volume = {7},
	url = {https://journals.sagepub.com/doi/abs/10.1177/1461445605054407},
	number = {4-5},
	journal = {Discourse Studies},
	author = {Bucholtz, Mary and Hall, Kira},
	year = {2005},
	note = {Publisher: SAGE Publications},
	keywords = {★},
	pages = {585--614},
}

@book{kroes2014moral,
	title = {The {Moral} {Status} of {Technical} {Artefacts}},
	isbn = {978-94-007-7914-3},
	url = {https://books.google.com/books?id=BMPEBAAAQBAJ},
	publisher = {Springer Netherlands},
	author = {Kroes, Paul and Verbeek, Peter-Paul},
	year = {2014},
	note = {Series Title: Philosophy of Engineering and Technology},
}

@inproceedings{Keyes2018,
	title = {The misgendering machines: {Trans}/{HCI} implications of automatic gender recognition},
	volume = {2},
	doi = {10.1145/3274357},
	abstract = {Automatic Gender Recognition (AGR) is a subfield of facial recognition that aims to algorithmically identify the gender of individuals from photographs or videos. In wider society the technology has proposed applications in physical access control, data analytics and advertising. Within academia, it is already used in the field of Human-Computer Interaction (HCI) to analyse social media usage. Given the long-running critiques of HCI for failing to consider and include transgender (trans) perspectives in research, and the potential implications of AGR for trans people if deployed, I sought to understand how AGR and HCI understand the term "gender", and how HCI describes and deploys gender recognition technology. Using a content analysis of papers from both fields, I show that AGR consistently operationalises gender in a trans-exclusive way, and consequently carries disproportionate risk for trans people subject to it. In addition, I use the dearth of discussion of this in HCI papers that apply AGR to discuss how HCI operationalises gender, and the implications that this has for the field’s research. I conclude with recommendations for alternatives to AGR, and some ideas for how HCI can work towards a more effective and trans-inclusive treatment of gender.},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human}-{Computer} {Interaction}},
	author = {Keyes, Os},
	year = {2018},
	note = {Issue: CSCW
ISSN: 25730142},
	keywords = {Automatic gender recognition, Gender, Machine learning, Transgender},
}

@inproceedings{Strengers2020,
	title = {Adhering, {Steering}, and {Queering}: {Treatment} of {Gender} in {Natural} {Language} {Generation}},
	isbn = {978-1-4503-6708-0},
	doi = {10.1145/3313831.3376315},
	abstract = {Natural Language Generation (NLG) supports the creation of personalized, contextualized, and targeted content. However, the algorithms underpinning NLG have come under scrutiny for reinforcing gender, racial, and other problematic biases. Recent research in NLG seeks to remove these biases through principles of fairness and privacy. Drawing on gender and queer theories from sociology and Science and Technology studies, we consider how NLG can contribute towards the advancement of gender equity in society. We propose a conceptual framework and technical parameters for aligning NLG with feminist HCI qualities. We present three approaches: (1) adhering to current approaches of removing sensitive gender attributes, (2) steering gender differences away from the norm, and (3) queering gender by troubling stereotypes. We discuss the advantages and limitations of these approaches across three hypothetical scenarios; newspaper headlines, job advertisements, and chatbots. We conclude by discussing considerations for implementing this framework and related ethical and equity agendas.},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Strengers, Yolande and Qu, Lizhen and Xu, Qiongkai and Knibbe, Jarrod},
	year = {2020},
	keywords = {feminist hci, natural language generation},
	pages = {1--14},
}

@article{Andersen2017,
	title = {Transformative {Events} in the {LGBTQ} {Rights} {Movement}},
	volume = {5},
	number = {2},
	journal = {Indiana Journal of Law and Social Equality},
	author = {Andersen, Ellen Ann and Andersen, Ellen Ann},
	year = {2017},
}

@article{Birhane2021,
	title = {Towards decolonising computational sciences},
	volume = {29},
	url = {http://arxiv.org/abs/2009.14258},
	abstract = {This article sets out our perspective on how to begin the journey of decolonising computational fields, such as data and cognitive sciences. We see this struggle as requiring two basic steps: a) realisation that the present-day system has inherited, and still enacts, hostile, conservative, and oppressive behaviours and principles towards women of colour (WoC); and b) rejection of the idea that centering individual people is a solution to system-level problems. The longer we ignore these two steps, the more "our" academic system maintains its toxic structure, excludes, and harms Black women and other minoritised groups. This also keeps the door open to discredited pseudoscience, like eugenics and physiognomy. We propose that grappling with our fields' histories and heritage holds the key to avoiding mistakes of the past. For example, initiatives such as "diversity boards" can still be harmful because they superficially appear reformatory but nonetheless center whiteness and maintain the status quo. Building on the shoulders of many WoC's work, who have been paving the way, we hope to advance the dialogue required to build both a grass-roots and a top-down re-imagining of computational sciences -- including but not limited to psychology, neuroscience, cognitive science, computer science, data science, statistics, machine learning, and artificial intelligence. We aspire for these fields to progress away from their stagnant, sexist, and racist shared past into carving and maintaining an ecosystem where both a diverse demographics of researchers and scientific ideas that critically challenge the status quo are welcomed.},
	number = {2},
	journal = {Kvinder, Køn \& Forskning},
	author = {Birhane, Abeba and Guest, Olivia},
	year = {2021},
	note = {arXiv: 2009.14258},
	keywords = {anti-blackness, artificial intelligence, cognitive sciences, computational sciences, decolonisation, machine learning, misogynoir, tokenism},
	pages = {60--73},
}

@inproceedings{Chu2020,
	title = {{EntyFi}: {Entity} typing in fictional texts},
	isbn = {978-1-4503-6822-3},
	doi = {10.1145/3336191.3371808},
	abstract = {Fiction and fantasy are archetypes of long-tail domains that lack comprehensive methods for automated language processing and knowledge extraction. We present ENTYFI, the first methodology for typing entities in fictional texts coming from books, fan communities or amateur writers. ENTYFI builds on 205 automatically induced high-quality type systems for popular fictional domains, and exploits the overlap and reuse of these fictional domains for fine-grained typing in previously unseen texts. ENTYFI comprises five steps: type system induction, domain relatedness ranking, mention detection, mention typing, and type consolidation. The recall-oriented typing module combines a supervised neural model, unsupervised Hearst-style and dependency patterns, and knowledge base lookups. The precision-oriented consolidation stage utilizes co-occurrence statistics in order to remove noise and to identify the most relevant types. Extensive experiments on newly seen fictional texts demonstrate the quality of ENTYFI.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining} ({WSDM} 2020)},
	author = {Chu, Cuong Xuan and Razniewski, Simon and Weikum, Gerhard},
	year = {2020},
	keywords = {Entity typing, Fictional domains, Knowledge acquisition, Named entity recognition, Neural networks},
	pages = {124--132},
}

@article{Russakovsky2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {15731405},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	number = {3},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	year = {2015},
	note = {arXiv: 1409.0575},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
	pages = {211--252},
}

@inproceedings{Rahimtoroghi2017,
	title = {Modelling {Protagonist} {Goals} and {Desires} in {First}-{Person} {Narrative}},
	url = {http://arxiv.org/abs/1708.09040},
	abstract = {Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.},
	booktitle = {Proceedings of the {SIGDIAL} 2017 {Conference}},
	author = {Rahimtoroghi, Elahe and Wu, Jiaqi and Wang, Ruimin and Anand, Pranav and Walker, Marilyn A},
	year = {2017},
	note = {arXiv: 1708.09040},
	pages = {360--369},
}

@inproceedings{Yang2017,
	title = {Bi-directional {Joint} {Inference} for {User} {Links} and {Attributes} on {Large} {Social} {Graphs}.},
	isbn = {978-1-4503-4914-7},
	url = {http://dblp.uni-trier.de/db/conf/www/www2017c.html#YangZLJ17},
	doi = {10.1145/3041021.3054181},
	abstract = {Users on social networks primarily do two things, connect to existing or new friends and exchange information. Re-cently, social media apps have become the primary infor-mation source for users to consume news stories. Users are inundated with a flood of information from social media net-works shared by their friends and other content providers. For social media networks, it is important for us to study users' friendships and interests in order to best serve their need. However, given the nature of online applications, information on both sides is highly incomplete and noisy. Inspired by the well-known homophily phenomenon which found the two properties strongly interleaving, we propose to jointly learn them by leveraging their data redundancy and mutual reinforcement. Specifically, we exploit homophily by iteratively addressing smoothness on the graph in two direc-tions, i.e., from closeness to similarity (stronger links lead to more similar attributes), and vice versa. The two processes are done in a unified probabilistic framework through label propagation and graph construction. The refined user links and attributes are immediately useful for various tasks in-cluding link recommendation and content targeting on social networks.},
	booktitle = {Proceedings of the {World} {Wide} {Web} {Conference} ({WWW} '17)},
	author = {Yang, Carl and Zhong, Lin and Li, Li-Jia and Jie, Luo},
	year = {2017},
	keywords = {attribute profiling, link prediction, social network analysis},
	pages = {564--573},
}

@inproceedings{Volkova2015,
	title = {Inferring {Latent} {User} {Properties} from {Texts} {Published} in {Social} {Media}},
	isbn = {978-1-57735-704-9},
	abstract = {Abstract{\textbackslash}r{\textbackslash}nWe demonstrate an approach to predict latent personal attributes{\textbackslash}r{\textbackslash}nincluding user demographics, online personality,{\textbackslash}r{\textbackslash}nemotions and sentiments from texts published on Twitter. We{\textbackslash}r{\textbackslash}nrely on machine learning and natural language processing{\textbackslash}r{\textbackslash}ntechniques to learn models from user communications. We{\textbackslash}r{\textbackslash}nfirst examine individual tweets to detect emotions and opinions{\textbackslash}r{\textbackslash}nemanating from them, and then analyze all the tweets{\textbackslash}r{\textbackslash}npublished by a user to infer latent traits of that individual.{\textbackslash}r{\textbackslash}nWe consider various user properties including age, gender,{\textbackslash}r{\textbackslash}nincome, education, relationship status, optimism and life satisfaction.{\textbackslash}r{\textbackslash}nWe focus on Ekman’s six emotions: anger, joy, surprise,{\textbackslash}r{\textbackslash}nfear, disgust and sadness. Our work can help social{\textbackslash}r{\textbackslash}nnetwork users to understand how others may perceive them{\textbackslash}r{\textbackslash}nbased on how they communicate in social media, in addition{\textbackslash}r{\textbackslash}nto its evident applications in online sales and marketing, targeted{\textbackslash}r{\textbackslash}nadvertising, large scale polling and healthcare analytics.},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {Conference} on {Artificial} {Intelligence} ({AAAI})},
	author = {Volkova, Svitlana and Bachrach, Yoram and Armstrong, Michael and Sharma, Vijay},
	year = {2015},
	keywords = {Demonstrations Track},
	pages = {4296--4297},
}

@inproceedings{Chang2014,
	title = {What is {Tumblr}: {A} {Statistical} {Overview} and {Comparison}},
	volume = {16},
	doi = {10.1145/2674026.2674030},
	abstract = {Tumblr, as one of the most popular microblogging platforms, has gained momentum recently. It is reported to have 166.4 millions of users and 73.4 billions of posts by January 2014. While many articles about Tumblr have been published in major press, there is not much scholar work so far. In this paper, we provide some pioneer analysis on Tumblr from a variety of aspects. We study the social network structure among Tumblr users, analyze its user generated content, and describe reblogging patterns to analyze its user behavior. We aim to provide a comprehensive statistical overview of Tumblr and compare it with other popular social services, including blogosphere, Twitter and Facebook, in answering a couple of key questions: What is Tumblr? How is Tumblr different from other social media networks? In short, we find Tumblr has more rich content than other microblogging platforms, and it contains hybrid characteristics of social networking, traditional blogosphere, and social media. This work serves as an early snapshot of Tumblr that later work can leverage.},
	booktitle = {{SIGKDD} {Explorations}},
	author = {Chang, Yi and Tang, Lei and Inagaki, Yoshiyuki and Liu, Yan},
	year = {2014},
	note = {arXiv: 1403.5206v1
Issue: 1
ISSN: 19310145},
	pages = {21--30},
}

@inproceedings{Koh2020,
	title = {Concept {Bottleneck} {Models}},
	isbn = {978-1-71382-112-0},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-The-Art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-To-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-To-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: Accuracy improves significantly if we can correct model mistakes on concepts at test time.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning} ({ICML} 2020)},
	author = {Koh, Pang Wei and Nguye, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierso, Emma and Kim, Been and Liang, Percy},
	year = {2020},
	note = {arXiv: 2007.04612},
	pages = {5294--5304},
}

@inproceedings{arguello2006talk,
	title = {Talk to me: {Foundations} for successful individual-group interactions in online communities},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Arguello, Jaime and Butler, Brian S and Joyce, Elisabeth and Kraut, Robert and Ling, Kimberly S and Rosé, Carolyn and Wang, Xiaoqing},
	year = {2006},
	pages = {959--968},
}

@inproceedings{Fiesler2020,
	title = {Moving {Across} {Lands}: {Online} {Platform} {Migration} in {Fandom} {Communities}},
	volume = {4},
	doi = {10.1145/3392847},
	abstract = {When online platforms rise and fall, sometimes communities fade away, and sometimes they pack their bags and relocate to a new home. To explore the causes and effects of online community migration, we examine transformative fandom, a longstanding, technology-agnostic community surrounding the creation, sharing, and discussion of creative works based on existing media. For over three decades, community members have left and joined many different online spaces, from Usenet to Tumblr to platforms of their own design. Through analysis of 28 in-depth interviews and 1,886 survey responses from fandom participants, we traced these migrations, the reasons behind them, and their impact on the community. Our findings highlight catalysts for migration that provide insights into factors that contribute to success and failure of platforms, including issues surrounding policy, design, and community. Further insights into the disruptive consequences of migrations (such as social fragmentation and lost content) suggest ways that platforms might both support commitment and better support migration when it occurs.},
	booktitle = {Proceedings of the {ACM} on {Human}-{Computer} {Interaction}},
	author = {Fiesler, Casey and Dym, Brianna},
	year = {2020},
	note = {Issue: CSCW1
ISSN: 25730142},
	keywords = {Tumblr, fandom, fanfiction, history, online communities, platform design, policy, social media, usenet},
	pages = {1--25},
}

@inproceedings{Wilson2021,
	title = {Building and {Auditing} {Fair} {Algorithms}: {A} {Case} {Study} in {Candidate} {Screening}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445928},
	abstract = {Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of "fairness" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.},
	booktitle = {{ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency} ({FAccT} ’21)},
	author = {Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
	year = {2021},
	keywords = {adverse impact testing, algorithm auditing, fairness, four-fifths rule},
}

@inproceedings{OKeefe2012,
	title = {A sequence labelling approach to quote attribution},
	isbn = {978-1-937284-43-5},
	abstract = {Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. The present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. We treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. We present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features. © 2012 Association for Computational Linguistics.},
	booktitle = {2012 {Joint} {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and {Computational} {Natural} {Language} {Learning}, {Proceedings} of the {Conference} ({EMNLP}-{CoNLL} 2012 )},
	author = {O'Keefe, Tim and Pareti, Silvia and Curran, James R. and Koprinska, Irena and Honnibal, Matthew},
	year = {2012},
	note = {Issue: July},
	pages = {790--799},
}

@inproceedings{Huang2020,
	title = {Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition},
	isbn = {979-10-95546-34-4},
	abstract = {Existing research on fairness evaluation of document classification models mainly uses synthetic monolingual data without ground truth for author demographic attributes. In this work, we assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. The corpus covers five languages: English, Italian, Polish, Portuguese and Spanish. We evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, we take an empirical analysis of demographic predictability on the English corpus. We measure the performance of four popular document classifiers and evaluate the fairness and bias of the baseline classifiers on the author-level demographic attributes.},
	booktitle = {12th {International} {Conference} on {Language} {Resources} and {Evaluation}, {Conference} {Proceedings} ({LREC} 2020)},
	author = {Huang, Xiaolei and Xing, Linzi and Dernoncourt, Franck and Paul, Michael J.},
	year = {2020},
	note = {arXiv: 2002.10361
Issue: May},
	keywords = {Demographic bias, Document classification, Fairness, Hate speech, Multilingual},
	pages = {1440--1448},
}

@inproceedings{Kasunic2018,
	address = {New Orleans, Lousiana},
	title = {Learning to {Listen}: {Critically} {Considering} the {Role} of {AI} in {Human} {Storytelling} and {Character} {Creation}},
	doi = {10.18653/v1/w18-1501},
	abstract = {In this opinion piece, we argue that there is a need for alternative design directions to complement existing AI efforts in narrative and character generation and algorithm development. To make our argument, we a) outline the predominant roles and goals of AI research in storytelling; b) present existing discourse on the benefits and harms of narratives; and c) highlight the pain points in character creation revealed by semi-structured interviews we conducted with 14 individuals deeply involved in some form of character creation. We conclude by proffering several specific design avenues that we believe can seed fruitful research collaborations. In our vision, AI collaborates with humans during creative processes and narrative generation, helps amplify voices and perspectives that are currently marginal-ized or misrepresented, and engenders experiences of narrative that support spectatorship and listening roles.},
	booktitle = {Proceedings of the {First} {Workshop} on {Storytelling}},
	author = {Kasunic, Anna and Kaufman, Geoff},
	year = {2018},
	pages = {1--13},
}

@article{Ng2017,
	title = {Between text, paratext, and context: {Queerbaiting} and the contemporary media landscape},
	volume = {24},
	issn = {1941-2258},
	doi = {10.3983/twc.2017.0917},
	abstract = {I discuss the concept of queerbaiting as emergent from viewer readings of both textual and paratextual content at a particular juncture of LGBT media representation. While fan works as paratexts have attracted attention for their queered readings and narratives, there has been little scholarly consideration of how official paratexts that suggest or address queer readings, particularly promotional material and public commentary from producers, inform viewer engagement with media texts, and how they interact with contemporary conditions of media production and LGBT content. Examining F/F pairings from two television shows, Rizzoli \& Isles (TNT, 2010–16) and The 100 (CW, 2014–), I propose a model that incorporates text, paratext, and the context of LGBT representation to account for how both noncanonical and canonically queer narratives can exemplify queerbaiting discourses, as well as where queer subtextual readings are positioned in this interpretative space. In addition, I highlight the historical contingency of queerbaiting in terms of shifts in producer/viewer interactions and the character of LGBT narratives in reshaping the contestation of media meaning making.},
	journal = {Transformative Works and Cultures},
	author = {Ng, Eve},
	year = {2017},
	pages = {1--25},
}

@inproceedings{Larson2017,
	title = {Gender as a {Variable} in {Natural}-{Language} {Processing}: {Ethical} {Considerations}},
	doi = {10.18653/v1/w17-1601},
	abstract = {Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories/variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using gender as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other social categories, such as race, applied to human beings connected to NLP research.},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	author = {Larson, Brian},
	year = {2017},
	note = {Issue: 3},
	pages = {1--11},
}

@inproceedings{Vilares2019,
	title = {Harry {Potter} and the action prediction challenge from natural language},
	volume = {1},
	isbn = {978-1-950737-13-0},
	abstract = {We explore the challenge of action prediction from textual descriptions of scenes, a testbed to approximate whether text inference can be used to predict upcoming actions. As a case of study, we consider the world of the Harry Potter fantasy novels and inferring what spell will be cast next given a fragment of a story. Spells act as keywords that abstract actions (e.g. 'Alohomora' to open a door) and denote a response to the environment. This idea is used to automatically build HPAC, a corpus containing 82 836 samples and 85 actions. We then evaluate different baselines. Among the tested models, an LSTM-based approach obtains the best performance for frequent actions and large scene descriptions, but approaches such as logistic regression behave well on infrequent actions.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({NAACL}-{HLT} 2019)},
	author = {Vilares, David and Gómez-Rodríguez, Carlos},
	year = {2019},
	note = {arXiv: 1905.11037},
	pages = {2124--2130},
}

@article{Wallach2018,
	title = {Viewpoint: {Computational} social science ≠ computer science + social data},
	volume = {61},
	issn = {15577317},
	doi = {10.1145/3132698},
	number = {3},
	journal = {Communications of the ACM},
	author = {Wallach, Hanna},
	year = {2018},
	pages = {42--44},
}

@article{Haimson2019,
	title = {Tumblr was a trans technology: the meaning, importance, history, and future of trans technologies},
	volume = {21},
	issn = {14715902},
	url = {https://doi.org/10.1080/14680777.2019.1678505},
	doi = {10.1080/14680777.2019.1678505},
	abstract = {Building from previous researchers’ conceptions of queer technologies, we consider what it means to be a trans technology. This research study draws from interviews with Tumblr transition bloggers (n = 20), along with virtual ethnography, trans theory, and trans technological histories, using Tumblr as a case study to understand how social technologies can meet the needs of trans communities. Tumblr supported trans experiences by enabling users to change over time within a network of similar others, separate from their network of existing connections, and to embody (in a digital space) identities that would eventually become material. Further, before 2018 policy changes banning “adult” content, Tumblr upheld policies and an economic model that allowed erotic content needed for intersectional trans community building. We argue that these aspects made Tumblr a trans technology. We examine themes of temporality, openness, change, separation, realness, intersectionality, and erotics, along with considering social media platforms’ policies and economic models, to show how trans technologies can provide meaningful spaces for trans communities.},
	number = {3},
	journal = {Feminist Media Studies},
	author = {Haimson, Oliver L. and Dame-Griff, Avery and Capello, Elias and Richter, Zahari},
	year = {2021},
	note = {Publisher: Routledge},
	keywords = {Transgender, Tumblr, social media, social media policy, transgender theory},
	pages = {345--361},
}

@incollection{Hacking1986,
	address = {Stanford, CA},
	title = {Making up people},
	booktitle = {Reconstructing {Individualism}},
	publisher = {Stanford University Press},
	author = {Hacking, Ian},
	editor = {Heller, T.L and Sosna, M. and Wellbery, D.E.},
	year = {1986},
}

@inproceedings{Dym2019,
	title = {“{Coming} out okay”: {Community} narratives for {LGBTQ} identity recovery work},
	volume = {3},
	url = {http://dl.acm.org/citation.cfm?doid=3371885.3359256},
	doi = {10.1145/3359256},
	abstract = {Online communities provide support for those who are vulnerable, such as LGBTQ people while coming out. Research shows that social support and personal narrative construction are important when recovering from personal crises and traumatic events. As an online community focused on writing fanfiction and also consisting of a large number of LGBTQ members, transformative fandom provides an opportunity to examine the relationship between support, crisis, and narrative. Through an interview study with 31 LGBTQ fanfiction authors, our findings mirror Herman’s model of trauma recovery: these spaces self-organize to support recovery work through constructing "community narratives" that help LGBTQ people establish safety when exploring their identity and build LGBTQ support structures without publicly outing themselves before they are ready, challenge stereotypes, and support others through reshaping existing media that perpetuate inaccurate or harmful LGBTQ narratives. These online communities embody "selective visibility"–that is, though not specifically designed as support structures for identity work and recovery, their design allows people to selectively find and create communities of support for stigmatized issues that they might be unable to safely seek out in other spaces. Based on lessons learned, we generate insights that can inform the design of safe support spaces online.},
	booktitle = {Proceedings of the {ACM} on {Human}-{Computer} {Interaction}},
	author = {Dym, Brianna and Brubaker, Jed R. and Fiesler, Casey and Semaan, Bryan},
	year = {2019},
	note = {Issue: CSCW
ISSN: 25730142},
	keywords = {Fandom, Fanfiction, LGBTQ, Recovery, Trauma, Visuality},
	pages = {1--28},
}

@article{Crenshaw1989,
	title = {Demarginalizing the intersection of race and sex: {A} black feminist critique of antidiscrimination doctrine, feminist theory, and antiracist politics},
	doi = {10.4324/9780429500480},
	abstract = {One of the very few Black women's studies books is entitled All the Women Are White, All the Blacks Are Men, But Some of Us are Brave. I have chosen this title as a point of departure in my efforts to develop a Black feminist criticism because it sets forth a problematic consequence of the tendency to treat race and gender as mutually exclusive categories of experience and analysis. In this talk, I want to examine how this tendency is perpetuated by a single-axis framework that is dominant in antidiscrimination law and that is also reflected in feminist theory and antiracist politics.},
	journal = {Feminist Legal Theory: Readings in Law and Gender},
	author = {Crenshaw, Kimberlé},
	year = {1989},
	note = {ISBN: 9780429969034},
	pages = {57--80},
}

@inproceedings{Laviolette2019,
	title = {Using platform signals for distinguishing discourses: {The} case of men’s rights and men’s liberation on {Reddit}},
	abstract = {Reddit’s men’s rights community (/r/MensRights) has been criticized for the promotion of misogynistic language, toxic masculinity and discourses that reinforce alt-right ideologies. Conversely, the men’s liberation (/r/MensLib) community integrates inclusive politics, intersectionality and masculinity within a broad umbrella of self-reflection that suggests toxic masculinity harms men as well as women. We use machine learning text classifiers, keyword frequencies, and qualitative approaches first to distinguish these two subreddits, and second to interpret the differences ideologically rather than topically. We further integrate platform metadata (referred to as ‘platform signals’) to distinguish the subreddits. These signals help us understand how similar terms can be used to arrive at different interpretations of gender and discrimination. Where /r/MensLib tends to see masculinity as an adjective and women as peers, /r/MensRights views being a man as an essential quality, men as the target of discrimination, and women as sources of personalized grievances.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} and {Social} {Media}, {ICWSM} 2019},
	author = {LaViolette, Jack and Hogan, Bernie},
	year = {2019},
	keywords = {Computational critical discourse analysis, Men’s issues, Men’s rights, Platform signals, Reddit},
	pages = {323--334},
}

@article{Mikolov2013,
	title = {Exploiting {Similarities} among {Languages} for {Machine} {Translation}},
	url = {http://arxiv.org/abs/1309.4168},
	abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
	author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
	year = {2013},
	note = {arXiv: 1309.4168},
}

@inproceedings{Zhao2018,
	title = {Learning {Gender}-{Neutral} {Word} {Embeddings}},
	abstract = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern , in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functional-ity of the embedding model.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Zhao, Jieyu and Zhou, Yichao and Li, Zeyu and Wang, Wei and Chang, Kai-Wei},
	year = {2018},
	note = {arXiv: 1809.01496v1},
	pages = {4847--4853},
}

@inproceedings{Alrajebah2017,
	title = {Deconstructing {Diffusion} on {Tumblr} : {Structural} and {Temporal} {Aspects}},
	isbn = {978-1-4503-4896-6},
	doi = {10.1145/3091478.3091491},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Web} {Science}},
	author = {Alrajebah, Nora and Carr, Leslie and Luczak-roesch, Markus and Tiropanis, Thanassis},
	year = {2017},
	keywords = {cascades, information diffusion, social network analysis},
	pages = {319--328},
}

@inproceedings{Herbelot2012,
	title = {Distributional techniques for philosophical enquiry},
	abstract = {This paper illustrates the use of distribu-tional techniques, as investigated in compu-tational semantics, for supplying data from large-scale corpora to areas of the human-ities which focus on the analysis of con-cepts. We suggest that the distributional no-tion of 'characteristic context' can be seen as evidence for some representative tenden-cies of general discourse. We present a case study where distributional data is used by philosophers working in the areas of gen-der studies and intersectionality as confir-mation of certain trends described in pre-vious work. Further, we highlight that dif-ferent models of phrasal distributions can be compared to support the claim of inter-sectionality theory that 'there is more to a phrase than the intersection of its parts'.},
	booktitle = {Proceedings of the 6th {Workshop} on {Language} {Technology} for {Cultural} {Heritage}, {Social} {Sciences}, and {Humanities}},
	publisher = {Association for Computational Linguistics},
	author = {Herbelot, Aurelie and von Redecker, Eva and Müller, Johanna},
	year = {2012},
	note = {Issue: April},
	keywords = {★},
	pages = {45--54},
}

@book{Browne2015,
	title = {Dark matters: on the surveillance of {Blackness}},
	abstract = {One of the greatest mysteries in the cosmos is that it is mostly dark. That is, not only is the night sky dark, but also most of the matter and the energy in the universe is dark. For every atom visible in planets, stars and galaxies today there exists at least five or six times as much "Dark Matter" in the universe. Astronomers and particle physicists today are seeking to unravel the nature of this mysterious but pervasive dark matter, which has profoundly influenced the formation of structure in the universe. Dark energy remains even more elusive, as we lack candidate fields that emerge from well established physics. I will describe various attempts to measure dark matter by direct and indirect means, and discuss the prospects for progress in unravelling dark energy. © 2010 IOP Publishing Ltd.},
	publisher = {Duke University Press},
	author = {Browne, Simone},
	year = {2015},
}

@article{Joshi2020,
	title = {{SpanBERT}: {Improving} pre-training by representing and predicting spans},
	volume = {8},
	issn = {23318422},
	doi = {10.1162/tacl_a_00300},
	abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	year = {2020},
	note = {arXiv: 1907.10529},
	pages = {64--77},
}

@inproceedings{Krishnan2016,
	title = {Seeing the forest for the trees: {New} approaches to forecasting cascades},
	isbn = {978-1-4503-4208-7},
	doi = {10.1145/2908131.2908155},
	abstract = {Cascades are a popular construct to observe and study in- formation propagation (or diffusion) in social media such as Twitter. and are defined using notions of influence, activity, or discourse commonality (e.g., hashtags). While these notions of cascades lead to different perspectives, primarily cascades are modeled as trees. We argue in this paper an alternative viewpoint of cascades as forests (of trees) which yields a richer vocabulary of features to understand information propagation. We develop a framework to extract forests and analyze their growth by studying their evolution at the tree-level and at the node-level. Moreover, we demonstrate how the structural features of forests, properties of the underlying network, and temporal features of the cascades provide significant predictive value in forecasting the future trajectory of both size and shape of forests. We observe that the forecasting performance increases with observations, that the temporal features are highly indicative of cascade size, and that the features extracted from the underlying connected graph best forecast the shape of the cascade.},
	booktitle = {{WebSci} 2016 - {Proceedings} of the 2016 {ACM} {Web} {Science} {Conference}},
	author = {Krishnan, Siddharth and Butler, Patrick and Tandon, Ravi and Leskovec, Jure and Ramakrishnan, Naren},
	year = {2016},
	pages = {249--258},
}

@incollection{Johnstone2018,
	title = {Prior texts, prior discourses},
	booktitle = {Discourse {Analysis}},
	publisher = {Wiley},
	author = {Johnstone, Barbara},
	year = {2018},
}

@article{Grimmer2013,
	title = {Text as data: {The} promise and pitfalls of automatic content analysis methods for political texts},
	volume = {21},
	issn = {14764989},
	doi = {10.1093/pan/mps028},
	abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods-they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation. © The Author 2013.Published by Oxford University Press on behalf of the Society for Political Methodology. All rights reserved.},
	number = {3},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon M.},
	year = {2013},
	pages = {267--297},
}

@article{roberts_model_2016,
	title = {A {Model} of {Text} for {Experimentation} in the {Social} {Sciences}},
	volume = {111},
	issn = {1537274X},
	url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	number = {515},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	year = {2016},
	note = {Publisher: Taylor \& Francis},
	keywords = {Causal inference, Experimentation, High-dimensional inference, Social sciences, Text analysis, Variational approximation},
	pages = {988--1003},
}

@inproceedings{Keith2017,
	title = {Identifying civilians killed by police with distantly supervised entity-event extraction},
	volume = {2000},
	isbn = {978-1-945626-83-8},
	doi = {10.18653/v1/d17-1163},
	abstract = {We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases. Appendix, software, and data are available online at: http://slanglab.cs.umass. edu/PoliceKillingsExtraction/.},
	booktitle = {{EMNLP} 2017 - {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {Proceedings}},
	author = {Keith, Katherine A. and Handler, Abram and Pinkham, Michael and Magliozzi, Cara and McDuffie, Joshua and O’Connor, Brendan},
	year = {2017},
	note = {arXiv: 1707.07086},
	pages = {1547--1557},
}

@article{Bucholtz2004,
	title = {Theorizing identity in language and sexuality research},
	volume = {33},
	journal = {Language in Society},
	author = {Bucholtz, Mary and Hall, Kira},
	year = {2004},
	pages = {469--515},
}

@inproceedings{arazy_how_2017,
	title = {On the "how" and "why" of emergent role behaviors in {Wikipedia}},
	isbn = {978-1-4503-4335-0},
	doi = {10.1145/2998181.2998317},
	abstract = {Research on peer-production suggests that as participants choose what actions to perform, prototypical activity patterns emerge. Recent work characterized these patterns and demonstrated that informal emergent roles are highly stable. Nonetheless, we know little about the ways in which contributors take on and shed emergent roles. The objectives of this study are to: (a) delineate the temporal dynamics of participants' emergent role taking behaviors, and (b) identify the motivations driving role-transition behaviors. Our study links motivation to role-transition behaviors within Wikipedia. Our first sample covered eleven years and 222,119 contributors, and was used to identify four categories of temporal role-taking behaviors, that differ in their mobility between emergent roles and across Wikipedia articles. Our second examination linked the motivations of 175 new participants to their subsequent role-taking activity over 14 months. Together, the two analyses reveal that role-taking categories can be distinguished based on participants' motivational orientation (intrinsic/extrinsic and self/others-oriented).},
	booktitle = {Proceedings of the {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work}, {CSCW}},
	author = {Arazy, Ofer and Lifshitz-Assaf, Hila and Nov, Oded and Daxenberger, Johannes and Balestra, Martina and Cheshire, Coye},
	year = {2017},
	keywords = {Emergent roles, Motivation, Online production communities, Role mobility, Role-taking, Wikipedia},
	pages = {2039--2051},
}

@inproceedings{raza_job_2013,
	title = {Job opportunities through entertainment : {Virally} spread speech -based services for low-literate users},
	isbn = {978-1-4503-1899-0},
	doi = {10.1145/2470654.2481389},
	abstract = {We explore how telephone-based services might be mass adopted by low-literate users in the developing world. We focus on speech and push-button dialog systems requiring neither literacy nor training. Building on the success of Polly, a simple telephone-based voice manipulation and forwarding system that was first tested in 2011, we report on its first large-scale sustained deployment. In 24/7 operation in Pakistan since May 9, 2012, as of mid-September Polly has spread to 85,000 users, engaging them in 495,000 interactions, and is continuing to spread to 1,000 new people daily. It has also attracted 27,000 people to a job search service, who in turn listened 279,000 times to job ads and forwarded them 22,000 times to their friends. We report users' activity over time and across demographics, analyze user behavior within several randomized controlled trials, and describe lessons learned regarding spread, scalability and sustainability of telephone-based speech-based services. Copyright © 2013 ACM.},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	author = {Raza, Agha Ali and Ul Haq, Farhan and Tariq, Zain and Pervaiz, Mansoor and Razaq, Samia and Saif, Umar and Rosenfeld, Roni},
	year = {2013},
	keywords = {Cellular phones, Communication services, Entertainment, HCI4D, ICT4D, Illiteracy, Information services, Job search, Low-literate, Low-skill jobs, Mobile phones, Speech interfaces, Telephone, Viral},
	pages = {2803--2812},
}

@inproceedings{Hamilton2016,
	title = {Cultural {Shift} or {Linguistic} {Drift}? {Comparing} {Two} {Computational} {Measures} of {Semantic} {Change}},
	url = {http://arxiv.org/abs/1606.02821},
	abstract = {Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics, it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise ("I promise." -{\textgreater} "It promised to be exciting."). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell ("prison cell" -{\textgreater} "cell phone"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics.},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	year = {2016},
	note = {arXiv: 1606.02821},
	pages = {2116},
}

@article{Devos2005,
	title = {American = {White}?},
	volume = {88},
	issn = {0022-3514},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.88.3.447},
	doi = {10.1037/0022-3514.88.3.447},
	number = {3},
	journal = {Journal of Personality and Social Psychology},
	author = {Devos, Thierry and Banaji, Mahzarin R.},
	year = {2005},
	pmid = {15740439},
	note = {ISBN: 0022-3514{\textbackslash}n1939-1315},
	pages = {447--466},
}

@article{Gavrilos2010,
	title = {Becoming ‘100\% {American}’: negotiating ethnic identities through nativist discourse},
	volume = {7},
	issn = {1740-5904},
	doi = {10.1080/17405901003675398},
	abstract = {This study traced the negotiation of national and ethnic identities in the USA in the early twentieth century - a time of intense anti-immigrant and anti-German war sentiment. The public discourse analyzed centered around a particularly infamous ban on all non-English languages in the state of Iowa. This ban symbolized the hegemonic definition of national identity at the time, one that implicitly equated national identity to Anglo ethnicity. Alternatively, non-Anglos re-positioned ethnic differences through a discourse of patriotism (e.g., praying in a 'foreign' language for US victory in the war was an innocent practice of ethnic difference in the service of the nation). Equating ethnic differences to patriotism, however, resulted in the de-legitimization and erasure of non-Anglo ethnic identities. By pointing to past discursive struggles to define national identity featuring ethnic groups who are today readily accepted as 'American' (e.g., German-Americans), we can recast today's predominant and highly problematic narratives concerning immigrants and national identity. These narratives glorify the nation's past (and earlier immigrants) by highlighting a long and easy tradition of national assimilation and unity. This historical past is represented in contrast to today's purportedly disruptive, menacing and non-assimilating immigrants. This study seeks to intervene in such popular discourses through an analysis of one case, out of many, that demonstrates how national identity is an ongoing social invention defined through Others, not as the exception, but as the norm. Historically, Others are always on the national scene. [ABSTRACT FROM AUTHOR]},
	number = {2},
	journal = {Critical Discourse Studies},
	author = {Gavrilos, Dina},
	year = {2010},
	keywords = {discourse, ethnicity, national identity},
	pages = {95--112},
}

@inproceedings{Jaroszewski2018,
	title = {"{Genderfluid}" or "attack helicopter": {Responsible} {HCI} practice with non-binary gender variation in online communities},
	volume = {2018-April},
	isbn = {978-1-4503-5620-6},
	doi = {10.1145/3173574.3173881},
	abstract = {As non-binary genders become increasingly prevalent, researchers face decisions in how to collect, analyze and interpret research participants' genders. We present two case studies on surveys with thousands of respondents, of which hundreds reported gender as something other than simply women or men. First, Tumblr, a blogging platform, resulted in a rich set of gender identities with very few aggressive or resistive responses; the second case study, online Fantasy Football, yielded opposite proportions. By focusing on variation rather than dismissing non-binary responses as noise, we suggest that researchers can better capture gender in a way that 1) addresses gender variation without othering or erasing non-binary respondents; and 2) minimizes "trolls'" opportunity to use surveys as a mischief platform. The analyses of these two distinct case studies find significant gender differences in community dimensions of participation in both networked spaces as well as offering a model for inclusive mixed-methods HCI research.},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	author = {Jaroszewski, Samantha and Lottridge, Danielle and Haimson, Oliver L. and Quehl, Katie},
	year = {2018},
	keywords = {Fantasy sports, Gender, LGBTQ, Non-binary, Online communities, Social media, Survey research, Transgender, Trolling, Tumblr},
	pages = {1--14},
}

@phdthesis{author,
	title = {The {Critical} {Fan} {Toolkit}: {Fanfiction} {Genres}, {Ideologies}, and {Pedagogies}},
	url = {https://www.proquest.com/dissertations-theses/critical-fan-toolkit-fanfiction-genres-ideologies/docview/2532587933/se-2?accountid=9902},
	school = {Northeastern University},
	author = {Messina, Cara Marta},
	year = {2021},
}

@article{yayat_rahmat_hidayat_no_1967,
	title = {{済無No} {Title} {No} {Title} {No} {Title}},
	volume = {4},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy bynhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	number = {March},
	journal = {Angewandte Chemie International Edition, 6(11), 951–952.},
	author = {{Yayat Rahmat Hidayat}},
	year = {1967},
	keywords = {perum bulog, rice, s, supply chain management},
	pages = {763--773},
}

@article{Bode2020,
	title = {Why you can't model away bias},
	volume = {8},
	issn = {00267929},
	doi = {10.1215/00267929-7933102},
	number = {1},
	journal = {Modern Language Quarterly},
	author = {Bode, Katherine},
	year = {2020},
	note = {ISBN: 0026792979331},
	pages = {95--124},
}

@article{drucker2011humanities,
	title = {Humanities approaches to graphical display},
	author = {Drucker, Johanna},
}

@article{drucker_and_nodate,
	title = {and {Can} {Be} {Characterized} {As}},
	author = {Drucker, Johanna},
	pages = {1--15},
}

@inproceedings{Nguyen2013,
	title = {"{How} {Old} {Do} {You} {Think} {I} {Am}?" {A} {Study} of {Language} and {Age} in {Twitter}},
	booktitle = {Proceedings of the {Seventh} {International} {AAAI} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Nguyen, Dong and Gravel, Rilana and Trieschnigg, Dolf and Meder, Theo},
	year = {2013},
	keywords = {Full Paper},
	pages = {439--448},
}

@article{Fang2015,
	title = {Relational user attribute inference in social media},
	volume = {17},
	issn = {15209210},
	doi = {10.1109/TMM.2015.2430819},
	abstract = {Nowadays, more and more people are engaged in social media to generate multimedia information, i.e., creating text and photo profiles and posting multimedia messages. Such multimodal social networking activities reveal multiple user attributes such as age, gender, and personal interest. Inferring user attributes is important for user profiling, retrieval , and personalization. Existing work is devoted to inferring user attributes independently and ignores the dependency relations between attributes. In this work, we investigate the problem of relational user attribute inference by exploring the relations between user attributes and extracting both lexical and visual features from online user-generated content. We systematically study six types of user attributes: gender, age, relationship , occupation , interest, and emotional orientation. In view of methodology , we propose a relational latent SVM (LSVM) model to combine a rich set of user features, attribute inference, and attribute relations in a unified framework. In the model, one attribute is selected as the target attribute and others are selected as the auxiliary attributes to assist the target attribute inference. The model infers user attributes and attribute relations simultaneously. Extensive experiments conducted on a collected dataset from Google+ with full attribute annotations demonstrate the effectiveness of the proposed approach in user attribute inference and attribute-based user retrieval.},
	number = {7},
	journal = {IEEE Transactions on Multimedia},
	author = {Fang, Quan and Sang, Jitao and Xu, Changsheng and Hossain, M. Shamim},
	year = {2015},
	keywords = {Attribute relation, latent SVM (LSVM), user attribute inference},
	pages = {1031--1044},
}

@article{Rangel2017,
	title = {Overview of the 5th author profiling task at {PAN} 2017: {Gender} and language variety identification in {Twitter}},
	volume = {1866},
	issn = {16130073},
	abstract = {This overview presents the framework and the results of the Author Profiling task at PAN 2017. The objective of this year is to address gender and language variety identification. For this purpose a corpus from Twitter has been provided for four different languages: Arabic, English, Portuguese, and Spanish. Altogether, the approaches of 22 participants are evaluated.},
	journal = {CEUR Workshop Proceedings},
	author = {Rangel, Francisco and Rosso, Paolo and Potthast, Martin and Stein, Benno},
	year = {2017},
}

@article{Hanna2020,
	title = {Towards a critical race methodology in algorithmic fairness},
	doi = {10.1145/3351095.3372826},
	abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
	journal = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
	year = {2020},
	note = {arXiv: 1912.03593
ISBN: 9781450369367},
	keywords = {Algorithmic fairness, Critical race theory, Ethnicity, Race},
	pages = {501--512},
}

@article{Nguyen2014,
	title = {{TweetGenie}: {Development}, {Evaluation}, and {Lessons} {Learned}},
	volume = {2},
	abstract = {TweetGenie is an online demo that infers the gender and age of Twitter users based on their tweets. TweetGenie was able to attract thousands of visitors. We collected data by asking feedback from visitors and launching an online game. In this paper, we describe the development of TweetGenie and evaluate the demo based on the received feedback and manual annotation. We also reflect on practical lessons learned from launching a demo for the general public.},
	number = {1},
	journal = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations},
	author = {Nguyen, Dong and Trieschnigg, Dolf and Meder, Theo},
	year = {2014},
	pages = {62--66},
}

@inproceedings{Kang2019,
	title = {({Male}, {Bachelor}) and ({Female}, {Ph}.{D}) have different connotations: {Parallelly} {Annotated} {Stylistic} {Language} {Dataset} with {Multiple} {Personas}},
	doi = {10.18653/v1/d19-1179},
	abstract = {Stylistic variation in text needs to be studied with different aspects including the writer's personal traits, interpersonal relations, rhetoric, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains {\textasciitilde}41K parallel sentences (8.3K parallel stories) annotated across different personas. Each persona has different styles in conjunction: gender, age, country, political view, education, ethnic, and time-of-writing. The dataset is collected from human annotators with solid control of input denotation: not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the dataset on two interesting applications of style language, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple supervised model with our parallel text outperforms the unsupervised models using nonparallel text in style transfer. Our dataset is publicly available.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Kang, Dongyeop and Gangal, Varun and Hovy, Eduard},
	year = {2019},
	note = {arXiv: 1909.00098},
	pages = {1696--1706},
}

@inproceedings{Li2019,
	title = {{ALOHA}: {Artificial} {Learning} of {Human} {Attributes} for {Dialogue} {Agents}},
	url = {http://arxiv.org/abs/1910.08293},
	abstract = {For conversational AI and virtual assistants to communicate with humans in a realistic way, they must exhibit human characteristics such as expression of emotion and personality. Current attempts toward constructing human-like dialogue agents have presented significant difficulties. We propose Human Level Attributes (HLAs) based on tropes as the basis of a method for learning dialogue agents that can imitate the personalities of fictional characters. Tropes are characteristics of fictional personalities that are observed recurrently and determined by viewers' impressions. By combining detailed HLA data with dialogue data for specific characters, we present a dataset, HLA-Chat, that models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs. We then introduce a three-component system, ALOHA (which stands for Artificial Learning of Human Attributes), that combines character space mapping, character community detection, and language style retrieval to build a character (or personality) specific language model. Our preliminary experiments demonstrate that two variations of ALOHA, combined with our proposed dataset, can outperform baseline models at identifying the correct dialogue responses of chosen target characters, and are stable regardless of the character's identity, the genre of the show, and the context of the dialogue.},
	booktitle = {Proceedings of {AAAI}},
	author = {Li, Aaron W. and Jiang, Veronica and Feng, Steven Y. and Sprague, Julia and Zhou, Wei and Hoey, Jesse},
	year = {2019},
	note = {arXiv: 1910.08293},
}

@inproceedings{Reddy2016,
	title = {Obfuscating {Gender} in {Social} {Media} {Writing}},
	booktitle = {Proceedings of the {First} {Workshop} on {NLP} and {Computational} {Social} {Science}},
	author = {Reddy, Sravana and Knight, Kevin},
	year = {2016},
	pages = {17--26},
}

@book{omi2014racial,
	title = {Racial {Formation} in the {United} {States}},
	isbn = {978-1-135-12751-0},
	url = {https://books.google.com/books?id=T7LcAwAAQBAJ},
	publisher = {Taylor {\textbackslash}\& Francis},
	author = {Omi, M and Winant, H},
	year = {2014},
}

@book{sedgwick1990epistemology,
	title = {Epistemology of the {Closet}},
	isbn = {978-0-520-07874-1},
	url = {https://books.google.com/books?id=u5jgaOhhmpgC},
	publisher = {University of California Press},
	author = {Sedgwick, E K},
	year = {1990},
	note = {Series Title: A centennial book},
}

@book{page1985acts,
	title = {Acts of {Identity}: {Creole}-{Based} {Approaches} to {Language} and {Ethnicity}},
	isbn = {978-0-521-31604-0},
	url = {https://books.google.com/books?id=cbQ8AAAAIAAJ},
	publisher = {Cambridge University Press},
	author = {Le Page, Robert Brock and Tabouret-Keller, Andrée},
	year = {1985},
}

@book{Saucier2016,
	title = {Conceptual aphasia in black: {Displacing} racial formation},
	isbn = {978-1-4985-1701-0},
	publisher = {Lexington Books},
	editor = {Saucier, P. Khalil and Wood, Tryon P.},
	year = {2016},
}

@article{CharityHudley2020,
	title = {Toward racial justice in linguistics: {Interdisciplinary} insights into theorizing race in the discipline and diversifying the profession},
	volume = {96},
	issn = {15350665},
	doi = {10.1353/lan.2020.0074},
	abstract = {This article builds on the Linguistic Society of America’s Statement on Race to argue that linguistics urgently needs an interdisciplinarily informed theoretical engagement with race and racism. To be adequate, a linguistic theory of race must incorporate the perspectives of linguistic researchers of different methodological approaches and racial backgrounds and must also draw on theories of race in neighboring fields, including anthropology, sociology, and psychology, as well as speech and hearing sciences, composition and literacy studies, education, and critical interdisciplinary race studies. The lack of comprehensive and up-to-date theoretical, analytical, and political understandings of race within linguistics not only weakens research by erasing, marginalizing, and misrepresenting racially minoritized groups, but it also diminishes the impact of the entire field by devaluing and excluding the intellectual contributions of researchers of color, whose work on this topic is rarely welcome within linguistics departments. The article therefore argues for a rethinking of both linguistic scholarship and linguistics as a discipline in more racially inclusive and socially just terms.*.},
	number = {4},
	journal = {Language},
	author = {Charity Hudley, Anne H. and Mallinson, Christine and Bucholtz, Mary},
	year = {2020},
	keywords = {Discipline of linguistics, Diversity, Interdisciplinarity, Race and racism, Racial justice, Social justice, Social theory},
	pages = {e200--e235},
}

@article{Eckert2012,
	title = {Three {Waves} of {Variation} {Study}: {The} {Emergence} of {Meaning} in the {Study} of {Sociolinguistic} {Variation}},
	volume = {41},
	issn = {0084-6570},
	doi = {10.1146/annurev-anthro-092611-145828},
	abstract = {The treatment of social meaning in sociolinguistic variation has come in three waves of analytic practice. The first wave of variation studies established broad correlations between linguistic variables and the macrosociological categories of socioeconomic class, gender, ethnicity, and age. The second wave employed ethnographic methods to explore the local categories and configurations that inhabit, or constitute, these broader categories. In both waves, variation was seen as marking social categories. This article sets out a theoretical foundation for the third wave, arguing that (a) variation constitutes a robust social semiotic system, potentially expressing the full range of social concerns in a given community; (b) the meanings of variables are underspecified, gaining more specific meanings in the context of styles, and (c) variation does not simply reflect, but also constructs, social meaning and hence is a force in social change.},
	number = {1},
	journal = {Annual Review of Anthropology},
	author = {Eckert, Penelope},
	year = {2012},
	keywords = {enregisterment, indexicality, social meaning, style},
	pages = {87--100},
}

@article{Laviolette2017,
	title = {Cyber-metapragmatics and alterity on reddit.com},
	url = {https://research.tilburguniversity.edu/en/publications/cyber-metapragmaticsand-alterity-on-redditcom},
	number = {196},
	journal = {Tilburg Papers in Culture Studies},
	author = {Laviolette, Jack},
	year = {2017},
}

@inproceedings{jorgensen_learning_2016,
	title = {Learning a {POS} tagger for {AAVE}-like language},
	isbn = {978-1-941643-91-4},
	abstract = {Part-of-speech (POS) taggers trained on newswire perform much worse on domains such as subtitles, lyrics, or tweets. In addition, these domains are also heterogeneous, e.g., with respect to registers and dialects. In this paper, we consider the problem of learning a POS tagger for subtitles, lyrics, and tweets associated with African-American Vernacular English (AAVE). We learn from a mixture of randomly sampled and manually annotated Twitter data and unlabeled data, which we au-tomatically and partially label using mined tag dictionaries. Our POS tagger obtains a tag-ging accuracy of 89\% on subtitles, 85\% on lyrics, and 83\% on tweets, with up to 55\% er-ror reductions over a state-of-the-art newswire POS tagger, and 15-25\% error reductions over a state-of-the-art Twitter POS tagger.},
	booktitle = {Proceedings of {NAACL}},
	author = {Jørgensen, Anna and Hovy, Dirk and Søgaard, Anders},
	year = {2016},
	pages = {1115--1120},
}

@article{roy_analysis_2021,
	title = {Analysis of {Nuanced} {Stances} and {Sentiment} {Towards} {Entities} of {US} {Politicians} through the {Lens} of {Moral} {Foundation} {Theory}},
	doi = {10.18653/v1/2021.socialnlp-1.1},
	abstract = {The Moral Foundation Theory suggests five moral foundations that can capture the view of a user on a particular issue. It is widely used to identify sentence-level sentiment. In this paper, we study the nuanced stances and partisan sentiment towards entities of US politicians using Moral Foundation Theory, on two politically divisive issues-Gun Control and Immigration. We define the nuanced stances of the US politicians on these two topics by the grades given by related organizations to the politicians. To conduct this study, we first filter out 74k and 87k tweets on the topics Gun Control and Immigration, respectively, from an existing tweet corpus authored by US parliament members. Then, we identify moral foundations in these tweets using deep relational learning. Finally, doing qualitative and quantitative evaluations on this dataset, we found out that there is a strong correlation between moral foundation usage and politicians' nu-anced stances on a particular topic. We also found notable differences in moral foundation usage by different political parties when they address different entities.},
	author = {Roy, Shamik and Goldwasser, Dan},
	year = {2021},
	pages = {1--13},
}

@article{Sybert2021,
	title = {The demise of \#{NSFW}: {Contested} platform governance and {Tumblr}’s 2018 adult content ban},
	issn = {14617315},
	doi = {10.1177/1461444821996715},
	abstract = {On December 3, 2018, Tumblr announced that it would ban sexually explicit content from the platform, drawing immediate backlash from users. The ensuing discord on the site is conceptualized here as contested platform governance, or a conflict between users and ownership, in which not only are a platform’s policies and features challenged, but also its core values, identity, and/or purposes are put into question. By examining 238 Tumblr posts, this analysis identifies the unique ways users combatted the ban and (re)inscribed community values, while also contesting the owners’ legitimacy to govern the platform. Holding implications for the site’s long-term survival, such conflicts capture a critical moment in which the boundaries of power between users and ownership are challenged and, possibly, transformed. By examining Tumblr’s Not Safe For Work (NSFW) ban through the lens of platform governance, this study offers insight into how power and its limits are negotiated online.},
	journal = {New Media and Society},
	author = {Sybert, Jeanna},
	year = {2021},
	keywords = {Adult content ban, NSFW, Tumblr, legitimacy, online resistance, platform governance, social media platforms},
}

@inproceedings{Martin2016,
	title = {Exploring limits to prediction in complex social systems},
	isbn = {978-1-4503-4143-1},
	doi = {10.1145/2872427.2883001},
	abstract = {How predictable is success in complex social systems? In spite of a recent profusion of prediction studies that exploit online social and information network data, this question remains unanswered, in part because it has not been adequately specified. In this paper we attempt to clarify the question by presenting a simple stylized model of success that attributes prediction error to one of two generic sources: insufficiency of available data and/or models on the one hand; and inherent unpredictability of complex social systems on the other. We then use this model to motivate an illustrative empirical study of information cascade size prediction on Twitter. Despite an unprecedented volume of information about users, content, and past performance, our best performing models can explain less than half of the variance in cascade sizes. In turn, this result suggests that even with unlimited data predictive performance would be bounded well below deterministic accuracy. Finally, we explore this potential bound theoretically using simulations of a difiusion process on a random scale free network similar to Twitter. We show that although higher predictive power is possible in theory, such performance requires a homogeneous system and perfect ex-Ante knowledge of it: even a small degree of uncertainty in estimating product quality or slight variation in quality across products leads to substantially more restrictive bounds on predictability. We conclude that realistic bounds on predictive accuracy are not dissimilar from those we have obtained empirically, and that such bounds for other complex social systems for which data is more difficult to obtain are likely even lower.},
	booktitle = {25th {International} {World} {Wide} {Web} {Conference}, {WWW} 2016},
	author = {Martin, Travis and Hofman, Jake M. and Sharma, Amit and Anderson, Ashton and Watts, Duncan J.},
	year = {2016},
	note = {arXiv: 1602.01013},
	pages = {683--694},
}

@inproceedings{Devito2018,
	title = {“{Too} {Gay} for {Facebook}”: {Presenting} {LGBTQ} + {Identity} {Throughout} the {Personal} {Social} {Media} {Ecosystem}},
	volume = {2},
	booktitle = {{CSCW}},
	author = {Devito, Michael A. and Walker, Ashley Marie and Birnholtz, Jeremy},
	year = {2018},
	note = {Issue: November},
	keywords = {identity management, self-presentation, sexual and gender minorities, social media, technology ecosystems},
	pages = {44},
}

@article{prentice1994asymmetries,
	title = {Asymmetries in attachments to groups and to their members: {Distinguishing} between common-identity and common-bond groups},
	volume = {20},
	number = {5},
	journal = {Personality and Social Psychology Bulletin},
	author = {Prentice, Deborah A and Miller, Dale T and Lightdale, Jenifer R},
	year = {1994},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {484--493},
}

@article{Warner2002,
	title = {Publics and counterpublics},
	volume = {14},
	issn = {15278018},
	doi = {10.1215/08992363-14-1-49},
	number = {1},
	journal = {Public Culture},
	author = {Warner, Michael},
	year = {2002},
	pages = {49--90},
}

@article{Tajfel1974,
	title = {Social identity and intergroup behaviour},
	volume = {13},
	number = {2},
	journal = {Social Science Information},
	author = {Tajfel, Henri},
	year = {1974},
	pages = {65--93},
}

@article{Byron2019,
	title = {“{Hey}, {I}'m {Having} {These} {Experiences}”: {Tumblr} {Use} and {Young} {People}'s {Queer} ({Dis})connections},
	volume = {13},
	journal = {International Journal of Communication},
	author = {Byron, Paul and Robards, Brady and Hanckel, Benjamin and Vivienne, Son and Churchill, Brendan},
	year = {2019},
	keywords = {community, disconnection, queer, social media, tumblr, young people},
	pages = {2239--2259},
}

@article{Pan2017,
	title = {Who {Do} {You} {Think} {You} {Are}? {Common} and {Differential} {Effects} of {Social} {Self}-{Identity} on {Social} {Media} {Usage}},
	volume = {34},
	issn = {1557928X},
	url = {http://dx.doi.org/10.1080/07421222.2017.1296747},
	doi = {10.1080/07421222.2017.1296747},
	abstract = {Intense competition requires that social media service providers execute two major business strategies: exploiting current functions while simultaneously exploring incremental innovation. Realization of these strategies is related to two types of member behavior: reinforced use and varied use. Drawing on identity theories, we examine the common and differential effects of two levels of social self-identity—relational identity and social identity—on reinforced and varied use and the moderating role of inertia on their effects on social media usage. Our results reveal that, although both identities have similar effects on usage behavior, users with higher social identities are more oriented toward variety seeking, while those with stronger relational identities are more oriented toward reinforcement. Inertia negatively moderates the impacts of social identity on social media use, but not the relationships between relational identity and social media use. The current research contributes to theory by decomposing social media usage into reinforced and varied use and reveals the common and differential influences of two levels of social self-identity on user behavior. Social media service providers should focus more on social identity to promote varied use and focus more on relational identity when they want to enhance reinforced use.},
	number = {1},
	journal = {Journal of Management Information Systems},
	author = {Pan, Zhao and Lu, Yaobin and Wang, Bin and Chau, Patrick Y.K.},
	year = {2017},
	note = {Publisher: Routledge},
	pages = {71--101},
}

@article{Yang2017,
	title = {Commitment of newcomers and old-timers to online health support communities},
	volume = {2017-May},
	doi = {10.1145/3025453.3026008},
	abstract = {For online communities to be successful, they must retain an adequate number of members who contribute to the community. The amount and type of communication members receive can play an important role in generating and sustaining members' commitment to it. However, the communication that members find valuable may change with their tenure in the community. This paper examines how the communication members receive in an health-support community influences their commitment and how this influence changes with their tenure in the community. Commitment was operationalized with three measures: self-reported attachment, continued participation in the community, and responding to others. Results show that receiving communication was generally associated with increased commitment across the three measures, with its impact increasing with members' tenure. However, the average amount of informational and emotional support members received per message was associated with decreased commitment. Results have implications for interventions to encourage members' commitment to their communities throughout their history in the community.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Yang, Diyi and Kraut, Robert and Levine, John M.},
	year = {2017},
	note = {ISBN: 9781450346559},
	keywords = {Commitment, Communication, Group socialization, Online health support communities, Social support},
	pages = {6363--6375},
}

@article{Tardini2005,
	title = {A {Semiotic} {Approach} to {Online} {Communities}: {Belonging}, {Interest} and {Identity} in {Websites}' and {Videogames}' {Communities}},
	url = {https://www.researchgate.net/publication/266218884},
	abstract = {Borrowing some concepts from the linguistic and semiotic traditions, the paper aims at understanding the nature of online (virtual) communities. First, it distinguishes two basic kinds of communities: communities where members just share something similar (paradigmatic communities) and communities where social relations are established through interactions (syntagmatic communities). On the basis of this distinction, the paper focuses on online communities and analyses the main critical issues concerning them, namely belonging, identity and interest. These issues are then discussed in the last part, where a semiotic approach to online communities is presented: interest is pivotal in order to define the semiosphere of a virtual community, i.e. the social space within the community where interactions are allowed and encouraged; belonging is a key factor for websites' communities, since it helps them becoming real syntagmatic communities; identity is as much important in websites as in videogames communities, since on the basis of the different identities considered, different play communities can be identified.},
	number = {March},
	journal = {International Conference e-Society},
	author = {Tardini, Stefano and Cantoni, Lorenzo},
	year = {2005},
	note = {ISBN: 972-8939-03-5},
	keywords = {Online communities, belonging, identity, interest, videogames, websites},
	pages = {371--378},
}

@article{Jones2006,
	title = {Online communities for teachers and lifelong learners: a framework for comparing similarities and identifying differences in communities of practice and communities of interest},
	volume = {2},
	issn = {1477-8386},
	doi = {10.1504/ijlt.2006.010615},
	abstract = {In recent years online and blended communities have become a popular topic among educationalists. In this paper we present a framework that supports the analysis, development and maintenance of online and blended communities. This is applied to two community case studies that differ along several key dimensions such as type of membership, the purpose of the communities, their policies and size. The analysis draws attention to the differences between the two types of communities. It also highlights the advantages and weaknesses of the framework with respect to these two case studies and suggests areas for future development. In the discussion that follows we highlight some key differences between this framework and Wenger's work on Communities of Practice (COPs).},
	number = {2/3},
	journal = {International Journal of Learning Technology},
	author = {Jones, Ann and Preece, Jenny},
	year = {2006},
	keywords = {2006, a, a framework for, and lifelong learners, and preece, blended communities, cois, communities of interest, communities of practice, cops, follows, informal learning, j, jones, online communities for teachers, reference to this paper, should be made as, sociability},
	pages = {112},
}

@article{Tausczik2014,
	title = {Building loyalty to online communities through bond and identity-based attachment to sub-groups},
	doi = {10.1145/2531602.2531688},
	abstract = {Researchers and theorists have proposed that feelings of attachment to subgroups within a larger online community or site can increase users' loyalty to the site. They have identified two types of attachment, with distinct causes and consequences. With bond-based attachment, people feel connections to other group members, while with identitybased attachment they feel connections to the group as a whole. In two experiments we show that these feelings of attachment to subgroups increase loyalty to the larger community. Communication with other people in a subgroup but not simple awareness of them increases attachment to the larger community. By varying how the communication is structured, between dyads or with all group members simultaneously, the experiments show that bond- and identity-based attachment have different causes. But the experiments show no evidence that bond and identity attachment have different consequences. We consider both theoretical and methodological reasons why the consequences of bond-based and identity-based attachment are so similar. Copyright © 2014 ACM.},
	journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
	author = {Tausczik, Yla R. and Dabbish, Laura A. and Kraut, Robert E.},
	year = {2014},
	note = {ISBN: 9781450325400},
	keywords = {Commitment, Computer-mediated communication, Group identity, Mechanical turk, Online communities, Social attachment},
	pages = {146--157},
}

@article{ren_applying_2007,
	title = {Applying common identity and bond theory to design of online communities},
	volume = {28},
	issn = {01708406},
	doi = {10.1177/0170840607076007},
	abstract = {Online communities depend upon the commitment and voluntary participation of their members. Community design - site navigation, community structure and features, and organizational policies - is critical in this regard. Community design affects how people can interact, the information they receive about one another and the community, and how they can participate in community activities. We argue that the constraints and opportunities inherent in online community design influence how people become attached to the community and whether they are willing to expend effort on its behalf. We examine two theories of group attachment and link these theories with design decisions for online communities. Common identity theory makes predictions about the causes and consequences of people's attachment to the group as a whole. Common bond theory makes predictions about the causes and consequences of people's attachment to individual group members. We review causes of common identity and common bond, and show how they result in different kinds of attachment and group outcomes. We then show how design decisions, such as those focused on recruiting newcomers versus retaining existing members, constraining or promoting off-topic discussion, and limiting group size or allowing uncontrolled growth, can lead to common identity or interpersonal bonds among community members, and consequently to different levels and forms of community participation by those so motivated. Copyright © 2007 SAGE Publications.},
	number = {3},
	journal = {Organization Studies},
	author = {Ren, Yuqing and Kraut, Robert and Kiesler, Sara},
	year = {2007},
	keywords = {Common bond, Common identity, Design, Member attachment, Online community},
	pages = {377--408},
}

@article{Sassenberg2002,
	title = {Common bond and common identity groups on the {Internet}: {Attachment} and normative behavior in on-topic and off-topic chats},
	volume = {6},
	issn = {10892699},
	doi = {10.1037/1089-2699.6.1.27},
	abstract = {The present research is based on D. A. Prentice, D. T. Miller, and J. R. Lightdale's (1994) distinction between common bond groups (formed by attachment between group members) and common identity groups (formed by attachment to the group as a whole). Study 1 showed the existence of both types of groups on the Internet: On-topic chats can be classified as common identity groups, and off-topic chats as common bond groups. In Study 2 the adherence to group norms as a behavioral consequence of the membership in both types of groups was analyzed. Members of common identity groups adhered more to the group norms of paralinguistic symbols than did members of common bond groups. The implications for the development and persistence of groups on the Internet are discussed.},
	number = {1},
	journal = {Group Dynamics},
	author = {Sassenberg, Kai},
	year = {2002},
	pages = {27--37},
}

@inproceedings{Seering2018,
	title = {Applications of {Social} {Identity} {Theory} to {Research} and {Design} in {Social} {Computing}},
	volume = {2},
	booktitle = {{CSCW}},
	author = {Seering, Joseph and Ng, Felicia and Yao, Zheng and Kaufman, Geoff},
	year = {2018},
	note = {Issue: January},
	pages = {1--33},
}

@inproceedings{10.1145/1935826.1935845,
	address = {New York, NY, USA},
	title = {Everyone's an {Influencer}: {Quantifying} {Influence} on {Twitter}},
	isbn = {978-1-4503-0493-1},
	url = {https://doi.org/10.1145/1935826.1935845},
	doi = {10.1145/1935826.1935845},
	abstract = {In this paper we investigate the attributes and relative influence of 1.6M Twitter users by tracking 74 million diffusion events that took place on the Twitter follower graph over a two month interval in 2009. Unsurprisingly, we find that the largest cascades tend to be generated by users who have been influential in the past and who have a large number of followers. We also find that URLs that were rated more interesting and/or elicited more positive feelings by workers on Mechanical Turk were more likely to spread. In spite of these intuitive results, however, we find that predictions of which particular user or URL will generate large cascades are relatively unreliable. We conclude, therefore, that word-of-mouth diffusion can only be harnessed reliably by targeting large numbers of potential influencers, thereby capturing average effects. Finally, we consider a family of hypothetical marketing strategies, defined by the relative cost of identifying versus compensating potential "influencers." We find that although under some circumstances, the most influential users are also the most cost-effective, under a wide range of plausible assumptions the most cost-effective performance can be realized using "ordinary influencers"---individuals who exert average or even less-than-average influence.},
	booktitle = {Proceedings of the {Fourth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Bakshy, Eytan and Hofman, Jake M and Mason, Winter A and Watts, Duncan J},
	year = {2011},
	note = {Series Title: WSDM '11},
	keywords = {communication networks, diffusion, influence, twitter, word of mouth marketing},
	pages = {65--74},
}

@article{white_human_2013,
	title = {Human supervision of automated systems and the implications of double loop learning},
	volume = {6},
	issn = {19355718},
	doi = {10.4018/jitsa.2013070102},
	abstract = {This paper describes the problem of human monitoring of automation. It considers the approaches involved in mental models and compares the ideas involved in double loop learning. The approaches are collected together with limited experimental experience to form a more complete model of the learning model involved in developing human control with proposed strategies for development. Copyright © 2013, IGI Global.},
	number = {2},
	journal = {International Journal of Information Technologies and Systems Approach},
	author = {White, A. S.},
	year = {2013},
	note = {ISBN: 9781450304931},
	keywords = {Automation, Double-loop learning, Human supervision of automated systems, Mental models, Supervisory control},
	pages = {13--21},
}

@article{goel_structural_2016,
	title = {The structural virality of online diffusion},
	volume = {62},
	issn = {15265501},
	doi = {10.1287/mnsc.2015.2158},
	abstract = {Viral products and ideas are intuitively understood to grow through a person-to-person diffusion process analogous to the spread of an infectious disease; however, until recently it has been prohibitively difficult to directly observe purportedly viral events, and thus to rigorously quantify or characterize their structural properties. Here we propose a formal measure of what we label "structural virality" that interpolates between two conceptual extremes: content that gains its popularity through a single, large broadcast and that which grows through multiple generations with any one individual directly responsible for only a fraction of the total adoption. We use this notion of structural virality to analyze a unique data set of a billion diffusion events on Twitter, including the propagation of news stories, videos, images, and petitions. We find that across all domains and all sizes of events, online diffusion is characterized by surprising structural diversity; that is, popular events regularly grow via both broadcast and viral mechanisms, as well as essentially all conceivable combinations of the two. Nevertheless, we find that structural virality is typically low, and remains so independent of size, suggesting that popularity is largely driven by the size of the largest broadcast. Finally, we attempt to replicate these findings with a model of contagion characterized by a low infection rate spreading on a scale-free network. We find that although several of our empirical findings are consistent with such a model, it fails to replicate the observed diversity of structural virality, thereby suggesting new directions for future modeling efforts.},
	number = {1},
	journal = {Management Science},
	author = {Goel, Sharad and Anderson, Ashton and Hofman, Jake and Watts, Duncan J.},
	year = {2016},
	keywords = {Diffusion, Twitter, Viral media},
	pages = {180--196},
}

@article{jackson_hijacking_2015,
	title = {Hijacking \#{myNYPD}: {Social} {Media} {Dissent} and {Networked} {Counterpublics}},
	volume = {65},
	issn = {14602466},
	doi = {10.1111/jcom.12185},
	abstract = {In this article we investigate the hijacking of the Twitter hashtag \#myNYPD following the launch of a public relations campaign by the New York City Police Department in April of 2014. Theorizing networked counterpublics, we examine how Twitter was used as a platform to generate and promote counterpublic narratives about racial profiling and police misconduct. Through a combination of large-scale network analysis and qualitative discourse analysis, we detail counterpublic structure and leadership, discursive strategies deployed by crowdsourced elites, and the reception of counterpublic activism in mainstream media. We conclude with implications for understanding the evolving nature of counterpublics, with particular consideration to the roles of new and old media in (re)shaping public debates around marginalization, profiling, and policing.},
	number = {6},
	journal = {Journal of Communication},
	author = {Jackson, Sarah J. and Foucault Welles, Brooke},
	year = {2015},
	keywords = {Discourse Analysis, Network Analysis, Networked Counterpublics, Policing, Social Media Activism},
	pages = {932--952},
}

@article{rathje_out-group_2021,
	title = {Out-group animosity drives engagement on social media},
	doi = {10.1073/pnas.2024292118/-/DCSupplemental.Published},
	author = {Rathje, Steve and Bavel, Jay J Van and Linden, Sander Van Der},
	year = {2021},
	pages = {1--9},
}

@article{Renninger2015,
	title = {“{Where} {I} can be myself … where {I} can speak my mind” : {Networked} counterpublics in a polymedia environment},
	volume = {17},
	issn = {1461-4448},
	url = {http://journals.sagepub.com/doi/10.1177/1461444814530095},
	doi = {10.1177/1461444814530095},
	abstract = {This article takes note of affordances for counterpublic communication on social networking sites (SNSs). Because of the important ways that counterpublic communication is tied to specific platforms, it is necessary to understand why certain platforms are especially conducive (or are seen to be conducive) to counterpublic address. This article uses the example of the asexual community's use of the SNS Tumblr to explore the affordances of the Tumblr platform for counterpublic communication, comparing Tumblr to the bulletin boards on the popular Asexuality Visibility Education Network website. This article modifies and extends boyd's analysis of SNSs as networked publics to account for the technological affordances for networked counterpublics. It ends by briefly considering ways that networked counterpublics can be antagonized.},
	number = {9},
	journal = {New Media \& Society},
	author = {Renninger, Bryce J},
	year = {2015},
	keywords = {2006, asexuality, benkler, communica-, counterpublics, it is widely acknowledged, of a networked public, papacharissi, polymedia environment, s possibility for decentralized, social networking sites, sphere, that the internet, tion affords the possibility, tumblr},
	pages = {1513--1529},
}

@article{cortes_communities_2001,
	title = {Communities of interest},
	volume = {2189},
	issn = {16113349},
	doi = {10.1007/3-540-44816-0_11},
	abstract = {We consider problems that can be characterized by large dynamic graphs. Communication networks provide the prototypical example of such problems where nodes in the graph are network IDs and the edges represent communication between pairs of network IDs. In such graphs, nodes and edges appear and disappear through time so that methods that apply to static graphs are not sufficient. We introduce a data structure that captures, in an approximate sense, the graph and its evolution through time. The data structure arises from a bottom-up representation of the large graph as the union of small subgraphs, called Communities of Interest (COI), centered on every node. These subgraphs are interesting in their own right and we discuss two applications in the area of telecommunications fraud detection to help motivate the ideas.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Cortes, Corinna and Pregibon, Daryl and Volinsky, Chris},
	year = {2001},
	note = {ISBN: 3540425810},
	pages = {105--114},
}

@article{fischer_communities_2001,
	title = {Communities of {Interest}: {Learning} through the {Interaction} of {Multiple} {Knowledge} {Systems}},
	volume = {1},
	abstract = {Complex design problems often cannot be solved by individuals or by homogenous groups. Communities of interest (CoIs) (defined by their collective concern with the resolution of a problem) bring together stakeholders from different communities of practice (CoP). Reaching a common understanding between these stakeholders is a major challenge due to the symmetry of ignorance caused by their respective cultures and their use of different knowledge systems. Our research has focused on the development of conceptual frameworks and innovative socio-technical environments to exploit the symmetry of ignorance as a source for social creativity among CoIs.},
	journal = {24th IRIS Conference},
	author = {Fischer, Gerhard},
	year = {2001},
	pages = {1--13},
}

@article{lee_higher-order_2018,
	title = {Higher-order coreference resolution with coarse-to-fine inference},
	volume = {2},
	doi = {10.18653/v1/n18-2108},
	abstract = {We introduce a fully differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.},
	journal = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
	author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
	year = {2018},
	note = {arXiv: 1804.05392
ISBN: 9781948087292},
	pages = {687--692},
}

@inproceedings{yoder_fanfictionnlp_2021,
	title = {{FanfictionNLP}: {A} {Text} {Processing} {Pipeline} for {Fanfiction}},
	copyright = {All rights reserved},
	booktitle = {Proceedings of the 3rd {Workshop} on {Narrative} {Understanding}},
	author = {Yoder, Michael Miller and Khosla, Sopan and Shen, Qinlan and Naik, Aakanksha and Jin, Huiming and Muralidharan, Hariharan and Rosé, Carolyn P},
	year = {2021},
	pages = {13--23},
}

@article{frekko_signs_2009,
	title = {Signs of {Respect} : {Neighborhood} ,},
	volume = {19},
	doi = {10.1111/j.1548-1395.2009.01032.x.I},
	number = {2},
	journal = {Journal of Linguistic Anthropology},
	author = {Frekko, Susan E},
	year = {2009},
	pages = {227--245},
}

@article{paulston_language_1997,
	title = {Language policies and language rights},
	volume = {26},
	issn = {00846570},
	doi = {10.1146/annurev.anthro.26.1.73},
	abstract = {This review is an overview of the newly developing field of language rights. It distinguishes between (a) historical/descriptive studies where language rights are treated as the resultant variable with no attempt to predict consequences, and (b) exhortatory and ideologically based studies in which language rights are considered a causal variable. An attempt at definitions follows, set within the field of language planning. Principal concerns, such as territoriality versus personality principles and individual versus collective rights, are discussed. The review ends with an argument to consider language rights as emic rights, which is to say culture-language-context-specific rights, rather than to consider linguistic human rights from a universal rights perspective which overstates issues and masks rights to as also being rights against. We need a careful exploration of the nature of language rights and their consequences.},
	journal = {Annual Review of Anthropology},
	author = {Paulston, Christina Bratt},
	year = {1997},
	keywords = {Individual vs collective rights, Linguistic human rights, Linguistic minorities, Principles of territoriality vs personality},
	pages = {73--85},
}

@article{alabdulkarim_automatic_2021,
	title = {Automatic {Story} {Generation}: {Challenges} and {Attempts}},
	url = {http://arxiv.org/abs/2102.12634},
	abstract = {The scope of this survey paper is to explore the challenges in automatic story generation. We hope to contribute in the following ways: 1. Explore how previous research in story generation addressed those challenges. 2. Discuss future research directions and new technologies that may aid more advancements. 3. Shed light on emerging and often overlooked challenges such as creativity and discourse.},
	author = {Alabdulkarim, Amal and Li, Siyan and Peng, Xiangyu},
	year = {2021},
	note = {arXiv: 2102.12634},
	pages = {72--83},
}

@article{kingdom_for_2020,
	title = {For {Chenoa}, {John}, and {Ariel} ' ""'},
	author = {Kingdom, United and Plaza, One Liberty and Melbourne, Port and Forum, Splendor and Centre, Jasola District},
	year = {2020},
	note = {ISBN: 9781316607312},
}

@book{Guinote2010,
	title = {The {Social} {Psychology} of {Power}},
	publisher = {The Guilford Press},
	editor = {Guinote, Ana and Vescio, Theresa},
	year = {2010},
}

@book{reddy_text_2021,
	title = {Text {Classification} for {AI} {Education}},
	volume = {1},
	abstract = {In recent years, Artificial Intelligence (AI) has become increasingly prevalent in our lives. Because of this, individuals of all ages need to be aware of how AI works. To introduce middle school students to AI concepts, we built a text classifier extension into a block-based programming interface that allows students to train custom machine learning models. To make the extension more accessible, a translator was incorporated where the language of each input is automatically detected and translated to English. Our classifier's accuracy was comparable to similar classifiers such as Machine Learning for Kids' text classifier and Uclassify's text classifier, and its effectiveness was tested against these classifiers with two test datasets. We piloted the classifier with middle school students in an online AI course. The students first learned the concepts behind the classifier which consisted of word embeddings, K-Nearest-Neighbors, and classification bias. They were then able to use the text classifier to create their own projects. Some of the projects were a snake identifier, a TV show suggester, a chat robot, and a healthcare robot. With this extension, students were able to engage in project-based learning to become more knowledgeable about the ever-growing field of AI and raise their awareness about applications of AI within their own lives.},
	publisher = {Association for Computing Machinery},
	author = {Reddy, Tejal and Williams, Randi and Breazeal, Cynthia},
	year = {2021},
	doi = {10.1145/3408877.3439689},
	note = {Publication Title: SIGCSE '21: ACM Special Interest Group on Computer Science Education, March 2021, Toronto, ON
Issue: 1},
	keywords = {2021, AI education, text classification, text tagging, m, acm reference format, ai education, and cynthia breazeal, machine learning, randi williams, tejal reddy, text classification, text tagging},
}

@article{Tiidenberg2016,
	title = {Boundaries and conflict in a {NSFW} community on tumblr: {The} meanings and uses of selfies},
	volume = {18},
	issn = {1461-4448},
	url = {http://journals.sagepub.com/doi/10.1177/1461444814567984},
	doi = {10.1177/1461444814567984},
	abstract = {This article is an exploration of what selfies and other images are and do in Not Safe For Work (NSFW) communities on tumblr.com. By analyzing ethnographic and interview data, images and blog outtakes, this article spotlights four kinds of conflicts that arise around how selfies and images are used. These are about: (a) reactions to photo-shopped images, (b) altering other people's selfies and/or reposting them as your own, (c) misunderstandings from separating text from image (caption-stripping), (d) disrespecting the self-shooters' way of curating their blogs. Boundary theory as well as concepts of social afterlife of content and assumed trust are used to illuminate that images, including selfies, have significant, yet different meanings to different people and play an important part in creating and maintaining meaningful relationships and communities.},
	number = {8},
	journal = {New Media \& Society},
	author = {Tiidenberg, Katrin},
	year = {2016},
	keywords = {boundaries, conflict, online community, photography, selfie, social afterlife of images, trust, trust responsiveness, tumblr, visual narrative analysis},
	pages = {1563--1578},
}

@article{raza_viral_2012,
	title = {Viral entertainment as a vehicle for disseminating speech-based services to low-literate users},
	doi = {10.1145/2160673.2160715},
	abstract = {Entertainment has recently been shown to be a powerful motivator for mastering new technologies. We therefore set out to use viral entertainment to introduce telephone-based, speech-based services to low-literate people in developing countries. We describe Polly, a simple voice manipulation and forwarding system that went viral in Pakistan last year. Seeded once by 32 low-skilled office workers in a Pakistani university, in 3 weeks Polly amassed 2,032 users and 10,629 interactions. From analyzing the traffic and its content, it is evident that Polly has been used extensively for entertainment and social contact, but it has also been put to an unintended use as a voicemail and group messaging facility. This demonstrated the potential for speech based services, and the pent-up demand for entertainment, among our target population. Also of note, Polly's viral spread crossed gender and age boundaries and even established itself in a female population. However, it appears to have not crossed socioeconomic boundaries. Copyright 2012 ACM.},
	journal = {ACM International Conference Proceeding Series},
	author = {Raza, Agha Ali and Pervaiz, Mansoor and Milo, Christina and Razaq, Samia and Alster, Guy and Sherwani, Jahanzeb and Saif, Umar and Rosenfeld, Roni},
	year = {2012},
	note = {ISBN: 9781450310451},
	keywords = {Cell phone, Communication services, Entertainment, Low literate, Speech interfaces, Telephone, Viral},
	pages = {350--358},
}

@article{ruane_conversational_2019,
	title = {Conversational {AI}: {Social} and ethical considerations},
	volume = {2563},
	issn = {16130073},
	abstract = {Conversational Agents are becoming ubiquitous in our daily lives. They are used in various areas including customer service, education, medicine, and entertainment. As tools that are increasingly permeating various social domains, Conversational Agents can have a direct impact on individual's lives and on social discourse in general. Consequently, critical evaluation of this impact is imperative. In this paper, we highlight some emerging ethical issues and suggest ways for agent designers, developers, and owners to approach them with the goal of responsible development of Conversational Agents.},
	journal = {CEUR Workshop Proceedings},
	author = {Ruane, Elayne and Birhane, Abeba and Ventresque, Anthony},
	year = {2019},
	keywords = {Conversational Agent, Ethics, Intelligent Systems, Social Impact},
	pages = {104--115},
}

@inproceedings{Yoder2020,
	title = {Phans, {Stans} and {Cishets}: {Self}-{Presentation} {Effects} on {Content} {Propagation} in {Tumblr}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-7989-2},
	doi = {10.1145/3394231.3397893},
	booktitle = {12th {ACM} {Conference} on {Web} {Science} ({WebSci} ’20)},
	author = {Yoder, Michael Miller and Shen, Qinlan and Wang, Yansen and Coda, Alex and Jang, Yunseok and Song, Yale and Thadani, Kapil and Rosé, Carolyn P.},
	year = {2020},
	keywords = {all or part of, classroom use is granted, content propagation, copies are not made, identity, identity labels, or, or distributed, or hard copies of, permission to make digital, social media, this work for personal, tumblr, without fee provided that},
	pages = {39--48},
}

@article{hillman_tumblr_2014,
	title = {Tumblr fandoms, community \& culture},
	url = {http://dl.acm.org/citation.cfm?doid=2556420.2557634},
	doi = {10.1145/2556420.2557634},
	abstract = {A growing trend is the participation in online fandom communities through the support of the blogging platform Tumblr. We investigated Tumblr fandom users’ motivations behind participating in fandoms, and how they interacted within the Tumblr community. Our results show that fandom users feel their Tumblr experience is ‘always-on’ where they participate at nearly any point in the day. They have also adopted a unique set of jargon and use of animated GIFs to match their desired fandom activities. Overall, our results show that Tumblr fandom users present a unique culture, much different from other social networking sites.},
	number = {February 2014},
	journal = {Proceedings of the companion publication of the 17th ACM conference on Computer supported cooperative work \& social computing - CSCW Companion '14},
	author = {Hillman, Serena},
	year = {2014},
	note = {ISBN: 9781450325417},
	keywords = {ACM, culture, fandoms, fanfiction, social networking, tumblr},
	pages = {285--288},
}

@article{dym_ethical_2020,
	title = {Ethical and privacy considerations for research using online fandom data},
	url = {https://journal.transformativeworks.org/index.php/twc/article/view/1733},
	number = {33},
	journal = {Transformative Works \& Cultures},
	author = {Dym, Brianna and Fiesler, Casey},
	year = {2020},
}

@article{hammer_lab_2020,
	title = {Lab counterculture},
	doi = {10.1145/3334480.3381824},
	abstract = {While academic research culture varies across schools, disciplines, and individual labs, the material and mental well-being of both graduate students and faculty are often negatively impacted by systemic factors in academia. Here we unpack these patterns in order to counter the narrative that individualistic solutions can bring about change. We illustrate how focus on quantitative outcomes, perfectionism, competition, time scarcity, power dynamics, bias towards maintaining the status quo, and financial stress contribute to negative lab culture. We describe specific, concrete, and actionable practices we institute in our lab to counter these systemic factors. We end by opening the conversation to other researchers to examine and counter toxic lab culture to promote supportive, inclusive, and ethical research.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Hammer, Jessica and To, Alexandra and Principe Cruz, Erica},
	year = {2020},
	note = {ISBN: 9781450368193},
	keywords = {Academic research, Counterculture, Lab culture},
	pages = {1--14},
}

@article{dovchin_language_2015,
	title = {Language, multiple authenticities and social media: {The} online language practices of university students in {Mongolia}},
	volume = {19},
	issn = {14679841},
	doi = {10.1111/josl.12134},
	abstract = {This paper seeks to contribute to the current discussion of the sociolinguistics of globalization by revealing youth linguistic diversity from the perspective of the online mixed language practices of university students in contemporary post-socialist Mongolia. Drawing on sets of Facebook data, the paper firstly argues that the online mixed youth language practices should be understood as 'translingual' not only due to their varied recombination of linguistic and cultural resources, genres, modes, styles and repertories, but also due to their direct subtextual connections with wider socio-cultural, historical and ideological meanings. Secondly, online users metalinguistically claim authenticity in terms of their own translingual practices as opposed to other colliding language ideologies such as linguistic dystopia. How they relocalize the notion of authenticity, however, differs profoundly depending on their own often-diverse criteria, identities, beliefs and ideas. This shows that, with mixing and recombining at its very core, the translingual practices of modern young speakers provide us with a significant insight into the co-existence of multiple authenticities and origins of authenticity in an increasingly interconnected world.},
	number = {4},
	journal = {Journal of Sociolinguistics},
	author = {Dovchin, Sender},
	year = {2015},
	keywords = {Facebook, Language ideologies, Mongolia, sociolinguistics of globalization, Sociolinguistic authenticity, Translingualism, Youth identities},
	pages = {437--459},
}

@article{garvey_sentiment_2020,
	title = {Sentiment {Analysis} of the {News} {Media} on {Artificial} {Intelligence} {Does} {Not} {Support} {Claims} of {Negative} {Bias} {Against} {Artificial} {Intelligence}},
	volume = {24},
	issn = {15578100},
	doi = {10.1089/omi.2019.0078},
	abstract = {Artificial intelligence (AI) is a hot topic in digital health, as automated systems are being adopted throughout the health care system. Because they are still flexible, emerging technologies can be shaped significantly by media representations as well as public engagement with science. In this context, we examine the belief that negative news media coverage of AI - and specifically, the alleged use of imagery from the movie Terminator - is to blame for public concerns about AI. This belief is identified as a potential barrier to meaningful engagement of AI scientists and technology developers with journalists and the broader public. We name this climate of risk perception the "Terminator Syndrome" - not because of its origins in the movie of the same name per se, but because such unchecked beliefs can terminate broad public engagement on AI before they even begin. Using both quantitative and qualitative approaches, this study examined the hypothesis that the news media coverage of AI is negative. We conducted a sentiment analysis of news data spanning over six decades, from 1956 to 2018, using the Google Cloud Natural Language API Sentiment Analysis tool. Contrary to the alleged negative sentiment in news media coverage of AI, we found that the available evidence does not support this claim. We conclude with an innovation policy-relevant discussion on the current state of AI risk perceptions, and what critical social sciences offer for responsible AI innovation in digital health, life sciences, and society.},
	number = {5},
	journal = {OMICS A Journal of Integrative Biology},
	author = {Garvey, Colin and Maskal, Chandler},
	year = {2020},
	pmid = {31313979},
	keywords = {AI and risk, Artificial intelligence (AI), Digital health, Public engagement, Risk governance, Technology policy},
	pages = {286--299},
}

@article{chuan_framing_2019,
	title = {Framing artificial intelligence in {American} newspapers},
	doi = {10.1145/3306618.3314285},
	abstract = {Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.},
	journal = {AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Chuan, Ching Hua and Tsai, Wan Hsiu Sunny and Cho, Su Yeon},
	year = {2019},
	note = {ISBN: 9781450363242},
	keywords = {Artiicial intelligence, Content analysis, News framing, Public perception},
	pages = {339--344},
}

@article{carley_social_nodate,
	title = {Social {Cyber}-{Security}},
	author = {Carley, Kathleen M and Cervone, Guido},
	note = {ISBN: 0000000256124},
	keywords = {network science, social cyber-security, social media analytics},
	pages = {1--6},
}

@article{mcintosh2004white,
	title = {White privilege: {Unpacking} the invisible knapsack},
	volume = {6},
	journal = {Race, class, and gender in the United States},
	author = {McIntosh, Peggy},
	year = {2004},
	note = {Publisher: Worth Publishers New York},
	pages = {188--192},
}

@article{raji_you_2021,
	title = {"{You} can't sit with us": {Exclusionary} pedagogy in {AI} ethics education},
	doi = {10.1145/3442188.3445914},
	abstract = {Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS) - -and its implications for AI - -has led to the current "ethics crisis". However, we claim that the current AI ethics education space relies on a form of "exclusionary pedagogy,"where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as "ethical unicorns"that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.},
	journal = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	author = {Raji, Inioluwa Deborah and Scheuerman, Morgan Klaus and Amironesei, Razvan},
	year = {2021},
	note = {ISBN: 9781450383097},
	pages = {515--525},
}

@article{fast_long-term_2017,
	title = {Long-term trends in the public perception of artificial intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10635/10494},
	abstract = {Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.},
	number = {January 1986},
	journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
	author = {Fast, Ethan and Horvitz, Eric},
	year = {2017},
	note = {arXiv: 1609.04904},
	keywords = {Human Computation and Crowd Sourcing},
	pages = {963--969},
}

@article{algee-hewitt_representing_2020,
	title = {Representing {Race} and {Ethnicity} in {American} {Fiction}, 1789-1920},
	volume = {12},
	issn = {2371-4549},
	doi = {10.22148/001c.18509},
	journal = {Journal of Cultural Analytics},
	author = {Algee-Hewitt, Mark and Porter, J.D. and Walser, Hannah},
	year = {2020},
	pages = {28--60},
}

@incollection{brown_appendix_2018,
	title = {appendix a {Corpus} {Composition}},
	booktitle = {English and {Empire}},
	author = {Brown, David West},
	year = {2018},
	pages = {262--267},
}

@incollection{brown_corpus_2018,
	title = {Corpus {Design}},
	abstract = {This paper describes the approach to spoken corpus design used by the British National Corpus (BNC) project.1 A twopart approach to spoken corpus design has been adopted. The demographic approach uses demographic parameters to sample the everyday speech of the population of British English speakers in the United Kingdom. The context governed approach is designed to cover the full range of linguistic variation found in spoken language using a typology based on four contextual categories: educational, business, public/institutional, and leisure. Details of the processing of recordings are given, together with a description of the context features included in the corpus (such as setting, location, topic, and participant details). The processing and transcription of BNC spoken recordings will be discussed in a forthcoming paper},
	booktitle = {English and {Empire}},
	author = {Brown, David West},
	year = {2018},
	doi = {10.1017/9781108551045.003},
	pages = {31--52},
}

@article{severyn_learning_2015,
	title = {Learning to rank short text pairs with convolutional deep neural networks},
	doi = {10.1145/2766462.2767738},
	abstract = {Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering - question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3\% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.},
	journal = {SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	author = {Severyn, Aliaksei and Moschittiy, Alessandro},
	year = {2015},
	note = {ISBN: 9781450336215},
	keywords = {Convolutional neural networks, Learning to rank, Microblog search, Question answering},
	pages = {373--382},
}

@article{bowman_large_2015,
	title = {A large annotated corpus for learning natural language inference},
	doi = {10.18653/v1/d15-1075},
	abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
	number = {September},
	journal = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	year = {2015},
	note = {arXiv: 1508.05326
ISBN: 9781941643327},
	pages = {632--642},
}

@article{Parikh2016,
	title = {A {Decomposable} {Attention} {Model} for {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/1606.01933},
	doi = {10.18653/v1/N16-1062},
	abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
	journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)},
	author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	year = {2016},
	note = {arXiv: 1606.01933
ISBN: 9781941643914},
	pages = {2249--2255},
}

@article{hannak_bias_2017,
	title = {Bias in {Online} freelance marketplaces: {Evidence} from {TaskRabbit} and {Fiverr}},
	doi = {10.1145/2998181.2998327},
	abstract = {Online freelancing marketplaces have grown quickly in recent years. In theory, these sites offer workers the ability to earn money without the obligations and potential social biases associated with traditional employment frameworks. In this paper, we study whether two prominent online freelance marketplaces-TaskRabbit and Fiverr-are impacted by racial and gender bias. From these two platforms, we collect 13,500 worker profiles and gather information about workers' gender, race, customer reviews, ratings, and positions in search rankings. In both marketplaces, we find evidence of bias: we find that perceived gender and race are significantly correlated with worker evaluations, which could harm the employment opportunities afforded to the workers. We hope that our study fuels more research on the presence and implications of discrimination in online environments.},
	journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
	author = {Hannák, Anikó and Mislove, Alan and Wagner, Claudia and Strohmaier, Markus and Garcia, David and Wilson, Christo},
	year = {2017},
	note = {ISBN: 9781450343350},
	keywords = {Discrimination, Gig economy, Information retrieval, Linguistic analysis},
	pages = {1914--1933},
}

@article{chen_investigating_2018,
	title = {Investigating the impact of gender on rank in resume search engines},
	volume = {2018-April},
	doi = {10.1145/3173574.3174225},
	abstract = {In this work we investigate gender-based inequalities in the context of resume search engines, which are tools that allow recruiters to proactively search for candidates based on keywords and filters. If these ranking algorithms take demographic features into account (directly or indirectly), they may produce rankings that disadvantage some candidates. We collect search results from Indeed, Monster, and CareerBuilder based on 35 job titles in 20 U. S. cities, resulting in data on 855K job candidates. Using statistical tests, we examine whether these search engines produce rankings that exhibit two types of indirect discrimination: individual and group unfairness. Furthermore, we use controlled experiments to show that these websites do not use inferred gender of candidates as explicit features in their ranking algorithms.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Chen, Le and Ma, Ruijun and Hannák, Anikó and Wilson, Christo},
	year = {2018},
	note = {ISBN: 9781450356206},
	keywords = {Algorithm auditing, Discrimination, Information retrieval},
}

@article{cowgill_bias_2019,
	title = {Bias and {Productivity} in {Humans} and {Machines}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3433737},
	abstract = {Where should better learning technology (such as machine learning or AI) improve deci-sions? I develop a model of decision-making in which better learning technology is complementary with experimentation. Noisy, inconsistent decision-making introduces quasi-experimental variation into training datasets, which complements learning. The model makes heterogeneous predictions about when machine learning algorithms can improve human biases. These algorithms will can remove human biases exhibited in historical training data, but only if the human training decisions are sufficiently noisy; otherwise the algorithms will codify or exacerbate existing biases. The minimum amount of noise necessary for machine learning to reduce bias is a decreasing function of the amount of bias (high bias requires little noise, low bias requires lots of noise). The theoretical conditions necessary to completely eliminate bias are extreme and unlikely to appear in real datasets. The model provides theoretical microfoundations for why learning from biased historical datasets may lead to a decrease (if not a full elimination) of bias, as has been documented in several empirical settings. We discuss implications for regulation, labor markets and business strategy.},
	journal = {SSRN Electronic Journal},
	author = {Cowgill, Bo},
	year = {2019},
	pages = {1--30},
}

@article{deshpande_mitigating_2020,
	title = {Mitigating {Demographic} {Bias} in {AI}-based {Resume} {Filtering}},
	doi = {10.1145/3386392.3399569},
	abstract = {With increasing diversity in the labor market as well as the work force, employers receive resumes from an increasingly diverse population. However, studies and field experiments have confirmed the presence of bias in the labor market based on gender, race, and ethnicity. Many employers use automated resume screening to filter the many possible matches. Depending on how the automated screening algorithm is trained it can potentially exhibit bias towards a particular population by favoring certain socio-linguistic characteristics. The resume writing style and socio-linguistics are a potential source of bias as they correlate with protected characteristics such as ethnicity. A biased dataset is often translated into biased AI algorithms and de-biasing algorithms are being contemplated. In this work, we study the effects of socio-linguistic bias on resume to job description matching algorithms. We develop a simple technique, called fair-tf-idf, to match resumes with job descriptions in a fair way by mitigating the socio-linguistic bias.},
	number = {FairUMAP},
	journal = {UMAP 2020 Adjunct - Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
	author = {Deshpande, Ketki V. and Pan, Shimei and Foulds, James R.},
	year = {2020},
	note = {ISBN: 9781450367110},
	keywords = {TF-IDF, fair machine learning, job recommendation, term weighting},
	pages = {268--275},
}

@article{york_exploring_2012,
	title = {Exploring the {Effect} of {Author} and {Reader} {Identity} in {Online} {Story} {Writing}: the {STORIESINTHEWILD} {Corpus}},
	number = {January},
	journal = {Professionalism Study},
	author = {York, Pennsylvania},
	year = {2012},
}

@article{cao_toward_2020,
	title = {Toward {Gender}-{Inclusive} {Coreference} {Resolution}},
	doi = {10.18653/v1/2020.acl-main.418},
	abstract = {Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that reinforce cis-normativity and can harm binary and non-binary trans (and cis) stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and investigate where in the machine learning pipeline such biases can enter a system. We inspect many existing datasets for trans-exclusionary biases, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we will build systems that fail for: quality of service, stereotyping, and over- or under-representation.},
	author = {Cao, Yang Trista and Daumé III, Hal},
	year = {2020},
	note = {arXiv: 1910.13913},
	pages = {4568--4595},
}

@article{david_critical_2020,
	title = {Critical {Race} {Theory}},
	doi = {10.4135/9781529714395.n129},
	journal = {The SAGE Encyclopedia of Higher Education},
	author = {David, Miriam E. and Amey, Marilyn J.},
	year = {2020},
	note = {ISBN: 9781450367080},
	keywords = {critical race theory, race, racism, storytelling, theory},
	pages = {1--16},
}

@article{bayley_evidence_2003,
	title = {Evidence for the production and use of opaque red glass in {Roman} {Britain}},
	doi = {10.1145/1557019.1557077},
	abstract = {Tracking new topics, ideas, and “memes” across theWeb has been an issue of considerable interest. Recent work has developed meth- ods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days—the time scale at which we perceive news and events. We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we iden- tify a broad class of memes that exhibit wide spread and rich vari- ation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent repre- sentation of the news cycle—the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a “heartbeat”-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits.},
	journal = {Annales du 15e Conrès de l'Association Internationale pur l'Histoire du Verre},
	author = {Bayley, J},
	year = {2003},
	note = {ISBN: 9781605584959},
	keywords = {16, 17, 30, 31, 5, 7, abilistic term mixtures have, at the, been successful at identifying, blogs, formation cascades, in-, information diffusion, long-, meme-tracking, news cycle, news media, range trends in general, social networks, topics over time},
	pages = {45--48},
}

@article{Fiesler2018,
	title = {“{Participant}” {Perceptions} of {Twitter} {Research} {Ethics}},
	volume = {4},
	issn = {20563051},
	doi = {10.1177/2056305118763366},
	abstract = {Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers. However, the availability of public social media data has also presented ethical challenges. As the research community works to create ethical norms, we should be considering users’ concerns as well. With this in mind, we report on an exploratory survey of Twitter users’ perceptions of the use of tweets in research. Within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. However, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. The findings of this study point to potential best practices for researchers conducting observation and analysis of public data.},
	number = {1},
	journal = {Social Media and Society},
	author = {Fiesler, Casey and Proferes, Nicholas},
	year = {2018},
	keywords = {Internet research ethics, Twitter, social media, user studies},
}

@article{fiesler_ethical_2019,
	title = {Ethical considerations for research involving ({Speculative}) public data},
	volume = {3},
	issn = {25730142},
	doi = {10.1145/3370271},
	abstract = {As the process of creating and sharing data about ourselves becomes more prevalent, researchers have access to increasingly rich data about human behavior. Framed as a fictional paper published at some point in the not-so-distant future, this design fiction draws from current inquiry and debate into the ethics of using public data for research, and speculatively extends this conversation into even more robust and more personal data that could exist when we design new technologies in the future. By looking to how the precedents of today might impact the practices of tomorrow, we can consider how we might design policies, ethical guidelines, and technologies that are forward-thinking.},
	number = {GROUP},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Fiesler, Casey},
	year = {2019},
	keywords = {Design fiction, Ethics, Lifelogging, Privacy, Public data, Quantified self, Research ethics, Research methods, Social computing},
}

@article{davidson_racial_2019,
	title = {Racial bias in hate speech and abusive language detection datasets},
	issn = {23318422},
	doi = {10.18653/v1/w19-3504},
	abstract = {Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.},
	number = {2018},
	journal = {arXiv},
	author = {Davidson, Thomas and Bhattacharya, Debasmita and Weber, Ingmar},
	year = {2019},
	note = {arXiv: 1905.12516},
	pages = {25--35},
}

@article{Ananya2019,
	title = {Genderquant: {Quantifying} mention-level genderedness},
	volume = {1},
	abstract = {Language is gendered if the context surrounding a mention is suggestive of a particular binary gender for that mention. Detecting the different ways in which language is gendered is an important task since gendered language can bias NLP models (such as for coreference resolution). This task is challenging since genderedness is often expressed in subtle ways. Existing approaches need considerable annotation efforts for each language, domain, and author, and often require handcrafted lexicons and features. Additionally, these approaches do not provide a quantifiable measure of how gendered the text is, nor are they applicable at the fine-grained mention level. In this paper, we use existing NLP pipelines to automatically annotate gender of mentions in the text. On corpora labeled using this method, we train a supervised classifier to predict the gender of any mention from its context and evaluate it on unseen text. The model confidence for a mention's gender can be used as a proxy to indicate the level of genderedness of the context. We test this gendered language detector on movie summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC of up to 0.71, and observe that the model predictions agree with human judgments collected for this task. We also provide examples of detected gendered sentences from aforementioned domains.},
	journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {{Ananya} and Parthasarthi, Nitya and Singh, Sameer},
	year = {2019},
	note = {ISBN: 9781950737130},
	pages = {2959--2969},
}

@article{Chang2020,
	title = {Automatically inferring gender associations from language},
	doi = {10.18653/v1/d19-1579},
	abstract = {In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings - one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.},
	journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
	author = {Chang, Serina and McKeown, Kathleen},
	year = {2019},
	note = {arXiv: 1909.00091
ISBN: 9781950737901},
	pages = {5746--5752},
}

@inproceedings{cao_toward_2020-1,
	title = {Toward {Gender}-{Inclusive} {Coreference} {Resolution}},
	author = {Cao, Yang Trista and Daume, Hal},
	year = {2020},
}

@article{rosa_automatic_2019,
	title = {Automatic cyberbullying detection: {A} systematic review},
	volume = {93},
	issn = {07475632},
	doi = {10.1016/j.chb.2018.12.021},
	abstract = {Automatic cyberbullying detection is a task of growing interest, particularly in the Natural Language Processing and Machine Learning communities. Not only is it challenging, but it is also a relevant need given how social networks have become a vital part of individuals' lives and how dire the consequences of cyberbullying can be, especially among adolescents. In this work, we conduct an in-depth analysis of 22 studies on automatic cyberbullying detection, complemented by an experiment to validate current practices through the analysis of two datasets. Results indicated that cyberbullying is often misrepresented in the literature, leading to inaccurate systems that would have little real-world application. Criteria concerning cyberbullying definitions and other methodological concerns seem to be often dismissed. Additionally, there is no uniformity regarding the methodology to evaluate said systems and the natural imbalance of datasets remains an issue. This paper aims to direct future research on the subject towards a viewpoint that is more coherent with the definition and representation of the phenomenon, so that future systems can have a practical and impactful application. Recommendations on future works are also made.},
	number = {December 2018},
	journal = {Computers in Human Behavior},
	author = {Rosa, H. and Pereira, N. and Ribeiro, R. and Ferreira, P. C. and Carvalho, J. P. and Oliveira, S. and Coheur, L. and Paulino, P. and Veiga Simão, A. M. and Trancoso, I.},
	year = {2019},
	keywords = {Abusive language, Automatic cyberbullying detection, Cyberbullying, Machine learning, Natural language processing, Social networks},
	pages = {333--345},
}

@article{mozafari_hate_2020,
	title = {Hate speech detection and racial bias mitigation in social media based on {BERT} model},
	volume = {15},
	issn = {19326203},
	doi = {10.1371/journal.pone.0237861},
	abstract = {Disparate biases associated with datasets and trained classifiers in hateful and abusive content identification tasks have raised many concerns recently. Although the problem of biased datasets on abusive language detection has been addressed more frequently, biases arising from trained classifiers have not yet been a matter of concern. In this paper, we first introduce a transfer learning approach for hate speech detection based on an existing pre-trained language model called BERT (Bidirectional Encoder Representations from Transformers) and evaluate the proposed model on two publicly available datasets that have been annotated for racism, sexism, hate or offensive content on Twitter. Next, we introduce a bias alleviation mechanism to mitigate the effect of bias in training set during the fine-tuning of our pre-trained BERT-based model for hate speech detection. Toward that end, we use an existing regularization method to reweight input samples, thereby decreasing the effects of high correlated training set’ s n-grams with class labels, and then fine-tune our pre-trained BERT-based model with the new re-weighted samples. To evaluate our bias alleviation mechanism, we employed a cross-domain approach in which we use the trained classifiers on the aforementioned datasets to predict the labels of two new datasets from Twitter, AAE-aligned and White-aligned groups, which indicate tweets written in African-American English (AAE) and Standard American English (SAE), respectively. The results show the existence of systematic racial bias in trained classifiers, as they tend to assign tweets written in AAE from AAE-aligned group to negative classes such as racism, sexism, hate, and offensive more often than tweets written in SAE from White-aligned group. However, the racial bias in our classifiers reduces significantly after our bias alleviation mechanism is incorporated. This work could institute the first step towards debiasing hate speech and abusive language detection systems.},
	number = {8 August},
	journal = {PLoS ONE},
	author = {Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noël},
	year = {2020},
	pmid = {32853205},
	note = {ISBN: 1111111111},
	pages = {1--26},
}

@article{xia_demoting_2020,
	title = {Demoting {Racial} {Bias} in {Hate} {Speech} {Detection}},
	issn = {23318422},
	doi = {10.18653/v1/2020.socialnlp-1.2},
	abstract = {In current hate speech datasets, there exists a high correlation between annotators’ perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.},
	journal = {arXiv},
	author = {Xia, Mengzhou and Field, Anjalie and Tsvetkov, Yulia},
	year = {2020},
	note = {arXiv: 2005.12246},
	pages = {7--14},
}

@article{zhou_challenges_2021,
	title = {Challenges in {Automated} {Debiasing} for {Toxic} {Language} {Detection}},
	url = {http://arxiv.org/abs/2102.00086},
	abstract = {Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.},
	author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
	year = {2021},
	note = {arXiv: 2102.00086},
}

@article{smith_new_2017,
	title = {The {New} {Principle}-policy {Gap}: {How} {Diversity} {Ideology} {Subverts} {Diversity} {Initiatives}},
	volume = {60},
	issn = {15338673},
	doi = {10.1177/0731121417719693},
	abstract = {Colorblind ideology is a dominant mode of thinking about race matters in the United States, but it is not the only racial ideology that operates today. The United States appears to be shifting toward becoming more race conscious. We add to the critical diversity studies literature, and argue that even though we see a greater appreciation for the presence of nonwhite bodies in various spaces, we are not likely to see real systemic change in the American racial hierarchy because of a reliance on diversity ideology. Through an analysis of semistructured interviews with 43 white Millennials, this article outlines the ways in which diversity ideology's four tenets - diversity as acceptance, commodity, intent, and liability - help whites maintain power in multiracial spaces. This article pinpoints how whites employ these tenets to subvert policy efforts that aim to incorporate people of color into predominately white institutions, introducing a new principle-policy gap for the twenty-first century.},
	number = {5},
	journal = {Sociological Perspectives},
	author = {Smith, Candis Watts and Mayorga-Gallo, Sarah},
	year = {2017},
	keywords = {colorblind, diversity, millennial generation, racial ideology, whiteness},
	pages = {889--911},
}

@article{Green2004,
	title = {Understanding media enjoyment: {The} role of transportation into narrative worlds},
	volume = {14},
	issn = {10503293},
	doi = {10.1111/j.1468-2885.2004.tb00317.x},
	abstract = {"Transportation into a narrative world" is an experience of cognitive, emotional, and imagery involvement in a narrative. Transportation theory (Green \& Brock, 2000, 2002) provides a lens for understanding the concept of media enjoyment. The theory suggests that enjoyment can benefit from the experience of being immersed in a narrative world, as well as from the consequences of that immersion. Consequences implied by transportation theory include connections with characters and self-transformations. © 2004 International Communication Association.},
	number = {4},
	journal = {Communication Theory},
	author = {Green, Melanie C. and Brock, Timothy C. and Kaufman, Geoff F.},
	year = {2004},
	pages = {311--327},
}

@article{Yin2017,
	title = {Where no one has gone before: {A} meta-dataset of the world's largest fanfiction repository},
	volume = {2017-May},
	doi = {10.1145/3025453.3025720},
	abstract = {With its roots dating to popular television shows of the 1960s such as Star Trek, fanfiction has blossomed into an extremely widespread form of creative expression. The transition from printed zines to online fanfiction repositories has facilitated this growth in popularity, with millions of fans writing stories and adding daily to sites such as Archive Of Our Own, Fanfiction.net, FIMfiction.net, and many others. Enthusiasts are sharing their writing, reading stories written by others, and helping each other to grow as writers. Yet, this domain is often undervalued by society and understudied by researchers. To facilitate the study of this large but often marginalized community, we present a fully anonymized data release (via differential privacy) of the metadata from a large fanfiction site (to protect author privacy, story, profile, and review text is excluded, and only metadata is provided). We use visual analytics techniques to draw several intriguing insights from the data and show the potential for future research. We hope other researchers can use this data to explore further questions related to online fanfiction communities. Copyright is held by the owner/author(s). Publication rights licensed to ACM.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Yin, Kodlee and Aragon, Cecilia and Evans, Sarah and Davis, Katie},
	year = {2017},
	note = {ISBN: 9781450346559},
	keywords = {Fanfiction, Online communities, Youth},
	pages = {6106--6110},
}

@article{Wall1984,
	title = {Characters in {Bakhtin}'s {Theory}},
	volume = {9},
	url = {http://newprairiepress.org/sttcl/vol9/iss1/4},
	doi = {10.4148/2334-4415.1150},
	abstract = {The originality of Bakhtin's fragmentary and partial theory of literary genre is underlined in this article. Bakhtin's reflexion on genre is very different from that of his Formalist contemporaries. Instead of proposing elaborate typologies or generic categories, Bakhtin more often devotes his attention to showing that a meaningful approach to the topic must be diachronic. From an epistemological point of view, the possibility of exact duplication or repetition of the same generic device from text to text is denied. Each text (or reading of a text) is a new performance in which generic material is reworked and re-presented. There are affinities, therefore, between the positions of Bakhtin and Fredric Jameson (in The Political Unconscious). Generic categories are useful only if they are seen as diagnostic tools which help us to better understand how texts enter into dialogic relations with each other.},
	number = {1},
	journal = {Studies in 20th Century Literature},
	author = {Wall, Anthony},
	year = {1984},
	pages = {2334--4415},
}

@article{Kestemont2018,
	title = {Overview of the author identification task at {PAN}-2018: {Cross}-domain authorship attribution and style change detection},
	volume = {2125},
	issn = {16130073},
	abstract = {Author identification attempts to reveal the authors behind texts. It is an emerging area of research associated with applications in literary research, cyber-security, forensics, and social media analysis. In this edition of PAN, we study two task, the novel task of cross-domain authorship attribution, where the texts of known and unknown authorship belong to different domains, and style change detection, where single-author and multi-author texts are to be distinguished. For the former task, we make use of fanfiction texts, a large part of contemporary fiction written by non-professional authors who are inspired by specific well-known works, to enable us control the domain of texts for the first time. We describe a new corpus of fanfiction texts covering five languages (English, French, Italian, Polish, and Spanish). For the latter, a new data set of Q\&As covering multiple topics in English is introduced. We received 11 submissions for the cross-domain authorship attribution task and 5 submissions for the style change detection task. A survey of participant methods and analytical evaluation results are presented in this paper.},
	journal = {CEUR Workshop Proceedings},
	author = {Kestemont, Mike and Tschuggnall, Michael and Stamatatos, Efstathios and Daelemans, Walter and Specht, Günther and Stein, Benno and Potthast, Martin},
	year = {2018},
}

@inproceedings{Muzny2017,
	title = {A two-stage sieve approach for quote attribution},
	volume = {1},
	isbn = {978-1-5108-3860-4},
	doi = {10.18653/v1/e17-1044},
	abstract = {We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset: QuoteLi31. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new algorithm for quote attribution. Our twostage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune either for overall performance or for the high precision appropriate for many use cases. Our system achieves an average F-score of 87.5\% across three novels, outperforming previous systems, and can be tuned for precision of 90.4\% at a recall of 65.1\%.},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics} ({EACL} 2017)},
	author = {Muzny, Felix and Fang, Michael and Chang, Angel X. and Jurafsky, Dan},
	year = {2017},
	pages = {460--470},
}

@article{vig_causal_2020,
	title = {Causal mediation analysis for interpreting neural {NLP}: {The} case of gender bias},
	issn = {23318422},
	abstract = {Common methods for interpreting neural models in natural language processing typically examine either their structure or their behavior, but not both. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. It enables us to analyze the mechanisms by which information flows from input to output through various model components, known as mediators. We apply this methodology to analyze gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are (i) sparse, concentrated in a small part of the network; (ii) synergistic, amplified or repressed by different components; and (iii) decomposable into effects flowing directly from the input and indirectly through the mediators.
MSC Codes 68T50},
	journal = {arXiv},
	author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
	year = {2020},
	note = {arXiv: 2004.12265},
}

@article{kurita_measuring_2019,
	title = {Measuring {Bias} in contextualized word representations},
	issn = {23318422},
	doi = {10.18653/v1/w19-3823},
	abstract = {Contextual word embeddings such as BERT have achieved state of the art performance in numerousNLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.},
	journal = {arXiv},
	author = {Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W. and Tsvetkov, Yulia},
	year = {2019},
	note = {arXiv: 1906.07337},
	pages = {166--172},
}

@article{gallagher_generalized_2021,
	title = {Generalized word shift graphs: a method for visualizing and explaining pairwise comparisons between texts},
	volume = {10},
	issn = {21931127},
	url = {http://dx.doi.org/10.1140/epjds/s13688-021-00260-3},
	doi = {10.1140/epjds/s13688-021-00260-3},
	abstract = {A common task in computational text analyses is to quantify how two corpora differ according to a measurement like word frequency, sentiment, or information content. However, collapsing the texts’ rich stories into a single number is often conceptually perilous, and it is difficult to confidently interpret interesting or unexpected textual patterns without looming concerns about data artifacts or measurement validity. To better capture fine-grained differences between texts, we introduce generalized word shift graphs, visualizations which yield a meaningful and interpretable summary of how individual words contribute to the variation between two texts for any measure that can be formulated as a weighted average. We show that this framework naturally encompasses many of the most commonly used approaches for comparing texts, including relative frequencies, dictionary scores, and entropy-based measures like the Kullback–Leibler and Jensen–Shannon divergences. Through a diverse set of case studies ranging from presidential speeches to tweets posted in urban green spaces, we demonstrate how generalized word shift graphs can be flexibly applied across domains for diagnostic investigation, hypothesis generation, and substantive interpretation. By providing a detailed lens into textual shifts between corpora, generalized word shift graphs help computational social scientists, digital humanists, and other text analysis practitioners fashion more robust scientific narratives.},
	number = {1},
	journal = {EPJ Data Science},
	author = {Gallagher, Ryan J. and Frank, Morgan R. and Mitchell, Lewis and Schwartz, Aaron J. and Reagan, Andrew J. and Danforth, Christopher M. and Dodds, Peter Sheridan},
	year = {2021},
	note = {arXiv: 2008.02250
Publisher: The Author(s)},
	keywords = {Computational social science, Data visualization, Digital humanities, Information theory, Natural language processing, Sentiment analysis, Text as data, Word shift graphs},
}

@article{kasunic_at_2018,
	title = {“{At} least the pizzas you make are hot⇝: {Norms}, values, and abrasive humor on the subreddit r/{RoastMe}},
	abstract = {We present a mixed methods study of the online forum r/RoastMe, a comedy-focused subreddit of the parent site reddit.com, wherein members post photos of themselves to be ridiculed by other members; the site generally encourages harsh and offensive forms of humor in these interpersonal exchanges. We conducted semi-structured interviews with sixteen participants (both “roasters” and “roastees”) in the online forum to understand their motivations for participating, their experiences in the subreddit, and their perceptions of their and other members' participation. To complement our qualitative analyses, we also analyzed a RoastMe data set of over 9,000 image posts and 230,000 comments from June-August of 2017. From our interviews, we found that, like other deviant online communities, RoastMe relies on a specific set of norms. In RoastMe, roasters rely heavily on perspective-taking rather than dissociation from their targets, roastees highly value the often scathing assessments offered by users on RoastMe, and, despite the salience of norms that enhance feelings of safety, there is lingering concern among participants about the potential for emotional or psychological harm. Our quantitative analyses confirm many of the statements made in our qualitative interviews and provide further insights into the specific nature of interactions on the subreddit. Our study directs us toward different vantage points from which to design online community spaces that account for or leverage users' predilections for baiting behaviors, harsh judgments, and caustic humor.},
	number = {Icwsm},
	journal = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
	author = {Kasunic, Anna and Kaufman, Geoff},
	year = {2018},
	note = {ISBN: 9781577357988},
	keywords = {Full Papers},
	pages = {161--170},
}

@article{dos_santos_fighting_2018,
	title = {Fighting offensive language on social media with unsupervised text style transfer},
	volume = {2},
	doi = {10.18653/v1/p18-2031},
	abstract = {We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Dos Santos, Cicero Nogueira and Melnyk, Igor and Padhi, Inkit},
	year = {2018},
	note = {arXiv: 1805.07685
ISBN: 9781948087346},
	pages = {189--194},
}

@inproceedings{sims_measuring_2020,
	title = {Measuring {Information} {Propagation} in {Literary} {Social} {Networks}},
	abstract = {We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Sims, Matthew and Bamman, David},
	year = {2020},
	note = {arXiv: 2004.13980},
	pages = {642--652},
}

@article{ballesteros_deep-syntactic_2014,
	title = {Deep-{Syntactic} {Parsing}},
	number = {1970},
	author = {Ballesteros, Miguel and Bohnet, Bernd and Mille, Simon and Wanner, Leo and Language, Natural and Group, Processing and Kingdom, United},
	year = {2014},
	pages = {1402--1413},
}

@article{hutchinson_social_2020,
	title = {Social {Biases} in {NLP} {Models} as {Barriers} for {Persons} with {Disabilities}},
	issn = {23318422},
	doi = {10.18653/v1/2020.acl-main.487},
	abstract = {Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.},
	journal = {arXiv},
	author = {Hutchinson, Ben and Prabhakaran, Vinodkumar and Denton, Emily and Webster, Kellie and Zhong, Yu and Denuyl, Stephen},
	year = {2020},
	note = {arXiv: 2005.00813},
	keywords = {NLP, fairness, bias, disabilities},
	pages = {5491--5501},
}

@article{Ghaziani2016,
	title = {Cycles of {Sameness} and {Difference} in {LGBT} {Social} {Movements}},
	volume = {42},
	issn = {03600572},
	doi = {10.1146/annurev-soc-073014-112352},
	abstract = {Research on lesbian, gay, bisexual, and transgender (LGBT) movements has accelerated in recent years. We take stock of this literature with a focus on the United States. Our review adopts a historical approach, surveying findings on three protest cycles: gay liberation and lesbian feminism, queer activism, and marriage equality. Existing scholarship focuses primarily on oscillations of the movement's collective identity between emphasizing similarities to the heterosexual mainstream and celebrating differences. We contrast earlier movement cycles mobilized around difference with efforts to legalize same-sex marriage. Our review highlights the turning points that led to shifts in protest cycles, and we trace the consequences for movement outcomes. Scholarship will advance if researchers recognize the path-dependent nature of social movements and that sameness and difference are not oppositional, static, or discrete choices. We conclude by recommending directions for future research.},
	journal = {Annual Review of Sociology},
	author = {Ghaziani, Amin and Taylor, Verta and Stone, Amy},
	year = {2016},
	keywords = {Gay liberation, Lesbian feminism, Marriage equality, Protest cycles, Queer activism, Turning points},
	pages = {165--183},
}

@article{starbird_examining_2017,
	title = {Examining the {Alternative} {Media} {Ecosystem} through the {Production} of {Alternative} {Narratives} of {Mass} {Shooting} {Events} on {Twitter}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/download/14878/14728},
	abstract = {This research explores the alternative media ecosystem through a Twitter lens. Over a ten-month period, we col-lected tweets related to alternative narratives—e.g. conspir-acy theories—of mass shooting events. We utilized tweeted URLs to generate a domain network, connecting domains shared by the same user, then conducted qualitative analysis to understand the nature of different domains and how they connect to each other. Our findings demonstrate how alter-native news sites propagate and shape alternative narratives, while mainstream media deny them. We explain how politi-cal leanings of alternative news sites do not align well with a U.S. left-right spectrum, but instead feature an anti-globalist (vs. globalist) orientation where U.S. Alt-Right sites look similar to U.S. Alt-Left sites. Our findings de-scribe a subsection of the emerging alternative media eco-system and provide insight in how websites that promote conspiracy theories and pseudo-science may function to conduct underlying political agendas.},
	number = {Icwsm},
	journal = {Icwsm17},
	author = {Starbird, Kate},
	year = {2017},
	note = {ISBN: 9781577357889},
	keywords = {Full Papers},
	pages = {230--239},
}

@article{zhang_conversations_2018,
	title = {Conversations gone awry: {Detecting} early signs of conversational failure},
	issn = {23318422},
	abstract = {One of the main challenges online social systems face is the prevalence of antisocial behavior, such as harassment and personal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesirable behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged. To this end, we develop a framework for capturing pragmatic devices-such as politeness strategies and rhetorical prompts-used to start a conversation, and analyze their relation to its future trajectory. Applying this framework in a controlled setting, we demonstrate the feasibility of detecting early warning signs of antisocial behavior in online discussions.},
	journal = {arXiv},
	author = {Zhang, Justine and Chang, Jonathan P. and Danescu-Niculescu-Mizil, Cristian and Dixon, Lucas and Thain, Nithum and Hua, Yiqing and Taraborelli, Dario},
	year = {2018},
	pages = {1350--1361},
}

@article{yohanes_understanding_2020,
	title = {Understanding quotation extraction and attribution: towards automatic extraction of public figure’s statements for journalism in {Indonesia}},
	issn = {25149350},
	url = {https://www.emerald.com/insight/content/doi/10.1108/GKMC-07-2020-0098/full/html?casa_token=WMAEaUeSBE8AAAAA:JfWMcPm_Un8-sYqv1tCSoilnNofvDUtIfmMn8OeSJvw8eGIuUQiXWITFajtvKVmsa7a3f5N5wPa010KvR33HicJQx_2QHUYCS1BWo476HvuZAW3hdNtg},
	doi = {10.1108/GKMC-07-2020-0098},
	abstract = {Purpose: Extracting information from unstructured data becomes a challenging task for computational linguistics. Public figure’s statement attributed by journalists in a story is one type of information that can be processed into structured data. Therefore, having the knowledge base about this data will be very beneficial for further use, such as for opinion mining, claim detection and fact-checking. This study aims to understand statement extraction tasks and the models that have already been applied to formulate a framework for further study. Design/methodology/approach: This paper presents a literature review from selected previous research that specifically addresses the topics of quotation extraction and quotation attribution. Research works that discuss corpus development related to quotation extraction and quotation attribution are also considered. The findings of the review will be used as a basis for proposing a framework to direct further research. Findings: There are three findings in this study. Firstly, the extraction process still consists of two main tasks, namely, the extraction of quotations and the attribution of quotations. Secondly, most extraction algorithms rely on a rule-based algorithm or traditional machine learning. And last, the availability of corpus, which is limited in quantity and depth. Based on these findings, a statement extraction framework for Indonesian language corpus and model development is proposed. Originality/value: The paper serves as a guideline to formulate a framework for statement extraction based on the findings from the literature study. The proposed framework includes a corpus development in the Indonesian language and a model for public figure statement extraction. Furthermore, this study could be used as a reference to produce a similar framework for other languages.},
	journal = {Global Knowledge, Memory and Communication},
	author = {Yohanes, Yohanes Sigit and Kumar, Yogan Jaya and Zulkarnain, Nur Zareen},
	year = {2020},
	keywords = {Corpus development, Indonesian language, Journalism, Online news, Quotation attribution, Quotation extraction, Statement extraction},
}

@inproceedings{papay_quotation_2019,
	title = {Quotation detection and classification with a corpus-agnostic model},
	isbn = {978-954-452-055-7},
	doi = {10.26615/978-954-452-056-4_103},
	abstract = {The detection of quotations (i.e., reported speech, thought, and writing) has established itself as an NLP analysis task. However, state-of-the-art models have been developed on the basis of specific corpora and incorporate a high degree of corpus-specific assumptions and knowledge, which leads to fragmentation. In the spirit of task-agnostic modeling, we present a corpus-agnostic neural model for quotation detection and evaluate it on three corpora that vary in language, text genre, and structural assumptions. The model (a) approaches the state-of-the-art on the corpora when using established feature sets and (b) shows reasonable performance even when using solely word forms, which makes it applicable for non-standard (i.e., historical) corpora.},
	booktitle = {International {Conference} {Recent} {Advances} in {Natural} {Language} {Processing}, {RANLP}},
	author = {Papay, Sean and Padó, Sebastian},
	year = {2019},
	note = {ISSN: 13138502},
	pages = {888--894},
}

@article{almeida_joint_2014,
	title = {Joint model for quotation attribution and coreference resolution},
	doi = {10.3115/v1/e14-1005},
	abstract = {We address the problem of automatically attributing quotations to speakers, which has great relevance in text mining and media monitoring applications. While current systems report high accuracies for this task, they either work at mention-level (getting credit for detecting uninformative mentions such as pronouns), or assume the coreferent mentions have been detected beforehand; the inaccuracies in this preprocessing step may lead to error propagation. In this paper, we introduce a joint model for entity-level quotation attribution and coreference resolution, exploiting correlations between the two tasks. We design an evaluation metric for attribution that captures all speakers' mentions. We present results showing that both tasks benefit from being treated jointly. © 2014 Association for Computational Linguistics.},
	journal = {14th Conference of the European Chapter of the Association for Computational Linguistics 2014, EACL 2014},
	author = {Almeida, Mariana S.C. and Almeida, Miguel B. and Martins, André F.T.},
	year = {2014},
	note = {ISBN: 9781632663962},
	pages = {39--48},
}

@article{papay_riqua_2020,
	title = {{RiQuA}: {A} corpus of rich quotation annotation for {English} literary text},
	abstract = {We introduce RiQuA (RIch QUotation Annotations), a corpus that provides quotations, including their interpersonal structure (speakers and addressees) for English literary text. The corpus comprises 11 works of 19th-century literature that were manually doubly annotated for direct and indirect quotations. For each quotation, its span, speaker, addressee, and cue are identified (if present). This provides a rich view of dialogue structures not available from other available corpora. We detail the process of creating this dataset, discuss the annotation guidelines, and analyze the resulting corpus in terms of inter-annotator agreement and its properties. RiQuA, along with its annotations guidelines and associated scripts, are publicly available for use, modification, and experimentation.},
	number = {May},
	journal = {LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings},
	author = {Papay, Sean and Padó, Sebastian},
	year = {2020},
	note = {ISBN: 9791095546344},
	keywords = {Interpersonal structure, Literary text, Quotations},
	pages = {835--841},
}

@article{stymne_slanda_2020,
	title = {{SLäNDa}: {An} annotated corpus of narrative and dialogue in swedish literary fiction},
	abstract = {We describe a new corpus, SLäNDa, the Swedish Literary corpus of Narrative and Dialogue. It contains Swedish literary fiction, which has been manually annotated for cited materials, with a focus on dialogue. The annotation covers excerpts from eight Swedish novels written between 1879-1940, a period of modernization of the Swedish language. SLäNDa contains annotations for all cited materials that are separate from the main narrative, like quotations and signs. The main focus is on dialogue, for which we annotate speech segments, speech tags, and speakers. In this paper we describe the annotation protocol and procedure and show that we can reach a high inter-annotator agreement. In total, SLäNDa contains annotations of 44 chapters with over 220K tokens. The annotation identified 4,733 instances of cited material and 1,143 named speaker-speech mappings. The corpus is useful for developing computational tools for different types of analysis of literary narrative and speech. We perform a small pilot study where we show how our annotation can help in analyzing language change in Swedish. We find that a number of common function words have their modern version appear earlier in speech than in narrative.},
	number = {May},
	journal = {LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings},
	author = {Stymne, Sara and Östman, Carin},
	year = {2020},
	note = {ISBN: 9791095546344},
	keywords = {Annotation, Dialogue, Direct speech, Literary corpora, Narrative},
	pages = {826--834},
}

@article{yeung_identifying_2017,
	title = {Identifying {Speakers} and {Listeners} of {Quoted} {Speech} in {Literary} {Works}},
	url = {https://www.aclweb.org/anthology/I17-2055},
	abstract = {We present the first study that evaluates both speaker and listeneridentification for direct speech in literary texts. Our approach consists oftwo steps: identification of speakers and listeners near the quotes, anddialogue chain segmentation. Evaluation results show that this approachoutperforms a rule-based approach that is state-of-the-art on a corpus ofliterary texts.},
	journal = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	author = {Yeung, Chak Yan and Lee, John},
	year = {2017},
	pages = {325--329},
}

@article{ek_identifying_2019,
	title = {Identifying speakers and addressees in dialogues extracted from literary fiction},
	abstract = {This paper describes an approach to identifying speakers and addressees in dialogues extracted from literary fiction, along with a dataset annotated for speaker and addressee. The overall purpose of this is to provide annotation of dialogue interaction between characters in literary corpora in order to allow for enriched search facilities and construction of social networks from the corpora. To predict speakers and addressees in a dialogue, we use a sequence labeling approach applied to a given set of characters. We use features relating to the current dialogue, the preceding narrative, and the complete preceding context. The results indicate that even with a small amount of training data, it is possible to build a fairly accurate classifier for speaker and addressee identification across different authors, though the identification of addressees is the more difficult task.},
	journal = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
	author = {Ek, Adam and Wirén, Mats and Östling, Robert and Björkenstam, Kristina N. and Grigonyte, Gintare and Capková, Sofia Gustafson},
	year = {2019},
	note = {ISBN: 9791095546009},
	keywords = {Addressee identification, Literary corpora, Quote attribution, Speaker identification},
	pages = {817--824},
}

@article{green_algorithmic_2020,
	title = {Algorithmic realism: {Expanding} the boundaries of algorithmic thought},
	doi = {10.1145/3351095.3372840},
	abstract = {Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as “algorithmic formalism” and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking-“algorithmic realism”-that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.},
	journal = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Green, Ben and Viljoen, Salomé},
	year = {2020},
	note = {ISBN: 9781450369367},
	keywords = {Algorithms, Critical algorithm studies, Epistemology, Law, STS},
	pages = {19--31},
}

@misc{buchanan_very_nodate,
	title = {A ({Very}) {Brief} {History} of {Artificial} {Intelligence}},
	author = {Buchanan, Bruce G.},
}

@article{haenlein_brief_2019,
	title = {A brief history of artificial intelligence: {On} the past, present, and future of artificial intelligence},
	volume = {61},
	issn = {21628564},
	doi = {10.1177/0008125619864925},
	abstract = {This introduction to this special issue discusses artificial intelligence (AI), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world’s leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.},
	number = {4},
	journal = {California Management Review},
	author = {Haenlein, Michael and Kaplan, Andreas},
	year = {2019},
	note = {ISBN: 0008125619},
	keywords = {Artificial intelligence, Big data, Machine-blearning, Regulation, Strategy},
	pages = {5--14},
}

@article{waseem_disembodied_2021,
	title = {Disembodied {Machine} {Learning}: {On} the {Illusion} of {Objectivity} in {NLP}},
	url = {http://arxiv.org/abs/2101.11974},
	abstract = {Machine Learning seeks to identify and encode bodies of knowledge within provided datasets. However, data encodes subjective content, which determines the possible outcomes of the models trained on it. Because such subjectivity enables marginalisation of parts of society, it is termed (social) `bias' and sought to be removed. In this paper, we contextualise this discourse of bias in the ML community against the subjective choices in the development process. Through a consideration of how choices in data and model development construct subjectivity, or biases that are represented in a model, we argue that addressing and mitigating biases is near-impossible. This is because both data and ML models are objects for which meaning is made in each step of the development pipeline, from data selection over annotation to model training and analysis. Accordingly, we find the prevalent discourse of bias limiting in its ability to address social marginalisation. We recommend to be conscientious of this, and to accept that de-biasing methods only correct for a fraction of biases.},
	author = {Waseem, Zeerak and Lulz, Smarika and Bingel, Joachim and Augenstein, Isabelle},
	year = {2021},
	note = {arXiv: 2101.11974},
	pages = {1--8},
}

@article{akoury_storium_2020,
	title = {{STORIUM}: {A} dataset and evaluation platform for machine-in-the-loop story generation},
	issn = {23318422},
	doi = {10.18653/v1/2020.emnlp-main.525},
	abstract = {Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.},
	journal = {arXiv},
	author = {Akoury, Nader and Wang, Shufan and Whiting, Josh and Hood, Stephen and Peng, Nanyun and Iyyer, Mohit},
	year = {2020},
	note = {arXiv: 2010.01717},
	pages = {6470--6484},
}

@article{sambasivan_re-imagining_2021,
	title = {Re-imagining {Algorithmic} {Fairness} in {India} and {Beyond}},
	url = {http://arxiv.org/abs/2101.09995},
	abstract = {Conventional algorithmic fairness is West-centric, as seen in its sub-groups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
	author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
	year = {2021},
	note = {arXiv: 2101.09995
ISBN: 9781450383097},
	keywords = {ability, acm reference format, algorithmic fairness, anti-caste politics, caste, class, critical algorithmic studies, decoloniality, fem-, gender, india, inism, religion},
}

@article{sommet_keep_2017,
	title = {Keep calm and learn multilevel logistic modeling: {A} simplified three-step procedure using stata, {R}, {Mplus}, and {SPSS}},
	volume = {30},
	issn = {23978570},
	doi = {10.5334/irsp.90},
	abstract = {This paper aims to introduce multilevel logistic regression analysis in a simple and practical way. First, we introduce the basic principles of logistic regression analysis (conditional probability, logit transformation, odds ratio). Second, we discuss the two fundamental implications of running this kind of analysis with a nested data structure: In multilevel logistic regression, the odds that the outcome variable equals one (rather than zero) may vary from one cluster to another (i.e. the intercept may vary) and the effect of a lower-level variable may also vary from one cluster to another (i.e. the slope may vary). Third and finally, we provide a simplified three-step “turnkey” procedure for multilevel logistic regression modeling: • Preliminary phase: Cluster- or grand-mean centering variables • Step \#1: Running an empty model and calculating the intraclass correlation coefficient (ICC) • Step \#2: Running a constrained and an augmented intermediate model and performing a likelihood ratio test to determine whether considering the cluster-based variation of the effect of the lower-level variable improves the model fit • Step \#3 Running a final model and interpreting the odds ratio and confidence intervals to determine whether data support your hypothesis Command syntax for Stata, R, Mplus, and SPSS are included. These steps will be applied to a study on Justin Bieber, because everybody likes Justin Bieber.},
	number = {1},
	journal = {International Review of Social Psychology},
	author = {Sommet, Nicolas and Morselli, Davide},
	year = {2017},
	keywords = {Cluster-mean centering, Grand-mean centering, Intraclass correlation coefficient, Justin Bieber, Likelihood ratio test, Logistic regression, Multilevel logistic modeling, Random random slope variance, Three-step simplified procedure},
	pages = {203--218},
}

@article{karl_whittington_this_2014,
	title = {This {PDF} contains the complete {Keywords} section of {TSQ}: {Transgender} {Studies} {Quarterly} , {Volume} 1, {Numbers} 1–2.},
	volume = {1},
	number = {1-2},
	journal = {Transgender Studies Quarterly},
	author = {{Karl Whittington}},
	year = {2014},
	pages = {19--290},
}

@article{guo_inflating_2020,
	title = {Inflating topic relevance with ideology: {A} case study of political ideology bias in social topic detection models},
	issn = {23318422},
	doi = {10.18653/v1/2020.coling-main.428},
	abstract = {We investigate the impact of political ideology biases in training data. Through a set of comparison studies, we examine the propagation of biases in several widely-used NLP models and its effect on the overall retrieval accuracy. Our work highlights the susceptibility of large, complex models to propagating the biases from human-selected input, which may lead to a deterioration of retrieval accuracy, and the importance of controlling for these biases. Finally, as a way to mitigate the bias, we propose to learn a text representation that is invariant to political ideology while still judging topic relevance.},
	journal = {arXiv},
	author = {Guo, Meiqi and Hwa, Rebecca and Lin, Yu Ru and Chung, Wen Ting},
	year = {2020},
	note = {arXiv: 2011.14293},
	pages = {4873--4885},
}

@article{liu_generalized_2019,
	title = {A generalized idiom usage recognition model based on semantic compatibility},
	issn = {2159-5399},
	doi = {10.1609/aaai.v33i01.33016738},
	abstract = {Many idiomatic expressions can be used figuratively or literally depending on the context. A particular challenge of automatic idiom usage recognition is that idioms, by their very nature, are idiosyncratic in their usages; therefore, most previous work on idiom usage recognition mainly adopted a “per idiom” classifier approach, i.e., a classifier needs to be trained separately for each idiomatic expression of interest, often with the aid of annotated training examples. This paper presents a transferred learning approach for developing a generalized model to recognize whether an idiom is used figuratively or literally. Our work is based on the observation that most idioms, when taken literally, would be somehow semantically at odds with their context. Therefore, a quantified notion of semantic compatibility may help to discern the intended usage for any arbitrary idiom. We propose a novel semantic compatibility model by adapting the training of a Continuous Bag-of-Words (CBOW) model for the purpose of idiom usage recognition. There is no need to annotate idiom usage examples for training. We perform evaluative experiments on two corpora; results show that the proposed generalized model achieves competitive results compared to state-of-the-art per-idiom models.},
	journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
	author = {Liu, Changsheng and Hwa, Rebecca},
	year = {2019},
	note = {ISBN: 9781577358091},
	pages = {6738--6745},
}

@article{yan_mimicprop_2020,
	title = {{MimicProp}: {Learning} to incorporate lexicon knowledge into distributed word representation for social media analysis},
	abstract = {Lexicon-based methods and word embeddings are the two widely used approaches for analyzing texts in social media. The choice of an approach can have a significant impact on the reliability of the text analysis. For example, lexicons provide manually curated, domain-specific attributes about a limited set of words, while word embeddings learn to encode some loose semantic interpretations for a much broader set of words. Text analysis can benefit from a representation that offers both the broad coverage of word embeddings and the domain knowledge of lexicons. This paper presents MimicProp, a new graph-mode method that learns a lexicon-aligned word embedding. Our approach improves over prior graph-based methods in terms of its interpretability (i.e., lexicon attributes can be recovered) and generalizability (i.e., new words can be learned to incorporate lexicon knowledge). It also effectively improves the performance of downstream analysis applications, such as text classification.},
	number = {Icwsm},
	journal = {Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM 2020},
	author = {Yan, Muheng and Lin, Yu Ru and Hwa, Rebecca and Ertugrul, Ali Mert and Guo, Meiqi and Chung, Wen Ting},
	year = {2020},
	note = {ISBN: 9781577357889},
	keywords = {ICWSM Full Papers},
	pages = {738--749},
}

@article{darmawan_four_2009,
	title = {Four {Stages} of {Social} {Movements}},
	abstract = {Abstract An explanation of what defines a social movement is followed by a description of the development and theory of the model of the four stages of social movements. The four stages of social movement development are emergence, coalescence, bureaucra- tization, and decline. The Decline stage can result from several different causes, such as repression, co-optation, success, failure, and mainstream. The four stages of development model can be applied to understand how movements form, grow, and dissi- pate. It has limitations, however, in its application to new social movements and movements that are not rooted in political action. Despite these limitations, the four stages model is still highly useful in understanding collective action and provides a useful frame of analysis for sociologists considering social movements and their effects in the past and present. Overview},
	journal = {EBSCO Publishing Inc.},
	author = {Darmawan, Ikhsan},
	year = {2009},
	note = {ISBN: 9781429834728},
	keywords = {and decline, bureaucra -, coalescence, movement development are emergence, result from several, social, social movements, the decline stage can, the four stages of, tization},
	pages = {1--7},
}

@article{neumayer_activism_2016,
	title = {Activism and radical politics in the digital age: {Towards} a typology},
	volume = {22},
	issn = {17487382},
	doi = {10.1177/1354856514553395},
	abstract = {This article aims to develop a typology for evaluating different types of activism in the digital age, based on the ideal of radical democracy. Departing from this ideal, activism is approached in terms of processes of identification by establishing conflictual frontiers to outside others as either adversaries or enemies. On the basis of these discussions, we outline a typology of four kinds of activists, namely the salon activist, the contentious activist, the law-abiding activist and the Gandhian activist. The typology's first axis, between antagonism and agonism, is derived from normative discussions in radical democracy concerning developing frontiers. The second axis, about readiness to engage in civil disobedience, is derived from a review of studies of different forms of online activism. The article concludes by suggesting that the different forms of political engagement online have to be taken into account when studying how online activism can contribute to social change.},
	number = {2},
	journal = {Convergence},
	author = {Neumayer, Christina and Svensson, Jakob},
	year = {2016},
	keywords = {Activism, civil disobedience, identity, radical democracy, social media},
	pages = {131--146},
}

@article{lee_stanford_2011,
	title = {Stanford ’ s {Multi}-{Pass} {Sieve} {Coreference} {Resolution} {System} at the {CoNLL}-2011 {Shared} {Task}},
	abstract = {This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task. Our system is a collection of deterministic coreference resolution mod- els that incorporate lexical, syntactic, seman- tic, and discourse information. All these mod- els use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both pre- dicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.},
	journal = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics},
	author = {Lee, Heeyoung and Peirsman, Yves and Chang, Angel and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
	year = {2011},
	pages = {28--34},
}

@article{sobel_culture_2015,
	title = {Culture {Shifting} at {Warp} {Speed}: {How} the {Law}, {Public} {Engagement}, and {Will} \& {Grace} {Led} to {Social} {Change} for {LGBT} {People}},
	volume = {89},
	url = {https://search.proquest.com/docview/1759328302?pq-origsite=gscholar},
	abstract = {This Article examines the interaction of culture shifting and rule shifting that has led to dramatic change on LGBT issues in the US. This Article examines how and why the combined legal and cultural change in the US is moving so rapidly since Tom Stoddard's visit to New Zealand by utilizing the framework Stoddard created in Bleeding Heart. This Article applies a fifth dimension: public engagement. Then, this Article reviews the legal rules that have shifted on LGBT issues since 1997 through court decisions, legislation, and ballot initiatives. Part III of this Article examines the interaction of culture shifting and rule shifting that has occurred since 1997. Finally, this Article employs the expanded culture-shifting framework to explore how culture shifting has occurred and concludes that it is the last component of public engagement that has caused culture shifting to move at warp speed. This Article concludes that in order for culture shifting to occur, there has to be active engagement in advocacy on multiple fronts.},
	journal = {St. John's Law Review},
	author = {Sobel, Stacey L.},
	year = {2015},
	pages = {143--194},
}

@article{lane_social_2019,
	title = {Social media expression and the political self},
	volume = {69},
	issn = {14602466},
	doi = {10.1093/joc/jqy064},
	abstract = {Expression has the power to shape how we see ourselves. In this paper, we argue that the dynamics of political expression on social media can influence not only political behavior, but also citizens' more fundamental political self-concepts. Specifically, political expression on social media can entail a public commitment to a political self-presentation, which may lead individuals to perceive themselves as politically active, interested, efficacious, and knowledgeable. Analyzing panel survey data from the 2016 U.S. election, we find that political expression on social media increases users' motivations to present themselves as politically active on social media. Political self-presentation motivations are, in turn, positively associated with strengthened dimensions of political self-concepts (i.e., political interest, political self-efficacy, and perceived participation). Findings emphasize the role of expression in shaping political self-concepts, and further hint that this relationship may depend on whether the expressive behavior constitutes a clear, public commitment to a political self-presentation.},
	number = {1},
	journal = {Journal of Communication},
	author = {Lane, Daniel S. and Lee, Slgi S. and Liang, Fan and Kim, Dam Hee and Shen, Liwei and Weeks, Brian E. and Kwak, Nojin},
	year = {2019},
	keywords = {Political Expression, Public Commitment, Self-Concept, Self-Presentation, Social Media},
	pages = {49--72},
}

@article{roth-gordon_producing_2020,
	title = {Producing white comfort through "corporate cool": {Linguistic} appropriation, social media, and @{BrandsSayingBae}},
	volume = {2020},
	issn = {16133668},
	doi = {10.1515/ijsl-2020-2105},
	abstract = {Drawing on branded tweets that linguistically appropriate slang, African American Language, and hip hop lyrics, this article examines how corporations rework black culture to create "corporate cool"as part of their advertising strategy on social media. We examine three processes that corporations engage in to associate themselves with "coolness"while managing levels of racial contact and proximity for their audience: 1) racially ambiguous voicing, 2) "bleaching"black bodies out of images, and 3) the forging of "racially tinged"intertextual connections. While previous scholarship has analyzed how acts of cultural and linguistic appropriation reap profit for white people and continue to stigmatize already racially marginalized groups, we describe how these seemingly innocent cultural and linguistic references harness a corporately constructed black cool to produce a sense of white comfort. We argue that white comfort is generated not only through the avoidance of overt references to racial conflict, as the term "white fragility"suggests, but also through well-worn, familiar, and comfortable reminders of racial difference and domination that are offered at a safe distance from actual black people and contexts of racial violence.},
	number = {265},
	journal = {International Journal of the Sociology of Language},
	author = {Roth-Gordon, Jennifer and Harris, Jessica and Zamora, Stephanie},
	year = {2020},
	keywords = {Intertextuality, Linguistic appropriation, Social media, Voicing, Whiteness},
	pages = {107--128},
}

@article{clarke_impact_2013,
	title = {The impact of {CSCL} beyond the online environment},
	volume = {1},
	issn = {15734552},
	abstract = {Accountable Talk is a form of classroom interaction that positions students as thinkers in interaction and encourages students to make their thinking visible for collaborative reasoning. This paper reports on a two-year teacher professional development program in which teachers were coached to use Accountable Talk practices in their classrooms. Online collaborative learning activities were used to prepare students for these whole-class, teacher lead discussions using the same paradigm. Findings from a series of studies embedded within the two year professional development program provides evidence that novel conversational agent designs, based on the Accountable Talk approach to discussion facilitation, improve learning during the online exercises and better prepare students to benefit from whole class discussions. In this paper we evaluate the effect on teacher uptake of Accountable Talk practices when their students have participated in these online small group activities. © ISLS.},
	journal = {Computer-Supported Collaborative Learning Conference, CSCL},
	author = {Clarke, Sherice N. and Chen, Gaowei and Stainton, Catherine and Katz, Sandra and Greeno, James G. and Resnick, Lauren B. and Dyke, Gregory and Howley, Iris and Adamson, David and Rosé, Carolyn P.},
	year = {2013},
	pages = {105--112},
}

@article{cohen_embedding_2017,
	title = {Embedding of semantic predications},
	volume = {68},
	issn = {15320464},
	url = {http://dx.doi.org/10.1016/j.jbi.2017.03.003},
	doi = {10.1016/j.jbi.2017.03.003},
	abstract = {This paper concerns the generation of distributed vector representations of biomedical concepts from structured knowledge, in the form of subject-relation-object triplets known as semantic predications. Specifically, we evaluate the extent to which a representational approach we have developed for this purpose previously, known as Predication-based Semantic Indexing (PSI), might benefit from insights gleaned from neural-probabilistic language models, which have enjoyed a surge in popularity in recent years as a means to generate distributed vector representations of terms from free text. To do so, we develop a novel neural-probabilistic approach to encoding predications, called Embedding of Semantic Predications (ESP), by adapting aspects of the Skipgram with Negative Sampling (SGNS) algorithm to this purpose. We compare ESP and PSI across a number of tasks including recovery of encoded information, estimation of semantic similarity and relatedness, and identification of potentially therapeutic and harmful relationships using both analogical retrieval and supervised learning. We find advantages for ESP in some, but not all of these tasks, revealing the contexts in which the additional computational work of neural-probabilistic modeling is justified.},
	journal = {Journal of Biomedical Informatics},
	author = {Cohen, Trevor and Widdows, Dominic},
	year = {2017},
	pmid = {28284761},
	note = {Publisher: Elsevier Inc.},
	keywords = {Distributional semantics, Literature-based discovery, Pharmacovigilance, Predication-based semantic indexing, Semantic predications, Word embeddings},
	pages = {150--166},
}

@article{penman_aspirational_2018,
	title = {An {Aspirational} {Rhetoric} of {Anti}- {Racism} : {A} {Participatory} {Study} of {Responsive} {Engagement}},
	number = {June},
	author = {Penman, Will},
	year = {2018},
}

@article{hua_characterizing_2020,
	title = {Characterizing {Twitter} {Users} {Who} {Engage} in {Adversarial} {Interactions} against {Political} {Candidates}},
	doi = {10.1145/3313831.3376548},
	abstract = {Social media provides a critical communication platform for political figures, but also makes them easy targets for harassment. In this paper, we characterize users who adversarially interact with political figures on Twitter using mixed-method techniques. The analysis is based on a dataset of 400 thousand users' 1.2 million replies to 756 candidates for the U.S. House of Representatives in the two months leading up to the 2018 midterm elections. We show that among moderately active users, adversarial activity is associated with decreased centrality in the social graph and increased attention to candidates from the opposing party. When compared to users who are similarly active, highly adversarial users tend to engage in fewer supportive interactions with their own party's candidates and express negativity in their user profiles. Our results can inform the design of platform moderation mechanisms to support political figures countering online harassment.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Hua, Yiqing and Naaman, Mor and Ristenpart, Thomas},
	year = {2020},
	note = {ISBN: 9781450367080},
	keywords = {online harassment, political candidates, twitter, user behavior},
	pages = {1--13},
}

@article{cohen_bringing_2018,
	title = {Bringing order to neural word embeddings with embeddings augmented by random permutations ({EARP})},
	doi = {10.18653/v1/k18-1045},
	abstract = {Word order is clearly a vital part of human language, but it has been used comparatively lightly in distributional vector models. This paper presents a new method for incorporating word order information into word vector embedding models by combining the benefits of permutation-based order encoding with the more recent method of skip-gram with negative sampling. The new method introduced here is called Embeddings Augmented by Random Permutations (EARP). It operates by applying permutations to the coordinates of context vector representations during the process of training. Results show an 8\% improvement in accuracy on the challenging Bigger Analogy Test Set, and smaller but consistent improvements on other analogy reference sets. These findings demonstrate the importance of order-based information in analogical retrieval tasks, and the utility of random permutations as a means to augment neural embeddings.},
	number = {CoNLL},
	journal = {CoNLL 2018 - 22nd Conference on Computational Natural Language Learning, Proceedings},
	author = {Cohen, Trevor and Widdows, Dominic},
	year = {2018},
	note = {ISBN: 9781948087728},
	pages = {465--475},
}

@article{widdows_reasoning_2014,
	title = {Reasoning with vectors: {A} continuous model for fast robust inference},
	volume = {23},
	issn = {13689894},
	doi = {10.1093/jigpal/jzu028},
	abstract = {This article describes the use of continuous vector space models for reasoning with a formal knowledge base. The practical significance of these models is that they support fast, approximate but robust inference and hypothesis generation, which is complementary to the slow, exact, but sometimes brittle behaviour of more traditional deduction engines such as theorem provers. The article explains the way logical connectives can be used in semantic vector models, and summarizes the development of Predication-based Semantic Indexing, which involves the use of Vector Symbolic Architectures to represent the concepts and relationships from a knowledge base of subject-predicate-object triples. Experiments show that the use of continuous models for formal reasoning is not only possible, but already demonstrably effective for some recognized informatics tasks, and showing promise in other traditional problem areas. Examples described in this article include: predicting new uses for existing drugs in biomedical informatics; removing unwanted meanings from search results in information retrieval and concept navigation; type inference from attributes; comparing words based on their orthography; and representing tabular data, including modelling numerical values. The algorithms and techniques described in this article are all publicly released and freely available in the Semantic Vectors open-source software package.1},
	number = {2},
	journal = {Logic Journal of the IGPL},
	author = {Widdows, Dominic and Cohen, Trevor},
	year = {2014},
	keywords = {Continuous-valued logics, Distributional semantics, Semantic vector models, Vector symbolic architectures},
	pages = {141--173},
}

@article{perozzi_deepwalk_2014,
	title = {{DeepWalk}: {Online} learning of social representations},
	doi = {10.1145/2623330.2623732},
	abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection. © 2014 ACM.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	note = {arXiv: 1403.6652
ISBN: 9781450329569},
	keywords = {deep learning, latent representations, learning with partial labels, network classification, online learning, social networks},
	pages = {701--710},
}

@article{bindel_diversifying_2018,
	title = {Diversifying cornell cs ph.d. admissions},
	volume = {33},
	doi = {20.1086/678973},
	number = {2015},
	author = {Bindel, David},
	year = {2018},
	pages = {1--10},
}

@article{mayfield_equity_2019,
	title = {Equity {Beyond} {Bias} in {Language} {Technologies} for {Education}},
	volume = {1},
	doi = {10.18653/v1/w19-4446},
	abstract = {There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, and identify future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.},
	author = {Mayfield, Elijah and Madaio, Michael and Prabhumoye, Shrimai and Gerritsen, David and McLaughlin, Brittany and Dixon-Román, Ezekiel and Black, Alan W},
	year = {2019},
	pages = {444--460},
}

@article{flores_undoing_2015,
	title = {Undoing appropriateness: {Racioling} uistic ideologies and language diversity in education},
	volume = {85},
	issn = {19435045},
	doi = {10.17763/0017-8055.85.2.149},
	abstract = {In this article, Nelson Flores and Jonathan Rosa critique appropriateness-based approaches to language diversity in education. Those who subscribe to these approaches conceptualize standardized linguistic practices as an objective set of linguistic forms that are appropriate for an academic setting. In contrast, Flores and Rosa highlight the raciolinguistic ideologies through which racialized bodies come to be constructed as engaging in appropriately academic linguistic practices. Drawing on theories of language ideologies and racialization, they offer a perspective from which students classified as long-term English learners, heritage language learners, and Standard English learners can be understood to inhabit a shared racial positioning that frames their linguistic practices as deficient regardless of how closely they follow supposed rules of appropriateness. The authors illustrate how appropriatenessbased approaches to language education are implicated in the reproduction of racial normativity by expecting language-minoritized students to model their linguistic practices after the white speaking subject despite the fact that the white listening subject continues to perceive their language use in racialized ways. They conclude with a call for reframing language diversity in education away from a discourse of appropriateness toward one that seeks to denaturalize standardized linguistic categories.},
	number = {2},
	journal = {Harvard Educational Review},
	author = {Flores, Nelson and Rosa, Jonathan},
	year = {2015},
	pages = {149--171},
}

@article{khosla_using_2020,
	title = {Using {Type} {Information} to {Improve} {Entity} {Coreference} {Resolution}},
	issn = {23318422},
	doi = {10.18653/v1/2020.codi-1.3},
	abstract = {Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.},
	author = {Khosla, Sopan and Rose, Carolyn},
	year = {2020},
	note = {arXiv: 2010.05738},
	pages = {20--31},
}

@article{toshniwal_learning_2020,
	title = {Learning to {Ignore}: {Long} {Document} {Coreference} with {Bounded} {Memory} {Neural} {Networks}},
	issn = {23318422},
	doi = {10.18653/v1/2020.emnlp-main.685},
	abstract = {Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.},
	author = {Toshniwal, Shubham and Wiseman, Sam and Ettinger, Allyson and Livescu, Karen and Gimpel, Kevin},
	year = {2020},
	note = {arXiv: 2010.02807},
	pages = {8519--8526},
}

@article{estevez_gentle_2019,
	title = {Gentle {Introduction} to {Artificial} {Intelligence} for {High}-{School} {Students} {Using} {Scratch}},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2956136},
	abstract = {The importance of educating the next generations in the understanding of the fundamentals of the upcoming scientific and technological innovations that will force a broad social and economical paradigm change can not be overstressed. One such breakthrough technologies is Artificial Intelligence (AI), specifically machine learning algorithms. Nowadays, the public has little understanding of the workings and implications of AI techniques that are already entering their lives in many ways. We aim to achieve widespread public understanding of these issues in an experiential learning framework. Following a design based research approach, we propose to implement program coding scaffoldings to teach and experiment some basic mechanisms of AI systems. Such experiments would be shedding new light into AI potentials and limitations. In this paper we focus on innovative ways to introduce high school students to the fundamentals and operation of two of the most popular AI algorithms. We describe the elements of a workshop where we provide an academic use-create-modify scaffolding where students work on the Scratch partial coding of the algorithms so they can explore the behavior of the algorithm, gaining understanding of the underlying computational thinking of AI processes. The extent of the impact on the students of this experience is measured through questionnaires filled before and after participation in the workshop. Preliminary experiments offer encouraging results, showing that the workshop has differential impact on the way students understand AI.},
	journal = {IEEE Access},
	author = {Estevez, Julian and Garate, Gorka and Grana, Manuel},
	year = {2019},
	keywords = {Scratch programming, public AI awareness, teaching AI fundamentals},
	pages = {179027--179036},
}

@article{brown_girls_2006,
	title = {Girls and guys, ghetto and bougie: {Metapragmatics}, ideology and the management of social identities},
	volume = {10},
	issn = {13606441},
	doi = {10.1111/j.1467-9841.2006.00297.x},
	abstract = {This case study explores the metapragmatic awareness of a young, academically successful, African American, female speaker. It describes some of the identities and orientations that the speaker performs through language and the perceived role of linguistic style in such performances. This study suggests that these linguistic performances are a complex negotiation of ethnicity, gender and class that both draw from and resist the macrosocial indexing of social categories. Further, the understood role of language in the social negotiations of the speaker serves as an illustration of the relationship among metapragmatics, ideology and identity and also highlights the dynamism of identity management as individuals position themselves in allegiance with, or opposition to, various groups that populate their social landscape. © The author 2006 Journal compilation © Blackwell Publishing Ltd. 2006.},
	number = {5},
	journal = {Journal of Sociolinguistics},
	author = {Brown, David West},
	year = {2006},
	keywords = {African American English, Language style, Metalanguage, Metapragmatic awareness, Youth culture},
	pages = {596--610},
}

@article{brown_singapore_2014,
	title = {Singapore {English} and styling the {Ah} {Beng}},
	volume = {33},
	issn = {08832919},
	doi = {10.1111/weng.12070},
	abstract = {This paper explores stylized renderings of Singapore English as such 'verbal art' is used by Singaporean youth in popular online forums. In order to analyze these stylizations, this study uses corpora collected from two forums frequented by Singaporean students. The data suggests that posters often use stylized representations to perform or to ventriloquize the identity of an Ah Beng (a kind of hustler or gangster). Implicated in such performances are sometimes complex negotiations of class, gender, and ethnicity. The relationship among the linguistic features central to our study and the social meanings signaled by those features suggest the value of approaches that emphasize the range of pragmatic and metapragmatic meanings or indexicalities that accrue to features in modeling variation in Singapore English. © 2014 John Wiley \& Sons Ltd.},
	number = {1},
	journal = {World Englishes},
	author = {Brown, David West and Jie, Teo Shi},
	year = {2014},
	pages = {60--84},
}

@article{aull_fighting_2013,
	title = {Fighting words: {A} corpus analysis of gender representations in sports reportage},
	volume = {8},
	issn = {17495032},
	doi = {10.3366/cor.2013.0033},
	abstract = {In this study, we explore linguistic constructions of gender in US sports reportage concerning two related basketball altercations: the Pacers-Pistons NBA fight in 2004 and the Shock-Sparks WNBA fight in 2008. We use a combined corpus and qualitative textual analysis to investigate coverage from the days immediately following the fights and to compare that coverage to sports reportage more generally. Our analysis reveals key differences in narrative focus; for example, that NBA coverage is most interested in blame assignation in the isolated event, while WNBA coverage concerns gender and the league writ large. Such patterns, which are realised linguistically in both explicit and implicit ways, contribute to the 'othering' of women and women athletes in the increasingly important sports-media-commercial complex. © Edinburgh University Press.},
	number = {1},
	journal = {Corpora},
	author = {Aull, Laura L. and Brown, David West},
	year = {2013},
	keywords = {Corpus linguistics, Gender, Sports, Sports writing},
	pages = {27--52},
}

@article{bamman_dataset_2020,
	title = {A {Dataset} of {Literary} {Coreference}},
	abstract = {We present in this work a new dataset of coreference annotations for works of literature in English, covering 29,103 mentions in 210,532 tokens from 100 works of fiction. This dataset differs from previous coreference datasets in containing documents whose average length (2,105.3 words) is four times longer than other benchmark datasets (463.7 for OntoNotes), and contains examples of difficult coreference problems common in literature. This dataset allows for an evaluation of cross-domain performance for the task of coreference resolution, and analysis into the characteristics of long-distance within-document coreference.},
	number = {May},
	author = {Bamman, David and Lewke, Olivia and Mansoor, Anya},
	year = {2020},
	keywords = {coreference, corpus annotation, literature},
	pages = {11--16},
}

@article{vishnubhotla_are_2019,
	title = {Are {Fictional} {Voices} {Distinguishable}? {Classifying} {Character} {Voices} in {Modern} {Drama}},
	doi = {10.18653/v1/w19-2504},
	abstract = {According to the literary theory of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use text classification to determine which authors best achieve dialogism, looking at a corpus of plays from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters' utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights.},
	author = {Vishnubhotla, Krishnapriya and Hammond, Adam and Hirst, Graeme},
	year = {2019},
	pages = {29--34},
}

@article{glass_hierarchical_2006,
	title = {Hierarchical rule generalisation for speaker identification in fiction books},
	volume = {204},
	doi = {10.1145/1216262.1216266},
	abstract = {This paper presents a hierarchical pattern matching and generalisation technique which is applied to the problem of locating the correct speaker of quoted speech found in fiction books. Patterns from a training set are generalised to create a small number of rules, which can be used to identify items of interest within the text. The pattern matching technique is applied to finding the Speech-Verb, Actor and Speaker of quotes found in fiction books. The technique performs well over the training data, resulting in rule-sets many times smaller than the training set, but providing very high accuracy. While the rule-set generalised from one book is less effective when applied to different books than an approach based on hand coded heuristics, performance is comparable when testing on data closely related to the training set. © 2006 SAICSIT.},
	journal = {ACM International Conference Proceeding Series},
	author = {Glass, Kevin and Bangay, Shaun},
	year = {2006},
	note = {ISBN: 1595935673},
	keywords = {Generalisation, Machine learning, Pattern matching},
	pages = {31--40},
}

@article{glass_naive_2007,
	title = {A naive, salience-based method for speaker identification in fiction books},
	abstract = {This paper presents a salience-based technique for the annotation of directly quoted speech from fiction text. In particular, this paper determines to what extent a naïve (without the use of complex machine learning or knowledge-based techniques) scoring technique can be used for the identification of the speaker of speech quotes. The presented technique makes use of a scoring technique, similar to that commonly found in knowledge-poor anaphora resolution research, as well as a set of hand-coded rules for the final identification of the speaker of each quote in the text. Speaker identification is shown to be achieved using three tasks: the identification of a speech-verb associated with a quote with a recall of 94.41\%; the identification of the actor associated with a quote with a recall of 88.22\%; and the selection of a speaker with an accuracy of 79.40\%.},
	journal = {Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA’07)},
	author = {Glass, Kevin and Bangay, Shaun},
	year = {2007},
	pages = {1--6},
}

@article{Keefe2013,
	title = {Examining the {Impact} of {Coreference} {Resolution} on {Quote} {Attribution}},
	abstract = {Quote attribution is the task of identifying the speaker of each quote within a document. While recent research has es- tablished large-scale corpora for this task, these corpora are not yet consistent in the way they handle candidate speakers, and many of the reported results rely on gold standard annotations of both entities and coreference chains. In this work we evaluate three quote at- tribution systems with automatically produced candidate speakers and coreference chains. We perform these experiments over four separate corpora, which allows us to determine how coreference resolution effects quote attribution, and to use the task as an extrinsic evaluation of three coreference systems.},
	journal = {Proc. ALTA 2013},
	author = {Keefe, Tim O and Webster, Kellie and Curran, James R},
	year = {2013},
	keywords = {coreference resolution, quote attribution},
	pages = {43--52},
}

@inproceedings{sun_mitigating_2019,
	title = {Mitigating {Gender} {Bias} in {Natural} {Language} {Processing}: {Literature} {Review}},
	doi = {10.18653/v1/p19-1159},
	abstract = {As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} ofthe {Association} for {Computational} {Linguistics}},
	author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
	year = {2019},
	note = {arXiv: 1906.08976},
	pages = {1630--1640},
}

@inproceedings{Elson2010,
	title = {Automatic attribution of quoted speech in literary narrative},
	isbn = {978-1-57735-465-9},
	abstract = {We describe a method for identifying the speakers of quoted speech in natural-language textual stories. We have assembled a corpus of more than 3,000 quotations, whose speakers (if any) are manually identified, from a collection of 19th and 20th century literature by six authors. Using rule-based and statistical learning, our method identifies candidate characters, determines their genders, and attributes each quote to the most likely speaker. We divide the quotes into syntactic classes in order to leverage common discourse patterns, which enable rapid attribution for many quotes. We apply learning algorithms to the remainder and achieve an overall accuracy of 83\%. Copyright © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). AU rights reserved.},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Elson, David K. and McKeown, Kathleen R.},
	year = {2010},
	keywords = {Technical Papers -- Natural-Language Processing},
	pages = {1013--1019},
}

@article{Iyyer2016,
	title = {Feuding {Families} and {Former} {Friends}: {Unsupervised} {Learning} for {Dynamic} {Fictional} {Relationships}},
	doi = {10.18653/v1/n16-1180},
	abstract = {Understanding how a fictional relationship between two characters changes over time (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship. We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories. While previous work on characterizing literary relationships relies on plot summaries annotated with predefined labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels. We find that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness). Our model outperforms topic model baselines on two crowdsourced tasks, and we also find interesting correlations to annotations in an existing dataset.},
	author = {Iyyer, Mohit and Guha, Anupam and Chaturvedi, Snigdha and Boyd-Graber, Jordan and Daumé III, Hal},
	year = {2016},
	pages = {1534--1544},
}

@inproceedings{Bamman2019,
	title = {An annotated dataset of literary entities},
	volume = {1},
	isbn = {978-1-950737-13-0},
	doi = {10.18653/v1/n19-1220},
	abstract = {We present a new dataset comprised of 210,532 tokens evenly drawn from 100 different English-language literary texts annotated for ACE entity categories (person, location, geo-political entity, facility, organization, and vehicle). These categories include non-named entities (such as “the boy”, “the kitchen”) and nested structure (such as [[the cook]'s sister]). In contrast to existing datasets built primarily on news (focused on geopolitical entities and organizations), literary texts offer strikingly different distributions of entity categories, with much stronger emphasis on people and description of settings. We present empirical results demonstrating the performance of nested entity recognition models in this domain; training natively on in-domain literary data yields an improvement of over 20 absolute points in F-score (from 45.7 to 68.3), and mitigates a disparate impact in performance for male and female entities present in models trained on news data.},
	booktitle = {{NAACL} {HLT} 2019 - 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Proceedings} of the {Conference}},
	author = {Bamman, David and Popat, Sejal and Shen, Sheng},
	year = {2019},
	pages = {2138--2144},
}

@article{vala_mr_2015,
	title = {Mr. {Bennet}, his coachman, and the {Archbishop} walk into a bar but only one of them gets recognized: {On} {The} {Difficulty} of {Detecting} {Characters} in {Literary} {Texts}},
	doi = {10.18653/v1/d15-1088},
	abstract = {Characters are fundamental to literary analysis. Current approaches are heav-ily reliant on NER to identify characters, causing many to be overlooked. We pro-pose a novel technique for character detec-tion, achieving significant improvements over state of the art on multiple datasets.},
	number = {September},
	author = {Vala, Hardik and Jurgens, David and Piper, Andrew and Ruths, Derek},
	year = {2015},
	pages = {769--774},
}

@article{holmes_cross-racial_2006,
	title = {Cross-racial voicing: {Carl} {Van} {Vechten}'s imagination and the search for an {African} {American} ethos},
	volume = {68},
	issn = {00100994},
	doi = {10.2307/25472153},
	number = {3},
	journal = {College English},
	author = {Holmes, David G.},
	year = {2006},
	pages = {291--307},
}

@article{Granger1969,
	title = {Investigating {Causal} {Relations} by {Econometric} {Models} and {Cross}-spectral {Methods} {Authors} ( s ): {C} . {W} . {J} . {Granger} {Published} by : {The} {Econometric} {Society} {Stable} {URL} : http://www.jstor.org/stable/1912791 {Accessed} : 25-03-2016 19 : 26 {UTC} {Your} use of the {JS}},
	volume = {37},
	abstract = {There occurs on some occasions a difficulty in deciding the direction of causality between two related variables and also whether or not feedback is occurring. Testable definitions of causality and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous causality is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible causal variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single causal arm of a feedback situation. Measures of causal lag and causal strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested. 1.},
	number = {3},
	journal = {Econometrica},
	author = {Granger, Clive J. W.},
	year = {1969},
	pages = {424--438},
}

@inproceedings{Field2018,
	title = {Framing and {Agenda}-setting in {Russian} {News}: a {Computational} {Analysis} of {Intricate} {Political} {Strategies}},
	url = {http://arxiv.org/abs/1808.09386},
	abstract = {Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and "fake news'". Here, we draw on two concepts from the political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.},
	booktitle = {Proceedings ofthe 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Field, Anjalie and Kliger, Doron and Wintner, Shuly and Pan, Jennifer and Jurafsky, Dan and Tsvetkov, Yulia},
	year = {2018},
	note = {arXiv: 1808.09386},
	pages = {3570--3580},
}

@article{wiseman_learning_2015,
	title = {Learning anaphoricity and antecedent ranking features for coreference resolution},
	volume = {1},
	doi = {10.3115/v1/p15-1137},
	abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-Training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date.},
	journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
	author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M. and Weston, Jason},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1416--1426},
}

@article{baumgartner_craft_2019,
	title = {{CRAFT} {Shared} {Tasks} 2019 {Overview} –- {Integrated} {Structure}, {Semantics}, and {Coreference}},
	doi = {10.18653/v1/d19-5725},
	abstract = {As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks-dependency parse construction, coref-erence resolution, and ontology concept identification over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task fo-cuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks.},
	author = {Baumgartner, William and Bada, Michael and Pyysalo, Sampo and Ciosici, Manuel R. and Hailu, Negacy and Pielke-Lombardo, Harrison and Regan, Michael and Hunter, Lawrence},
	year = {2019},
	pages = {174--184},
}

@article{sukthanker_anaphora_2020,
	title = {Anaphora and coreference resolution: {A} review},
	volume = {59},
	issn = {15662535},
	url = {https://doi.org/10.1016/j.inffus.2020.01.010},
	doi = {10.1016/j.inffus.2020.01.010},
	abstract = {Coreference resolution aims at resolving repeated references to an object in a document and forms a core component of natural language processing (NLP) research. When used as a component in the processing pipeline of other NLP fields like machine translation, sentiment analysis, paraphrase detection, and summarization, coreference resolution has a potential to highly improve accuracy. A direction of research closely related to coreference resolution is anaphora resolution. Existing literature is often ambiguous in its usage of these terms and often uses them interchangeably. Through this review article, we clarify the scope of these two tasks. We also carry out a detailed analysis of the datasets, evaluation metrics and research methods that have been adopted to tackle these NLP problems. This survey is motivated by the aim of providing readers with a clear understanding of what constitutes these two tasks in NLP research and their related issues.},
	number = {February},
	journal = {Information Fusion},
	author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
	year = {2020},
	note = {arXiv: 1805.11824
Publisher: Elsevier B.V.},
	keywords = {Anaphora resolution, Coreference resolution, Deep learning, Natural language processing, Sentiment analysis},
	pages = {139--162},
}

@article{trieu_coreference_2019,
	title = {Coreference {Resolution} in {Full} {Text} {Articles} with {BERT} and {Syntax}-based {Mention} {Filtering}},
	doi = {10.18653/v1/d19-5727},
	abstract = {This paper describes our system developed for the coreference resolution task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than other existing corpora because it contains full text articles. We have employed an existing span-based state-of-the-art neural coreference resolution system as a baseline system. We enhance the system with two different techniques to capture long-distance coreferent pairs. Firstly, we filter noisy mentions based on parse trees with increasing the number of antecedent candidates. Secondly, instead of relying on the LSTMs, we integrate the highly expressive language model-BERT into our model. Experimental results show that our proposed systems significantly outperform the baseline. The best performing system obtained F-scores of 44\%, 48\%, 39\%, 49\%, 40\%, and 57\% on the test set with B 3 , BLANC, CEAFE, CEAFM, LEA, and MUC metrics, respectively. Additionally, the proposed model is able to detect coreferent pairs in long distances, even with a distance of more than 200 sentences.},
	author = {Trieu, Hai-Long and Duong Nguyen, Anh-Khoa and Nguyen, Nhung and Miwa, Makoto and Takamura, Hiroya and Ananiadou, Sophia},
	year = {2019},
	pages = {196--205},
}

@article{he_identification_2013,
	title = {Identification of speakers in novels},
	volume = {1},
	abstract = {Speaker identification is the task of attributing utterances to characters in a literary narrative. It is challenging to automate because the speakers of the majority of utterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem. © 2013 Association for Computational Linguistics.},
	journal = {ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {He, Hua and Barbosa, Denilson and Kondrak, Grzegorz},
	year = {2013},
	note = {ISBN: 9781937284503},
	pages = {1312--1320},
}

@article{Seering2017,
	title = {Shaping pro and anti-social behavior on twitch through moderation and example-setting},
	doi = {10.1145/2998181.2998277},
	abstract = {Online communities have the potential to be supportive, cruel, or anywhere in between. The development of positive norms for interaction can help users build bonds, grow, and learn. Using millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors. Consistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users. Proactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors. This work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies.},
	journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
	author = {Seering, Joseph and Kraut, Robert E. and Dabbish, Laura},
	year = {2017},
	note = {ISBN: 9781450343350},
	keywords = {Authority and imitation, Chatroom behavior, Moderation strategies},
	pages = {111--125},
}

@article{joshi_bert_2019,
	title = {{BERT} for {Coreference} {Resolution}: {Baselines} and {Analysis}},
	url = {http://arxiv.org/abs/1908.09091},
	abstract = {We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available.},
	author = {Joshi, Mandar and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
	year = {2019},
	note = {arXiv: 1908.09091},
}

@article{mcnally_monsters_2011,
	title = {Monsters of the {Market}},
	doi = {10.1163/ej.9789004201576.i-296},
	abstract = {"Monsters of the Market" investigates modern capitalism through the prism of the body panics it arouses. Examining "Frankenstein," Marx s "Capital" and zombie fables from sub-Saharan Africa, it offers a novel account of the cultural and corporeal economy of global capitalism.},
	journal = {Monsters of the Market},
	author = {McNally, David},
	year = {2011},
}

@article{boggust_embedding_2019,
	title = {Embedding comparator: {Visualizing} differences in global structure and local neighborhoods via small multiples},
	abstract = {Embeddings- mappings from high-dimensional discrete input to lower-dimensional continuous vector spaces- have been widely adopted in machine learning, linguistics, and computational biology as they often surface interesting and unexpected domain semantics. Through semi-structured interviews with embedding model researchers and practitioners, we find that current tools poorly support a central concern: comparing different embeddings when developing fairer, more robust models. In response, we present the Embedding Comparator, an interactive system that balances gaining an overview of the embedding spaces with making fine-grained comparisons of local neighborhoods. For a pair of models, we compute the similarity of the k-nearest neighbors of every embedded object, and visualize the results as Local Neighborhood Dominoes: small multiples that facilitate rapid comparisons. Using case studies, we illustrate the types of insights the Embedding Comparator reveals including how fine-tuning embeddings changes semantics, how language changes over time, and how training data differences affect two seemingly similar models.},
	journal = {arXiv},
	author = {Boggust, Angie and Carter, Brandon and Satyanarayan, Arvind},
	year = {2019},
	note = {arXiv: 1912.04853},
	keywords = {Embedding spaces, Interactive, Machine learning, Small multiples, Visualization system},
}

@article{wanyan_attribute2vec_2020,
	title = {Attribute2vec: {Deep} network embedding through multi-filtering {GCN}},
	abstract = {We present a multi-filtering Graph Convolution Neural Network (GCN) framework for network embedding task. It uses multiple local GCN filters to do feature extraction in every propagation layer. We show this approach could capture different important aspects of node features against the existing attribute embedding based method. We also show that with multi-filtering GCN approach, we can achieve significant improvement against baseline methods when training data is limited. We also perform many empirical experiments and demonstrate the benefit of using multiple filters against single filter as well as most current existing network embedding methods for both the link prediction and node classification tasks.
MSC Codes 68T99},
	journal = {arXiv},
	author = {Wanyan, Tingyi and Zhang, Chenwei and Azad, Ariful and Liang, Xiaomin and Li, Daifeng and Ding, Ying},
	year = {2020},
	note = {arXiv: 2004.01375},
}

@book{Marche2019,
	title = {Sexuality, {Subjectivity}, and {LGBTQ} {Militancy} in the {United} {States}},
	isbn = {978-90-8964-960-7},
	author = {Marche, Guillaume},
	year = {2019},
	doi = {10.5117/9789089649607},
	note = {Publication Title: Sexuality, Subjectivity, and LGBTQ Militancy in the United States},
}

@article{choi_moving_2020,
	title = {Moving for the movement: {Applying} viewpoints and composition techniques to the design of online social justice campaigns},
	doi = {10.1145/3357236.3395435},
	abstract = {By leveraging approaches from other disciplines, designers can expand the boundaries of interaction design to tackle complex socio-technical problems. To address the challenges of networked social justice movements, we developed a workshop for designers and social justice activists based in Viewpoints and Composition, a philosophy and set of techniques for the theatre. Building on other experience prototyping and somatic methods, the workshop leads participants through the design of a hypothetical internet-enabled social justice campaign, encouraging them to imagine the felt-experience of networked social justice movement building in a socio-spatial context. We conclude with insights from the workshop and plans to further develop these techniques.},
	journal = {DIS 2020 - Proceedings of the 2020 ACM Designing Interactive Systems Conference},
	author = {Choi, Judeth Oden and Hammer, Jessica and Royal, Jon and Forlizzi, Jodi},
	year = {2020},
	note = {ISBN: 9781450369749},
	keywords = {Activism, Embodied design methods, Hci, Interaction design, Social justice},
	pages = {75--86},
}

@inproceedings{Choi2020,
	title = {Identity-{Based} {Roles} in {Rhizomatic} {Social} {Justice} {Movements} on {Twitter}},
	volume = {14},
	url = {https://www.aaai.org/ojs/index.php/ICWSM/article/view/7317},
	abstract = {Contemporary social justice movements can be understood as rhizomatic, growing laterally without a central structure. In this mixed methods study, we investigated the roles that activists develop based on their personal and professional identities and carry with them through the dynamic landscape of rhizomatic social justice movements on Twitter. We conducted interviews with self-identified social justice activists and analyzed seven weeks of their Twitter timeline and retweets. We found three activist roles–organizer, storyteller and advocate–and described the identities, approaches to activism, behaviors on Twitter, and the relationship to social justice movements for each role. We used these roles as a lens to better understand how movement identities are constructed, laid out an agenda for future research on roles in rhizomatic social justice movements and suggested design directions.},
	booktitle = {Proceedings of the {Fourteenth} {International} {AAAI} {Conference} on {Web} and {Social} {Media}},
	author = {Choi, Judeth Oden and Herbsleb, James and Hammer, Jessica and Forlizzi, Jodi},
	year = {2020},
	keywords = {ICWSM Full Papers},
	pages = {488--498},
}

@article{newman-griffis_jointly_2018,
	title = {Jointly embedding entities and text with distant supervision},
	doi = {10.18653/v1/w18-3026},
	abstract = {Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of mappings between entities and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that entities and words capture complementary information that can be effectively combined for downstream use.},
	journal = {arXiv},
	author = {Newman-Griffis, Denis and Lai, Albert M. and Fosler-Lussier, Eric},
	year = {2018},
	note = {arXiv: 1807.03399},
	pages = {195--206},
}

@article{chang_divergence_2020,
	title = {Divergence and the {Complexity} of {Difference} in {Text} and {Culture}},
	doi = {10.22148/001c.17585},
	abstract = {Measuring how much two documents differ is a basic task in the quantitative analysis of text. Because difference is a complex, interpretive concept, researchers often operationalize difference as distance, a mathematical function that represents documents through a metaphor of physical space. Yet the constraints of that metaphor mean that distance can only capture some of the ways that documents can relate to each other. We show how a more general concept, divergence, can help solve this problem, alerting us to new ways in which documents can relate to each other. In contrast to distance, divergence can capture enclosure relationships, where two documents differ because the patterns found in one are a partial subset of those in the other, and the emergence of shortcuts, where two documents can be brought closer through mediation by a third. We provide an example of this difference measure, Kullback-Leibler Divergence, and apply it to two worked examples: the presentation of scientific arguments in Charles Darwin's Origin of Species (1859) and the rhetorical structure of philosophical texts by Aristotle, David Hume, and Immanuel Kant. These examples illuminate the complex relationship between time and what we refer to as an archive's "enclosure architecture", and show how divergence can be used in the quantitative analysis of historical, literary, and cultural texts to reveal cognitive structures invisible to spatial metaphors. Those who study culture look for differences. Just as ethnographers might study the differences between the practices of regions, villages, or families, 1 literary scholars might study the differences between genres, modes, or periods. 2 Those who approach culture from a quantitative standpoint are no exception , and we are often tasked with the goal of measuring the differences between different, computationally identified, patterns of expression. 3 In the digital analysis of texts, for example, we might ask how much two documents or sets of documents differ, and relate these differences to other aspects of psychological or social life. 4},
	journal = {Journal of Cultural Analytics},
	author = {Chang, Kent K. and DeDeo, Simon},
	year = {2020},
}

@article{green_data_2020,
	title = {Data {Science} as {Political} {Action} {Grounding} {Data} {Science} in a {Politics} of {Justice}},
	doi = {10.2139/ssrn.3658431},
	abstract = {In response to numerous recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics (by which I mean not simply debates about specific political parties and candidates but more broadly the collective social processes that influence rights, status, and resources across society). Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people’s lives.},
	journal = {arXiv},
	author = {Green, Ben},
	year = {2020},
	note = {arXiv: 1811.03435},
}

@inproceedings{Demszky2019,
	title = {Analyzing {Polarization} in {Social} {Media}: {Method} and {Application} to {Tweets} on 21 {Mass} {Shootings}},
	url = {http://arxiv.org/abs/1904.01596},
	abstract = {We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms "terrorist" and "crazy", that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Demszky, Dorottya and Garg, Nikhil and Voigt, Rob and Zou, James and Gentzkow, Matthew and Shapiro, Jesse and Jurafsky, Dan},
	year = {2019},
	note = {arXiv: 1904.01596},
	pages = {2970--3005},
}

@article{zhang_policykit_2020,
	title = {{PolicyKit}: {Building} {Governance} in {Online} {Communities}},
	doi = {10.1145/3379337.3415858},
	abstract = {The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members' opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia's Request for Adminship (RfA) process.},
	author = {Zhang, Amy X. and Hugh, Grant and Bernstein, Michael S.},
	year = {2020},
	note = {arXiv: 2008.04236
ISBN: 9781450375146},
	pages = {365--378},
}

@article{schaaf_analysis_2010,
	title = {Analysis of gender normalization using {MLP} and {VTLN} features},
	abstract = {This paper analyzes the capability of multilayer perceptron frontends to perform speaker normalization. We find the context decision tree to be a very useful tool to assess the speaker normalization power of different frontends. We introduce a gender question into the training of the phonetic context decision tree. After the context clustering the gender specific models are counted. We compare this for the following frontends: (1) Bottle-Neck (BN) with and without vocal tract length normalization (VTLN), (2) standard MFCC, (3) stacking of multiple MFCC frames with linear discriminant analysis (LDA). We find the BN-frontend to be even more effective in reducing the number of gender questions than VTLN. From this we conclude that a Bottle-Neck frontend is more effective for gender normalization. Combining VTLN and BN-features reduces the number of gender specific models further. © 2010 ISCA.},
	journal = {Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
	author = {Schaaf, Thomas and Metze, Florian},
	year = {2010},
	keywords = {Phonetic context tree, Speaker normalization, Speech recognition},
	pages = {306--309},
}

@article{nallasamy_normalization_2011,
	title = {Normalization of {Gender} , {Dialect} and {Speaking} style using {Probabilistic} front-ends},
	journal = {Ratio},
	author = {Nallasamy, Udhyakumar and Metze, Florian and Schaaf, Thomas},
	year = {2011},
	pages = {225--226},
}

@article{nallasamy_analysis_2011,
	title = {Analysis of dialectal influence in pan-{Arabic} {ASR}},
	issn = {19909772},
	abstract = {In this paper, we analyze the impact of five Arabic dialects on the front-end and pronunciation dictionary components of an Automatic Speech Recognition (ASR) system. We use ASR's phonetic decision tree as a diagnostic tool to compare the robustness of MFCC and MLP front-ends to dialectal variations in the speech data and found that MLP Bottle-Neck features are less robust to such variations. We also perform a rule-based analysis of the pronunciation dictionary, which enables us to identify dialectal words in the vocabulary and automatically generate pronunciations for unseen words. We show that our technique produces pronunciations with an average phone error rate 9.2\%. Copyright © 2011 ISCA.},
	number = {August},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	author = {Nallasamy, Udhyakumar and Garbus, Michael and Metze, Florian and Jin, Qin and Schaaf, Thomas and Schultz, Tanja},
	year = {2011},
	keywords = {Automatic speech recognition, Dialect analysis, Front-end evaluation},
	pages = {1721--1724},
}

@article{jung_posterior_2020,
	title = {Posterior {Calibrated} {Training} on {Sentence} {Classification} {Tasks}},
	doi = {10.18653/v1/2020.acl-main.242},
	abstract = {Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65\% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5\% of task performance gain and 16.1\% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2\% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.},
	author = {Jung, Taehee and Kang, Dongyeop and Cheng, Hua and Mentch, Lucas and Schaaf, Thomas},
	year = {2020},
	note = {arXiv: 2004.14500},
	pages = {2723--2730},
}

@inproceedings{Fiesler2016,
	title = {An {Archive} of {Their} {Own}},
	isbn = {978-1-4503-3362-7},
	url = {http://dl.acm.org/citation.cfm?doid=2858036.2858409},
	doi = {10.1145/2858036.2858409},
	abstract = {Rarely are computing systems developed entirely by members of the communities they serve, particularly when that community is underrepresented in computing. Archive of Our Own (AO3), a fan fiction archive with nearly 750,000 users and over 2 million individual works, was designed and coded primarily by women to meet the needs of the online fandom community. Their design decisions were informed by existing values and norms around issues such as accessibility, inclusivity, and identity. We conducted interviews with 28 users and developers, and with this data we detail the history and design of AO3 using the framework of feminist HCI and focusing on the successful incorporation of values into design. We conclude with considering examples of complexity in values in design work: the use of design to mitigate tensions in values and to influence value formation or change.},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '16},
	author = {Fiesler, Casey and Morrison, Shannon and Bruckman, Amy S.},
	year = {2016},
	pages = {2574--2585},
}

@article{jenkins_cultural_2015,
	title = {"{Cultural} {Acupuncture}": {Fan} {Activism} and the {Harry} {Potter} {Alliance}},
	doi = {10.1057/9781137350374_11},
	journal = {Popular Media Cultures: Fans, Audiences and Paratexts},
	author = {Jenkins, Henry},
	year = {2015},
	note = {ISBN: 9781137350374},
	pages = {206--229},
}

@article{Navar-Gill2018,
	title = {“{We} {Shouldn}'t {Have} to {Trend} to {Make} {You} {Listen}”: {Queer} {Fan} {Hashtag} {Campaigns} as {Production} {Interventions}},
	volume = {70},
	journal = {Journal of Film and Video},
	author = {Navar-Gill, Annemarie and Stanfill, Mel},
	year = {2018},
	pages = {85--100},
}

@article{Evans2017,
	title = {More {Than} {Peer} {Production}: {Fanfiction} {Communities} as {Sites} of {Distributed} {Mentoring}},
	doi = {10.1145/2998181.2998342},
	abstract = {From Harry Potter to American Horror Story, fanfiction is extremely popular among young people. Sites such as Fanfiction.net host millions of stories, with thousands more posted each day. Enthusiasts are sharing their writing and reading stories written by others. Exactly how does a generation known more for videogame expertise than long-form writing become so engaged in reading and writing in these communities? Via a nine-month ethnographic investigation of fanfiction communities that included participant observation, interviews, a thematic analysis of 4,500 reader reviews and an in-depth case study of a discussion group, we found that members of fanfiction communities spontaneously mentor each other in open forums, and that this mentoring builds upon previous interactions in a way that is distinct from traditional forms of mentoring and made possible by the affordances of networked publics. This work extends and develops the theory of distributed mentoring. Our findings illustrate how distributed mentoring supports fanfiction authors as they work to develop their writing skills. We believe distributed mentoring holds potential for supporting learning in a variety of formal and informal learning environments.},
	journal = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
	author = {Evans, Sarah and Davis, Katie and Evans, Abigail and Campbell, Julie Ann and Randall, David P and Yin, Kodlee and Aragon, Cecilia},
	year = {2017},
	note = {ISBN: 9781450343350},
	keywords = {Author Keywords Mentoring, Web-based interaction, distributed mentoring, fanfiction, informal learning, online communities},
	pages = {259--272},
}

@article{moore_distribution_2013,
	title = {Distribution {Is} {Queen}: {LGBTQ} {Media} on {Demand}},
	volume = {53},
	number = {1},
	author = {Moore, Candace and Moore, Candace},
	year = {2013},
	pages = {137--144},
}

@article{craig_you_2014,
	title = {You {Can} {Form} a {Part} of {Yourself} {Online}: {The} {Influence} of {New} {Media} on {Identity} {Development} and {Coming} {Out} for {LGBTQ} {Youth}},
	volume = {18},
	issn = {19359713},
	doi = {10.1080/19359705.2013.777007},
	abstract = {Internet-based new media are increasingly utilized by lesbian, gay, bisexual, transgender, and queer (LGBTQ) youth, yet little is known about the ways in which it influences their identity development. Employing grounded theory, this study explores the influence of online media on the identity development and coming out processes (n = 19) of LGBTQ youth. Results indicate that new media enabled participants to access resources, explore identity, find likeness, and digitally engage in coming out. Participants also discussed the expansion of these newly developed identities into their offline lives. Practice implications are addressed. © 2014 Copyright Taylor and Francis Group, LLC.},
	number = {1},
	journal = {Journal of Gay and Lesbian Mental Health},
	author = {Craig, Shelley L. and McInroy, Lauren},
	year = {2014},
	keywords = {LGBTQ, coming out, identity, media, youth},
	pages = {95--109},
}

@article{sereda_dirty_nodate,
	title = {"{Dirty} {Stories} {Saved} {My} {Life}": {Fanfiction} as a {Source} of {Emotional} {Support}},
	author = {Sereda, Anastasiia},
}

@article{mcinroy_transgender_2015,
	title = {Transgender {Representation} in {Offline} and {Online} {Media}: {LGBTQ} {Youth} {Perspectives}},
	volume = {25},
	issn = {15403556},
	doi = {10.1080/10911359.2014.995392},
	abstract = {Transgender people are increasingly depicted in both offline and online media. These representations inform the general public about transgender communities and have a significant impact on transgender young peoples’ identity development and lived experiences. However, despite increasing awareness of this representation, a lack of research persists on the perspectives of lesbian, gay, bisexual, transgender, and queer (LGBTQ) youth regarding depictions of transgender people in contemporary media. This is despite the fact that this population may be particularly well positioned to consider the impact of these representations on themselves and their peers in the LGBTQ community. In this article the trends in contemporary media representations of transgender individuals are described, the perspectives of LGBTQ youth (N = 19) regarding these messages are examined, and the particular perspectives of transgender youth participants (n = 4) are explored. Clinical implications for social work practice are considered, including the potential usefulness of an ecological framework in considering the media engagement of LGBTQ young people.},
	number = {6},
	journal = {Journal of Human Behavior in the Social Environment},
	author = {McInroy, Lauren B. and Craig, Shelley L.},
	year = {2015},
	keywords = {LGBTQ, Transgender, ecological theory, media, online, youth},
	pages = {606--617},
}

@article{belinkov_dont_2020,
	title = {Don't take the premise for granted: {Mitigating} artifacts in natural language inference},
	doi = {10.18653/v1/p19-1084},
	abstract = {Natural Language Inference (NLI) datasets often contain hypothesis-only biases-artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.},
	journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {Belinkov, Yonatan and Poliak, Adam and Shieber, Stuart M. and van Durme, Benjamin and Rush, Alexander M.},
	year = {2020},
	note = {arXiv: 1907.04380
ISBN: 9781950737482},
	pages = {877--891},
}

@article{zhang_ordinal_2017,
	title = {Ordinal {Common}-sense {Inference}},
	volume = {5},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhang, Sheng and Rudinger, Rachel and Duh, Kevin and Van Durme, Benjamin},
	year = {2017},
	pages = {379--395},
}

@article{poliak_collecting_2020,
	title = {Collecting diverse natural language inference problems for sentence representation evaluation},
	doi = {10.18653/v1/w18-5441},
	abstract = {We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at http://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.},
	journal = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
	author = {Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Edward Hu, J. and Pavlick, Ellie and White, Aaron Steven and van Durme, Benjamin},
	year = {2020},
	note = {arXiv: 1804.08207
ISBN: 9781948087841},
	pages = {67--81},
}

@article{poliak_hypothesis_2018,
	title = {Hypothesis {Only} {Baselines} in {Natural} {Language} {Inference}},
	doi = {10.18653/v1/s18-2023},
	abstract = {We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on ten distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.},
	number = {1},
	author = {Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
	year = {2018},
	note = {arXiv: 1805.01042},
	pages = {180--191},
}

@inproceedings{rudinger_social_2017,
	title = {Social {Bias} in {Elicited} {Natural} {Language} {Inferences}},
	doi = {10.1093/intqhc/mzt079},
	abstract = {We analyze the Stanford Natural Lan-guage Inference (SNLI) corpus in an in-vestigation of bias and stereotyping in NLP data. The human-elicitation proto-col employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	author = {Rudinger, Rachel and May, Chandler and Van Durme, Benjamin},
	year = {2017},
	pmid = {24282154},
	note = {Issue: 2015
ISSN: 1464-3677},
	pages = {74--79},
}

@article{nguyen_queer_2015,
	title = {A queer learner’s identity positioning in second language classroom discourse},
	volume = {6},
	issn = {19463022},
	doi = {10.1080/19463014.2015.1093952},
	abstract = {This case study examines the classroom participation of a Korean queer (transgender) learner of English as a second language at a language institute for international adult students in the United States. To understand the dynamics of this learner’s participation, we focus on how she constructed gender identity and learner identity in interaction. Our analysis indicates that although the class content was not designed to elicit biographic information from students, this learner agentively managed her gender identity expression, which, at times, was met with challenge by her peers. As a second language learner, she self-positioned both as a lazy student and an effective language user – contradictory positionings that might be explained by the disconnection between the class content and the cultural capital that she sought to gain. Our study extends research on investment in second language learning by examining identity positioning in actual discourses and by linking classroom interaction and the learner’s experiences outside of the classroom. The analysis can also inform researchers and teachers about the complexities and nuances of gender identity construction and negotiation in classroom discourse.},
	number = {3},
	journal = {Classroom Discourse},
	author = {Nguyen, Hanhthi and Yang, Lajlim},
	year = {2015},
	keywords = {agency, identity positioning, queer, second language learning, transgender},
	pages = {221--241},
}

@article{moe_initial_2015,
	title = {Initial {Assessment} and {Screening} with {LGBTQ} {Clients}: {A} {Critical} {Perspective}},
	volume = {9},
	issn = {15538338},
	doi = {10.1080/15538605.2014.997332},
	abstract = {Counseling with people that identify as lesbian, gay, bisexual, transgender, queer (LGBTQ), or who are otherwise nonheterosexual or cisgender identified, should be based on a critical approach to assessment. Although general competencies have been articulated, further guidance is needed to help counselors avoid hetero-normative and cisgender biases in their assessment practice. The authors provide recommendations, based on critical review of the literature, for how counselors can address biases in assessment and screening tools in work with LGBTQ clients.},
	number = {1},
	journal = {Journal of LGBT Issues in Counseling},
	author = {Moe, Jeffry L. and Finnerty, Peter and Sparkman, Narketta and Yates, Chad},
	year = {2015},
	keywords = {LGBTQ issues, prejudice, queer theory, social justice, treatment},
	pages = {36--56},
}

@article{wagaman_self-definition_2016,
	title = {Self-definition as resistance: {Understanding} identities among {LGBTQ} emerging adults},
	volume = {13},
	issn = {19361661},
	doi = {10.1080/19361653.2016.1185760},
	abstract = {Scholars have questioned the relevance of existing identity categories and labels for lesbian, gay, bisexual, transgender, and queer (LGBTQ) youth and emerging adults. Little is understood, however, about the ways in which LGBTQ emerging adults perceive their own identities and self-define the aspects of themselves that are most relevant to who they are. This study qualitatively explored descriptions and depictions of identities and aspects of self among a group of LGBTQ-identified emerging adults. Findings suggest that emerging adults' uses of socially constructed identities are contextually specific, intersectional, and make use of agency. In addition, participants identified aspects of themselves that they saw as emerging from and existing in relation to their LGBTQ-specific identities. These identities are used to resist the social stigma and limitations placed on them as LGBTQ-identified young people, and may serve as sources of resilience.},
	number = {3},
	journal = {Journal of LGBT Youth},
	author = {Wagaman, M. Alex},
	year = {2016},
	keywords = {LGBTQ, emerging adults, identity, youth},
	pages = {207--230},
}

@article{dahl_interplay_2012,
	title = {The {Interplay} of {Sexual} and {Religious} {Identity} {Development} in {LGBTQ} {Adolescents} and {Young} {Adults}: {A} {Qualitative} {Inquiry}},
	volume = {12},
	issn = {15283488},
	doi = {10.1080/15283488.2012.691255},
	abstract = {Eight sexual-minority adolescents (15-18 years old) and 11 young adults (19-24 years old) participated in individual interviews, journal writing, and focus groups to provide greater insight into the interplay of religious and sexual identity development in lesbian, gay, bisexual, transgender, and queer youth. The majority of the sample identified as being raised in the Church of Jesus Christ of Latter-day Saints, two participants identified being raised Catholic, and one participant identified being raised Presbyterian. Participants described an early behavioral connection to their childhood religious faiths, reported feeling "different," and shared efforts to deny their same-sex attractions. Next, participants stated they questioned their faith, and whereas some participants described feeling disconnected religiously, others worked diligently to maintain connection with their faith communities. Participants also endorsed internalized conflict and efforts to change their attractions. Finally, participants disengaged with their childhood faiths, disclosed their sexual orientation to others, and redefined their values and beliefs. © 2012 Copyright Taylor and Francis Group, LLC.},
	number = {3},
	journal = {Identity},
	author = {Dahl, Angie and Galliher, Renee V.},
	year = {2012},
	pages = {217--246},
}

@article{downey_rhetorical_2017,
	title = {A {RHETORICAL} {ANALYSIS} {OF} {HEGEMONIC} {AND} {COUNTERHEGEMONIC} {PERFORMANCES} {IN} {WORLD} {WRESTLING} {ENTERTAINMENT} ( {WWE} ) {A} {THESIS} {Presented} to the {Department} of {Communication} {Studies} {California} {State} {University} , {Long} {Beach} {In} {Partial} {Fulfillment} of the {Requirements}},
	number = {August},
	author = {Downey, Sharon and Ph, D and Heyse, Amy and Ph, D and Pfister, Raven and Ph, D},
	year = {2017},
}

@article{coletta_missing_2018,
	title = {The missing {B} word: {Compulsory} binarization and bisexual representation in children's literature},
	volume = {10},
	issn = {1920261X},
	doi = {10.1353/jeu.2018.0004},
	abstract = {This project uses some LGBTQ youth's social media posts and Allison Weir's framework for identification-with identity politics to analyze and evaluate bisexual representation in YA literature. The essay posits that bisexual characters are often erased in literature because of common stereotypes and what the author calls “compulsory binarization,” or the assumption that a character is either gay or straight unless otherwise labelled. To combat this, the essay suggests that a superb bisexual YA novel first include a clear naming of bisexuality within the text; in addition, effective bisexual representation should challenge normative binaries and stereotypes.},
	number = {1},
	journal = {Jeunesse: Young People, Texts, Cultures},
	author = {Coletta, Jennifer},
	year = {2018},
	keywords = {Binary, Bisexual, Children's literature, Identity politics, LGBTQ},
	pages = {85--108},
}

@article{velickovic_deep_2019,
	title = {Deep graph infomax},
	abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs-both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
	journal = {7th International Conference on Learning Representations, ICLR 2019},
	author = {Veličković, Petar and Fedus, William and Hamilton, William L. and Bengio, Yoshua and Liò, Pietro and Devon Hjelm, R.},
	year = {2019},
	note = {arXiv: 1809.10341v2},
	pages = {1--17},
}

@inproceedings{Grover2016,
	title = {node2vec: {Scalable} {Feature} {Learning} for {Networks}},
	isbn = {978-1-4503-4232-2},
	url = {http://arxiv.org/abs/1607.00653},
	abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	note = {arXiv: 1607.00653},
	keywords = {dings, feature learning, graph representations, information networks, node embed-},
	pages = {855--864},
}

@article{blodgett_twitter_2018,
	title = {Twitter universal dependency parsing for {African}-{American} and mainstream {American} {English}},
	volume = {1},
	doi = {10.18653/v1/p18-1131},
	abstract = {Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods' impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: http://slanglab.cs.umass.edu/TwitterAAE/.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Blodgett, Su Lin and Wei, Johnny Tian Zheng and O'Connor, Brendan},
	year = {2018},
	note = {ISBN: 9781948087322},
	pages = {1415--1425},
}

@article{vashishth_tutorial_2019,
	title = {A {Tutorial} on {Graph} {Neural} {Networks} for {Natural} {Language} {Processing}},
	author = {Vashishth, Shikhar and Naganand, Y and Talukdar, Partha},
	year = {2019},
}

@article{mohammad_gender_2020,
	title = {Gender {Gap} in {Natural} {Language} {Processing} {Research}: {Disparities} in {Authorship} and {Citations}},
	doi = {10.18653/v1/2020.acl-main.702},
	abstract = {Disparities in authorship and citations across gender can have substantial adverse consequences not just on the disadvantaged genders, but also on the field of study as a whole. Measuring gender gaps is a crucial step towards addressing them. In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing (1965 to 2019). We determine aggregate-level statistics using an existing manually curated author--gender list as well as first names strongly associated with a gender. We find that only about 29\% of first authors are female and only about 25\% of last authors are female. Notably, this percentage has not improved since the mid 2000s. We also show that, on average, female first authors are cited less than male first authors, even when controlling for experience and area of research. Finally, we discuss the ethical considerations involved in automatic demographic analysis.},
	author = {Mohammad, Saif M.},
	year = {2020},
	note = {arXiv: 2005.00962},
	pages = {7860--7870},
}

@article{ramakrishna_quantitative_2015,
	title = {A quantitative analysis of gender differences in movies using psycholinguistic normatives},
	doi = {10.18653/v1/d15-1234},
	abstract = {Direct content analysis reveals important details about movies including those of gender representations and potential biases. We investigate the differences between male and female character depictions in movies, based on patterns of language used. Specifically, we use an automatically generated lexicon of linguistic norms characterizing gender ladenness. We use multivariate analysis to investigate gender depictions and correlate them with elements of movie production. The proposed metric differentiates between male and female utterances and exhibits some interesting interactions with movie genres and the screenplay writer gender..},
	number = {September},
	journal = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
	author = {Ramakrishna, Anil and Malandrakis, Nikolaos and Staruk, Elizabeth and Narayanan, Shrikanth},
	year = {2015},
	note = {ISBN: 9781941643327},
	pages = {1996--2001},
}

@article{qian_reducing_2019,
	title = {Reducing {Gender} {Bias} in {Word}-{Level} {Language} {Models} with a {Gender}-{Equalizing} {Loss} {Function}},
	doi = {10.18653/v1/p19-2031},
	abstract = {Gender bias exists in natural language datasets which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity by much. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach, and show that it outperforms existing strategies in all bias evaluation metrics.},
	number = {2016},
	journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Student Research Workshop},
	author = {Qian, Yusu and Muaz, Urwa and Zhang, Ben and Hyun, Jae Won},
	year = {2019},
	note = {arXiv: 1905.12801
ISBN: 9781950737475},
	pages = {223--228},
}

@article{radford_theory_2020,
	title = {Theory {In}, {Theory} {Out}: {The} {Uses} of {Social} {Theory} in {Machine} {Learning} for {Social} {Science}},
	volume = {3},
	doi = {10.3389/fdata.2020.00018},
	abstract = {Research at the intersection of machine learning and the social sciences has provided critical new insights into social behavior. At the same time, a variety of critiques have been raised ranging from technical issues with the data used and features constructed, problematic assumptions built into models, their limited interpretability, and their contribution to bias and inequality. We argue such issues arise primarily because of the lack of social theory at various stages of the model building and analysis. In the first half of this paper, we walk through how social theory can be used to answer the basic methodological and interpretive questions that arise at each stage of the machine learning pipeline. In the second half, we show how theory can be used to assess and compare the quality of different social learning models, including interpreting, generalizing, and assessing the fairness of models. We believe this paper can act as a guide for computer and social scientists alike to navigate the substantive questions involved in applying the tools of machine learning to social data.},
	number = {May},
	journal = {Frontiers in Big Data},
	author = {Radford, Jason and Joseph, Kenneth},
	year = {2020},
	note = {arXiv: 2001.03203},
	keywords = {bias, computational social science, fairness, machine learning, machine learning and social, machine learning, computational social science, ma, science},
}

@incollection{McConnell-Ginet2003,
	title = {“{What}’s in a {Name}?” {Social} {Labeling} and {Gender} {Practices}},
	booktitle = {The handbook of language and gender},
	author = {McConnell-Ginet, Sally},
	year = {2003},
	pages = {69--97},
}

@article{kiesling_interactional_2011,
	title = {The interactional construction of desire as gender},
	volume = {5},
	issn = {17476321},
	doi = {10.1558/genl.v5i2.213},
	abstract = {Recently there have been spirited debates about the role of desire in the study of language, gender and sexuality – debates largely centred around the publication of Cameron and Kulick’s (2003) volume, Language and Sexuality. In the present paper, I address two important questions arising from this debate: defining evidence of desire in interaction, and the relationship of this ‘interactional desire’ to gender and masculinities more specifically. First, I argue for a definition of desire that goes beyond sexual desire, in a direction indicated by Cameron and Kulick; following Whitehead (2002:205–221), I suggest that another kind of desire that we should think about (in addition to interpersonal desire) is ontological desire – essentially the desire to have or emulate qualities of a particular identity. Second, I use the notion of alignment to discover interactants ‘doing’ desire of different types in interaction. Finally, I argue that these alignments and how they are accomplished are not a separate part of masculine (and other gender) constructions of performances, but are at the centre of such identities. I use three excerpts, selected for their likelihood to have desire as an issue in the speech event, to illustrate these points},
	number = {2},
	journal = {Gender and Language},
	author = {Kiesling, Scott},
	year = {2011},
	keywords = {alignment, desire, discourse, masculinities, sexuality},
	pages = {213--239},
}

@article{Bruce1981,
	title = {A social interaction model of reading},
	volume = {4},
	issn = {15326950},
	doi = {10.1080/01638538109544523},
	abstract = {An author and a reader are engaged in a social interaction which depends on their goals and their beliefs about the world and each other. One aspect of this interaction is the creation of another level of social interaction involving an “implied author” and an “implied reader. ” The newly created characters may, in their turn, create another level of social interaction involving, for example, a “narrator” and a “narratee.” Each level so created permits the creation of an additional level. A model for the levels of social interaction in reading is discussed in the paper. The model provides a framework for examining devices such as author commentary, irony, stories within stories, first person narration and point of view. Examples such as Benjamin Bunny and The Turn of the Screw are discussed. © 1981, Taylor \& Francis Group. All rights reserved.},
	number = {4},
	journal = {Discourse Processes},
	author = {Bruce, Bertram},
	year = {1981},
	pages = {273--311},
}

@article{de_beaugrande_narrative_1979,
	title = {Narrative models of action and interaction},
	volume = {3},
	issn = {03640213},
	doi = {10.1207/s15516709cog0301_3},
	abstract = {This paper explores some issues which a humanlike story system ought to encompass, but which are usually not in the main focus of narrative models since Propp. We argue that knowledge about actions and interactions can account not only for how stories are constructed, but also for why some stories are more interesting and enduring than others. We analyze a traditional English folktale in these terms, and show how classes of surface expressions make recoverable the underlying structures and dependencies that schema-based understanders comprise. © 1979.},
	number = {1},
	journal = {Cognitive Science},
	author = {de Beaugrande, Robert and Colby, Benjamin N.},
	year = {1979},
	pages = {43--66},
}

@article{Chen2019,
	title = {Unsupervised cluster analyses of character networks in fiction: {Community} structure and centrality},
	volume = {163},
	issn = {09507051},
	url = {https://doi.org/10.1016/j.knosys.2018.10.005},
	doi = {10.1016/j.knosys.2018.10.005},
	abstract = {We present an integrated approach to cluster and visualize character networks in fiction with the aid of computational and statistical methods. An unsupervised clustering algorithm, minimum span clustering (MSC), was applied to cluster fictional characters at various characteristic resolutions based on their activities in the novel. As a demonstration, we study the character network in Dream of the Red Chamber, the greatest novel in Chinese literature. The character network of the novel is found to exhibit properties of scale-free and small-world networks. Based on unsupervised cluster analyses, we construct and visualize the community structure of the network, and find a three-tiered structure of core, secondary, and peripheral characters. By treating the network as a weighted graph, we further analyze the centralities of characters to determine their importance in the network, and find that betweenness centrality, as a measure of characters’ control over the flow of the narrative, is differentiated from other centrality measures for Dream of the Red Chamber. We believe that these analytic methods provide beneficial tools for applications such as autonomous novel writing.},
	journal = {Knowledge-Based Systems},
	author = {Chen, Rex H.-G. and Chen, C. C. and Chen, Chi Ming J.},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Centrality measures, Community structure detection, Computer-aided visualization, Social network, Unsupervised clustering},
	pages = {800--810},
}

@article{Agarwal2013,
	title = {Automatic {Extraction} of {Social} {Networks} from {Literary} {Text}: {A} {Case} {Study} on {Alice} in {Wonderland}},
	abstract = {In this paper we present results for two tasks: social event detection and social network extraction from a literary text, Al-ice in Wonderland. For the first task, our system trained on a news corpus using tree kernels and support vector machines beats the baseline systems by a statisti-cally significant margin. Using this sys-tem we extract a social network from Al-ice in Wonderland. We show that while we achieve an F-measure of about 61\% on social event detection, our extracted un-weighted network is not statistically dis-tinguishable from the un-weighted gold network according to popularly used net-work measures.},
	number = {October},
	journal = {International Joint Conference on Natural Language Processing},
	author = {Agarwal, Apoorv and Kotalwar, Anup and Rambow, Owen},
	year = {2013},
	pages = {1202--1208},
}

@article{keith_text_2020,
	title = {Text and {Causal} {Inference}: {A} {Review} of {Using} {Text} to {Remove} {Confounding} from {Causal} {Estimates}},
	doi = {10.18653/v1/2020.acl-main.474},
	abstract = {Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual's entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders. Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent. This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.},
	author = {Keith, Katherine and Jensen, David and O’Connor, Brendan},
	year = {2020},
	note = {arXiv: 2005.00649},
	pages = {5332--5344},
}

@article{Hall2005,
	title = {Intertextual sexuality: {Parodies} of class, identity, and desire in liminal {Delhi}},
	volume = {15},
	issn = {1055-1360},
	doi = {10.1525/jlin.2005.15.1.125},
	abstract = {This article examines articulations of class, identity, and desire as performed by a com- munity of kotis in northern India, a transgender group that impersonates a second trans- gender group known as hijras in a staged event called “hijra-acting.” Through a linguistic parody of lower-class hijras performing a birth celebration for their upper- class patrons, kotis critique the class-based animosity between hijra and gay sexualities in contemporary India, spoofing the sexual desires associated with both groups as infe- rior to their own. The analysis demonstrates that identity and desire are best understood as mutually constituted intertextual phenomena, with both importantly reliant on ideo- logical linkages of language and socioeconomic class for their articulation.},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {Hall, Kira},
	year = {2005},
	keywords = {Desire, Hijras, Intertextuality, Kotis, Parody},
	pages = {125--144},
}

@article{bazarova_self-disclosure_2014,
	title = {Self-disclosure in social media: {Extending} the functional approach to disclosure motivations and characteristics on social network sites},
	volume = {64},
	issn = {14602466},
	doi = {10.1111/jcom.12106},
	abstract = {This article introduces the functional model of self-disclosure on social network sites by integrating a functional theory of self-disclosure and research on audience representations as situational cues for activating interpersonal goals. According to this model, people pursue strategic goals and disclose differently depending on social media affordances, and self-disclosure goals mediate between media affordances and disclosure intimacy. The results of the empirical study examining self-disclosure motivations and characteristics in Facebook status updates, wall posts, and private messaging lend support to this model and provide insights into the motivational drivers of self-disclosure on SNSs, helping to reconcile traditional views on self-disclosure and self-disclosing behaviors in new media contexts. © 2014 International Communication Association.},
	number = {4},
	journal = {Journal of Communication},
	author = {Bazarova, Natalya N. and Choi, Yoon Hyung},
	year = {2014},
	pages = {635--657},
}

@article{bargh_can_2002,
	title = {Can you see the real me? {Activation} and expression of the "true self" on the internet},
	volume = {58},
	issn = {00224537},
	doi = {10.1111/1540-4560.00247},
	abstract = {Those who feel better able to express their "true selves" in Internet rather than face-to-face interaction settings are more likely to form close relationships with people met on the Internet (McKenna, Green, \& Gleason, this issue). Building on these correlational findings from survey data, we conducted three laboratory experiments to directly test the hypothesized causal role of differential self-expression in Internet relationship formation. Experiments 1 and 2, using a reaction time task, found that for university undergraduates, the true-self concept is more accessible in memory during Internet interactions, and the actual self more accessible during face-to-face interactions. Experiment 3 confirmed that people randomly assigned to interact over the Internet (vs. face to face) were better able to express their true-self qualities to their partners.},
	number = {1},
	journal = {Journal of Social Issues},
	author = {Bargh, John A. and McKenna, Katelyn Y.A. and Fitzsimons, Grainne M.},
	year = {2002},
	pages = {33--48},
}

@phdthesis{pathak_extraction_2020,
	title = {Extraction and analysis of self identity in {Twitter} biographies},
	school = {University at Buffalo, The State University of New York},
	author = {Pathak, Arjunil},
	year = {2020},
	note = {ISBN: 9789896540821},
}

@article{lampe_familiar_2007,
	title = {A {Familiar} {Face}( book ): {Profile} {Elements} as {Signals} in an {Online} {Social} {Network}},
	issn = {00242535},
	doi = {10.1145/1240624.1240695},
	abstract = {Using data from a popular online social network site, this paper explores the relationship between profile structure (namely, which fields are completed) and number of friends, giving designers insight into the importance of the profile and how it works to encourage connections and articulated relationships between users. We describe a theoretical framework that draws on aspects of signaling theory, common ground theory, and transaction costs theory to generate an understanding of why certain profile fields may be more predictive of friendship articulation on the site. Using a dataset consisting of 30,773 Facebook profiles, we determine which profile elements are most likely to predict friendship links and discuss the theoretical and design implications of our findings.},
	journal = {CHI 2007 Proceedings},
	author = {Lampe, Cliff and Ellison, Nicole and Steinfield, Charles},
	year = {2007},
	pmid = {10540012},
	note = {ISBN: 9781595935939},
	keywords = {Social network sites, profile elements, signaling theory},
	pages = {435--444},
}

@article{underwood_life_2016,
	title = {The {Life} {Cycles} of {Genres}},
	volume = {70},
	issn = {2371-4549},
	doi = {10.22148/16.005},
	abstract = {The concept of genre is as old as literary theory itself, but centuries of debate haven't produced much consensus on the topic. Part of the reason is that genre looks like a different thing at different points in the life of a text. Scholars of rhetoric tend to focus on the patterns of communicative action that produce memoranda or tragedies (Devitt 2004; Miller 1984). Sociologists are sometimes more interested in institutions that organize reception (Bourdieu 1984; DiMaggio 1987). Literary scholars, for their part, have traditionally been preoccupied with the patterning of the texts themselves. Of course, all of these aspects of genre are connected. But it's not easy to describe the connections. Distant reading may seem to lend itself, inevitably, to literary scholars' fixation on genre as an attribute of textual artefacts. But the real value of quantitative methods could be that they allow scholars to coordinate textual and social approaches to genre. This essay will draw one tentative connection of that kind. It approaches genre initially as a question about the history of reception — gathering lists of titles that were grouped by particular readers or institutions at particular historical moments. But it also looks beyond those titles to the texts themselves. Contemporary practices of statistical modeling allow us to put different groups of texts into dialogue with each other, in order to discover, for instance, whether competing definitions of the Gothic (created at different times and embodied in entirely different lists of works) were nevertheless as compatible as some critics claim.},
	number = {1984},
	journal = {Journal of Cultural Analytics},
	author = {Underwood, Ted},
	year = {2016},
	pages = {151--167},
}

@article{vashishth_confidence-based_2020,
	title = {Confidence-based graph convolutional networks for semi-supervised learning},
	volume = {89},
	abstract = {Predicting properties of nodes in a graph is an important problem with applications in a variety of domains. Graph-based Semi Supervised Learning (SSL) methods aim to address this problem by labeling a small subset of the nodes as seeds and then utilizing the graph structure to predict label scores for the rest of the nodes in the graph. Recently, Graph Convolutional Networks (GCNs) have achieved impressive performance on the graph-based SSL task. In addition to label scores, it is also desirable to have confidence scores associated with them. Unfortunately, confidence estimation in the context of GCN has not been previously explored. We fill this important gap in this paper and propose ConfGCN, which estimates labels scores along with their confidences jointly in GCN-based setting. ConfGCN uses these estimated confidences to determine the influence of one node on another during neighborhood aggregation, thereby acquiring anisotropic1 capabilities. Through extensive analysis and experiments on standard benchmarks, we find that ConfGCN is able to outperform state-of-the-art baselines. We have made ConfGCN's source code available to encourage reproducible research.},
	journal = {AISTATS 2019 - 22nd International Conference on Artificial Intelligence and Statistics},
	author = {Vashishth, Shikhar and Yadav, Prateek and Bhandari, Manik and Talukdar, Partha},
	year = {2020},
	note = {arXiv: 1901.08255},
}

@article{kumar_conversational_2011,
	title = {Conversational strategies that support idea generation productivity in groups},
	volume = {1},
	abstract = {Our recent work has shown that conversational tutors that use social interaction strategies can achieve significant learning improvement through better management of student attention. In continuation with this line of investigation, this paper presents an experiment that measures the separate and joint effects of the nature and style of automatically generated interventions. We investigate aspects of designing intervention prompts from two different angles: first in terms of the addition of socially oriented turns over and above the instructional turns required to keep the students on track, and second in terms of the style of presentation of the tutor's instructional turns. We see positive effects of both manipulations, but on different outcome metrics. Thus, we conclude that the two directions can be productively combined into a single socially enhanced feedback approach, which contributes to the literature on dynamic forms of support for collaborative learning. © ISLS.},
	journal = {Connecting Computer-Supported Collaborative Learning to Policy and Practice: CSCL 2011 Conference Proceedings - Long Papers, 9th International Computer-Supported Collaborative Learning Conference},
	author = {Kumar, Rohit and Beuth, Jack L. and Rosé, Carolyn P.},
	year = {2011},
	note = {ISBN: 9780578091525},
	pages = {398--405},
}

@article{Underwood2018,
	title = {The {Transformation} of {Gender} in {English}-{Language} {Fiction}},
	doi = {10.22148/16.019},
	abstract = {This essay explores the changing significance of gender in fiction, asking especially whether its prominence in characterization has varied from the end of the eighteenth century to the beginning of the twenty-first. We have reached two conclusions, which may seem in tension with each other. The first is that gender divisions between characters have become less sharply marked over the last 170 years. In the middle of the nineteenth century, very different language is used to describe fictional men and women. But that difference weakens steadily as we move forward to the present; the actions and attributes of characters are less clearly sorted into gender categories. On the other hand, we haven't found the same progressive story in the history of authorship. In fact, there is an eye-opening, under-discussed decline in the proportion of fiction actually written by women, which drops by half (from roughly 50\% of titles to roughly 25\%) as we move from 1850 to 1950. The number of characters who are women or girls also drops. We are confronted with a paradoxical pattern. While gender roles were becoming more flexible, the space actually allotted to (real, and fictional) women on the shelves of libraries was contracting sharply. We explore the evidence for this paradox and suggest a few explanations. This essay considers both the gender positions ascribed to authors as biographical personages, and the signs of gender they used in producing characters. In both cases, we understand gender as a conventional role that people were expected to assume in order to become legible in a social context. Authors and characters have 1 The evidence used in this paper has depended heavily on the labor of other hands. The HathiTrust corpus we use was processed by Boris Capitanu. The Chicago Novel Corpus was collected by Hoyt Long and Richard Jean So, and enriched with metadata by Teddy Roland. Conversation with Heather Love, Laura Mandell, and Allen Riddell, and the whole NovelTM research group, turned up valuable leads. Funding for this project was provided by SSHRC via NovelTM, directed by Andrew Piper, and by the Andrew W. Mellon Foundation via the WCSA+DC project, directed by Stephen J. Downie.},
	number = {February},
	journal = {Journal of Cultural Analytics},
	author = {Underwood, Ted and Bamman, David and Lee, Sabrina},
	year = {2018},
	pages = {1--32},
}

@article{rudin_stop_2019,
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	volume = {1},
	issn = {25225839},
	url = {http://dx.doi.org/10.1038/s42256-019-0048-x},
	doi = {10.1038/s42256-019-0048-x},
	abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
	number = {5},
	journal = {Nature Machine Intelligence},
	author = {Rudin, Cynthia},
	year = {2019},
	note = {Publisher: Springer US
ISBN: 4225601900},
	pages = {206--215},
}

@article{nadal_interpersonal_2012,
	title = {Interpersonal and systemic microaggressions toward transgender people: {Implications} for counseling},
	volume = {6},
	issn = {15538605},
	doi = {10.1080/15538605.2012.648583},
	abstract = {This study utilized a qualitative method with transgender female and male participants (N = 9) to identify types of microaggressions, or subtle forms of discrimination, that transgender people experience. Twelve categories of microaggressions were identified: (a) use of transphobic and/or incorrectly gendered terminology, (b) assumption of universal transgender experience, (c) exoticization, (d) discomfort/disapproval of transgender experience, (e) endorsement of gender normative and binary culture or behaviors, (f) denial of existence of transphobia, (g) assumption of sexual pathology/abnormality, (h) physical threat or harassment, (i) denial of individual transphobia, (j) denial of bodily privacy, (k) familial microaggressions, and (l) systemic and environmental microaggressions. Implications for counseling are discussed. © Taylor and Francis Group, LLC.},
	number = {1},
	journal = {Journal of LGBT Issues in Counseling},
	author = {Nadal, Kevin L. and Skolnik, Avy and Wong, Yinglee},
	year = {2012},
	keywords = {Gender identity, LGBT, Microaggressions, Transgender},
	pages = {55--82},
}

@article{starbird_disinformation_2019,
	title = {Disinformation as collaborative work: {Surfacing} the participatory nature of strategic information operations},
	volume = {3},
	issn = {25730142},
	doi = {10.1145/3359229},
	abstract = {In this paper, we argue that strategic information operations (e.g. disinformation, political propaganda, and other forms of online manipulation) are a critical concern for CSCW researchers, and that the CSCW community can provide vital insight into understanding how these operations function—by examining them as collaborative “work” within online crowds. First, we provide needed definitions and a framework for conceptualizing strategic information operations, highlighting related literatures and noting historical context. Next, we examine three case studies of online information operations using a sociotechnical lens that draws on CSCW theories and methods to account for the mutual shaping of technology, social structure, and human action. Through this lens, we contribute a more nuanced understanding of these operations (beyond “bots” and “trolls”) and highlight a persistent challenge for researchers, platform designers, and policy makers—distinguishing between orchestrated, explicitly coordinated, information operations and the emergent, organic behaviors of an online crowd.},
	number = {CSCW},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Starbird, Kate and Arif, Ahmer and Wilson, Tom},
	year = {2019},
	keywords = {Disinformation, Information operations, Media manipulation, Misinformation, Social media},
}

@article{Kim2019,
	title = {An {Analysis} of {Emotion} {Communication} {Channels} in {Fan}-{Fiction}: {Towards} {Emotional} {Storytelling}},
	doi = {10.18653/v1/w19-3406},
	abstract = {Centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology. The research in automatic storytelling has recently turned towards emotional storytelling, in which characters' emotions play an important role in the plot development. However, these studies mainly use emotion to generate propositional statements in the form "A feels affection towards B" or "A confronts B". At the same time, emotional behavior does not boil down to such propositional descriptions, as humans display complex and highly variable patterns in communicating their emotions, both verbally and non-verbally. In this paper, we analyze how emotions are expressed non-verbally in a corpus of fan fiction short stories. Our analysis shows that stories written by humans convey character emotions along various non-verbal channels. We find that some non-verbal channels, such as facial expressions and voice characteristics of the characters, are more strongly associated with joy, while gestures and body postures are more likely to occur with trust. Based on our analysis, we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters' emotions.},
	author = {Kim, Evgeny and Klinger, Roman},
	year = {2019},
	note = {arXiv: 1906.02402},
	pages = {56--64},
}

@article{curtis_hlvu_2020,
	title = {{HLVU}: {A} new challenge to test deep understanding of movies the way humans do},
	doi = {10.1145/3372278.3390742},
	abstract = {In this paper we propose a new evaluation challenge and direction in the area of High-level Video Understanding. The challenge we are proposing is designed to test automatic video analysis and understanding, and how accurately systems can comprehend a movie in terms of actors, entities, events and their relationship to each other. A pilot High-Level Video Understanding (HLVU) dataset of open source movies were collected for human assessors to build a knowledge graph representing each of them. A set of queries will be derived from the knowledge graph to test systems on retrieving relationships among actors, as well as reasoning and retrieving non-visual concepts. The objective is to benchmark if a computer system can "understand" non-explicit but obvious relationships the same way humans do when they watch the same movies. This is long-standing problem that is being addressed in the text domain and this project moves similar research to the video domain. Work of this nature is foundational to future video analytics and video understanding technologies. This work can be of interest to streaming services and broadcasters hoping to provide more intuitive ways for their customers to interact with and consume video content.},
	journal = {ICMR 2020 - Proceedings of the 2020 International Conference on Multimedia Retrieval},
	author = {Curtis, Keith and Awad, George and Rajput, Shahzad and Soboroff, Ian},
	year = {2020},
	note = {ISBN: 9781450370875},
	keywords = {Information retrieval, Multimedia, Video ontology, Video understanding},
	pages = {355--361},
}

@article{sap_connotation_2018,
	title = {Connotation {Frames} of {Power} and {Agency} in {Modern} {Films}},
	doi = {10.18653/v1/d17-1247},
	abstract = {The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.},
	author = {Sap, Maarten and Prasettio, Marcella Cindy and Holtzman, Ari and Rashkin, Hannah and Choi, Yejin},
	year = {2018},
	pages = {2329--2334},
}

@article{Ethayarajh2019,
	title = {Understanding undesirable word embedding associations},
	doi = {10.18653/v1/p19-1166},
	abstract = {Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.},
	journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
	year = {2019},
	note = {arXiv: 1908.06361
ISBN: 9781950737482},
	pages = {1696--1705},
}

@article{noauthor_learning_2019,
	title = {Learning {To} {Understand} {Entities} {In} {Text}},
	year = {2019},
}

@article{Lapesa2020,
	title = {Analysis of {Political} {Debates} through {Newspaper} {Reports}: {Methods} and {Outcomes}},
	issn = {1618-2162},
	doi = {10.1007/s13222-020-00344-w},
	journal = {Datenbank-Spektrum},
	author = {Lapesa, Gabriella and Blessing, Andre and Blokker, Nico and Dayanik, Erenay and Haunss, Sebastian and Kuhn, Jonas and Padó, Sebastian},
	year = {2020},
	keywords = {computational social science, discourse network analysis, machine learning},
	pages = {143--153},
}

@article{zehe_harrymotions_2020,
	title = {Harrymotions - {Classifying} relationships in harry potter based on emotion analysis},
	volume = {2624},
	issn = {16130073},
	abstract = {Sentiment Analysis has long been a topic of interest in natural language processing and computational literary studies, where it can be used to infer the relationships between fictional characters. Building on the dataset and results of Kim and Klinger (2019), we propose a classifier based on BERT that improves the results reported therein and show that we can use this classifier to determine the relation between characters in Harry Potter novels. Our proposed sentiment classifier yields an F1-score of up to 75 \% for binary classification of emotions. Aggregating these emotions over novels, we reach an F1-score of up to 68 \% for the classification of a pair of characters as friendly or unfriendly.},
	journal = {CEUR Workshop Proceedings},
	author = {Zehe, Albin and Arns, Julia and Hettinger, Lena and Hotho, Andreas},
	year = {2020},
}

@inproceedings{Agarwal2013,
	title = {{SINNET}: {Social} {Interaction} {Network} {Extractor} from {Text}},
	abstract = {In this paper we present a demo of our sys-tem: Social Interaction Network Extractor from Text (SINNET). SINNET is able to extract a social network from unstructured text. Nodes in the network are people and links are social events.},
	booktitle = {The {Companion} {Volume} ofthe {Proceedings} of {IJCNLP} 2013: {System} {Demonstrations}},
	author = {Agarwal, Apoorv and Kotalwar, Anup and Zheng, Jiehan and Rambow, Owen},
	year = {2013},
	note = {Issue: October},
	pages = {33--36},
}

@inproceedings{elson_extracting_2010,
	title = {Extracting {Social} {Networks} from {Literary} {Fiction}},
	booktitle = {Proceedings of the 48th {Annual} {Meeting} ofthe {Association} for {Computational} {Linguistics}},
	author = {Elson, David K and Mckeown, Kathleen R},
	year = {2010},
	note = {Issue: July},
	pages = {138--147},
}

@inproceedings{Chaturvedi2016,
	title = {Modeling evolving relationships between characters in literary novels},
	isbn = {978-1-57735-760-5},
	abstract = {Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semisupervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.},
	booktitle = {30th {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2016},
	author = {Chaturvedi, Snigdha and Srivastava, Shashank and Dauḿe, Hal and Dyer, Chris},
	year = {2016},
	note = {arXiv: 1511.09376v1},
	pages = {2704--2710},
}

@article{eisenberg_automatic_2020,
	title = {Automatic extraction of personal events from dialogue},
	doi = {10.18653/v1/2020.nuse-1.8},
	author = {Eisenberg, Joshua and Sheriff, Michael},
	year = {2020},
	pages = {63--71},
}

@article{mintz_distant_2009,
	title = {Distant supervision for relation extraction without labeled data},
	doi = {10.3115/1690219.1690287},
	abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE- style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of enti- ties that appears in some Freebase relation, we find all sentences containing those entities in a large un- labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 re- lations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
	number = {August},
	author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
	year = {2009},
	pages = {1003},
}

@article{zhang_making_2018,
	title = {Making sense of group chat through collaborative tagging and summarization},
	volume = {2},
	issn = {25730142},
	doi = {10.1145/3274465},
	abstract = {While group chat is becoming increasingly popular for team collaboration, these systems generate long streams of unstructured back-and-forth discussion that are difficult to comprehend. In this work, we investigate ways to enrich the representation of chat conversations, using techniques such as tagging and summarization, to enable users to better make sense of chat. Through needfinding interviews with 15 active group chat users, who were shown mock-up alternative chat designs, we found the importance of structured representations, including signals such as discourse acts. We then developed Tilda, a prototype system that enables people to collaboratively enrich their chat conversation while conversing. From lab evaluations, we examined the ease of marking up chat using Tilda as well as the effectiveness of Tilda-enabled summaries for getting an overview. From a field deployment, we found that teams actively engaged with Tilda both for marking up their chat as well as catching up on chat.},
	number = {CSCW},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Zhang, Amy X. and Cranshaw, Justin},
	year = {2018},
	keywords = {Annotation, Collaboration, Group chat, Knowledge management, Online communities, Online discussions, Sensemaking, Summarization, Tagging, group chat},
}

@article{Shahsavari2020,
	title = {An {Automated} {Pipeline} for {Character} and {Relationship} {Extraction} from {Readers} {Literary} {Book} {Reviews} on {Goodreads}.com},
	doi = {10.1145/3394231.3397918},
	abstract = {Reader reviews of literary fiction on social media, especially those in persistent, dedicated forums, create and are in turn driven by underlying narrative frameworks. In their comments about a novel, readers generally include only a subset of characters and their relationships, thus offering a limited perspective on that work. Yet in aggregate, these reviews capture an underlying narrative framework comprised of different actants (people, places, things), their roles, and interactions that we label the "consensus narrative framework". We represent this framework in the form of an actant-relationship story graph. Extracting this graph is a challenging computational problem, which we pose as a latent graphical model estimation problem. Posts and reviews are viewed as samples of sub graphs/networks of the hidden narrative framework. Inspired by the qualitative narrative theory of Greimas, we formulate a graphical generative Machine Learning (ML) model where nodes represent actants, and multi-edges and self-loops among nodes capture context-specific relationships. We develop a pipeline of interlocking automated methods to extract key actants and their relationships, and apply it to thousands of reviews and comments posted on Goodreads.com. We manually derive the ground truth narrative framework from SparkNotes, and then use word embedding tools to compare relationships in ground truth networks with our extracted networks. We find that our automated methodology generates highly accurate consensus narrative frameworks: for our four target novels, with approximately 2900 reviews per novel, we report average coverage/recall of important relationships of {\textgreater} 80\% and an average edge detection rate of {\textgreater}89{\textbackslash}\%. These extracted narrative frameworks can generate insight into how people (or classes of people) read and how they recount what they have read to others.},
	author = {Shahsavari, Shadi and Ebrahimzadeh, Ehsan and Shahbazi, Behnam and Falahi, Misagh and Holur, Pavan and Bandari, Roja and R. Tangherlini, Timothy and Roychowdhury, Vwani},
	year = {2020},
	note = {ISBN: 9781450379892},
	keywords = {acm reference format, behnam shahbazi, ehsan ebrahimzadeh, graph theory, knowledge base, machine learning, misagh falahi, narrative theory, shadi shahsavari},
	pages = {277--286},
}

@article{Nakandala2017,
	title = {Gendered conversation in a social game-streaming platform},
	abstract = {Online social media and games are increasingly replacing offline social activities. Social media is now an indispensable mode of communication; online gaming is not only a genuine social activity but also a popular spectator sport. Although online interaction shrinks social and geographical barriers, it is argued that social disparities, such as gender inequality, persists. For instance, online gaming communities have been criticized for objectifying women, which is a pressing question as gaming evolves into a social platform. However, few large-scale, systematic studies of gender inequality and objectification in social gaming platforms exist. Here we analyze more than one billion chat messages from Twitch, a social game-streaming platform, to study how the gender of streamers is associated with the nature of conversation. We find that female streamers receive significantly more objectifying comments while male streamers receive more gamerelated comments. This difference is more pronounced for popular streamers. We also show that the viewers' choice of channels is also strongly gendered. Our findings suggest that gendered conversation and objectification is prevalent, and most users produce strongly gendered messages.},
	number = {Icwsm},
	journal = {Proceedings of the 11th International Conference on Web and Social Media, ICWSM 2017},
	author = {Nakandala, Supun and Ciampaglia, Giovanni Luca and Su, Norman Makoto and Ahn, Yong Yeol},
	year = {2017},
	note = {arXiv: 1611.06459
ISBN: 9781577357889},
	keywords = {Full Papers},
	pages = {162--171},
}

@article{Joseph2017,
	title = {Girls rule, boys drool: {Extracting} semantic and affective stereotypes on {Twitter}},
	doi = {10.1145/2998181.2998187},
	abstract = {Social identities carry widely agreed upon meanings, called stereo-types, that have important effects on social processes. We develop a method to extract the stereotypes of a particular population of Twit-ter users. Our model is grounded in social theory on stereotypes as both identities' affective meanings and their semantic relationships to each other. We apply our model to a dataset of 45K Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. This case study furthers our understanding of both the stereotypes present for those who actively discussed these tragedies online as well as the structure of stereotypes in wider populations both online and off.},
	journal = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing - CSCW '17},
	author = {Joseph, Kenneth and Wei, Wei and Carley, Kathleen C.},
	year = {2017},
	note = {ISBN: 9781450343350},
	keywords = {computational social science, identity, social psychology, stereo-},
	pages = {1362--1374},
}

@article{leighton_myths_2020,
	title = {Myths of {Meritocracy}, {Friendship}, and {Fun} {Work}: {Class} and {Gender} in {North} {American} {Academic} {Communities}},
	volume = {122},
	issn = {15481433},
	doi = {10.1111/aman.13455},
	abstract = {Using the example of Andean archaeology, this article focuses on subtle forms of inequality that arise when academic communities are conceptualized as friendship-based and egalitarian, rejecting explicit hierarchy. I describe this as performative informality and argue that it stems from a meritocratic ideology that inadvertently reproduces Euro-American white-male privilege. In a discipline that prides itself on its friendliness, openness, and alcohol-fueled drinking culture, those who find themselves unable to enact or perform informality appropriately are at a distinct disadvantage. Drawing from a multisited ethnography of Andeanist archaeologists, I make the case that it is the ephemerality and plausible deniability of performative informality that makes it hard to recognize and thus mitigate against it. In doing so, I draw on and contribute to the theorization of gender/class intersectionality in anthropology and science studies, US conceptualizations of meritocracy in academia and higher education, and feminist Jo Freeman's concept of “the tyranny of structurelessness.” [anthropology of science, ethnography of archaeology, class, gender, anthropology of work and education].},
	number = {3},
	journal = {American Anthropologist},
	author = {Leighton, Mary},
	year = {2020},
	pages = {444--458},
}

@article{bucholtz_sociolinguistic_2003,
	title = {Sociolinguistic nostalgia and the authentication of identity},
	volume = {7},
	issn = {1360-6441},
	doi = {10.1111/1467-9481.00232},
	abstract = {Page 1. Sociolinguistic nostalgia and the authentication of identity1 Mary Bucholtz University of California, Santa Barbara INTRODUCTION},
	number = {3},
	journal = {Journal of Sociolinguistics},
	author = {Bucholtz, Mary},
	year = {2003},
	pages = {398--416},
}

@article{galinsky_further_2007,
	title = {Further ironies of suppression: {Stereotype} and counterstereotype accessibility},
	volume = {43},
	issn = {00221031},
	doi = {10.1016/j.jesp.2006.09.001},
	abstract = {Three experiments explored the accessibility of stereotypes and counterstereotypes following stereotype suppression. Using a lexical decision task, experiment 1 demonstrated that the counterstereotype showed greater accessibility following stereotype suppression compared to stereotype expressers and no prime control participants. Using a person perception task, experiment 2 revealed that suppression can make both the stereotype and the counterstereotype more accessible. Experiment 3 manipulated cognitive load and found evidence that the stereotype and counterstereotype are made accessible through two different processes associated with suppression: The stereotype is made accessible through the more automatic monitoring system, whereas the counterstereotype is made accessible through the resource-dependent operating system. The three experiments demonstrate a novel lack of inhibition of the counterstereotype by the stereotype, provide a clear demonstration of hyperaccessibility of suppressed stereotypes by comparing stereotype suppression to a stereotype expression condition, and contribute to the priming literature by demonstrating the interactive effects of accessibility, applicability, and judgment order on person perception evaluations. © 2006 Elsevier Inc. All rights reserved.},
	number = {5},
	journal = {Journal of Experimental Social Psychology},
	author = {Galinsky, Adam D. and Moskowitz, Gordon B.},
	year = {2007},
	keywords = {Construct accessibility, Counterstereotypes, Inhibition, Stereotypes, Suppression},
	pages = {833--841},
}

@article{monteith_suppression_1998,
	title = {Suppression as a stereotype control strategy},
	volume = {2},
	issn = {10888683},
	doi = {10.1207/s15327957pspr0201_4},
	abstract = {Recent research reveals that efforts to suppress Stereotypic thoughts can backfire and produce a rebound effect, such that Stereotypic thinking increases to a level that is even greater than if no attempt at stereotype control was initially exercised (e.g., Macrae, Bodenhausen, Milne, \& Jetten, 1994). The primary goal of this article is to present an in-depth theoretical analysis of stereotype. suppression that identifies numerous potential moderators of the effect of stereotype suppression on the likelihood of subsequent rebound. Our analysis of stereotype suppression focuses on two broad issues: the influence of level of prejudice and the influence of processing goals on the activation versus application of stereotypes. Although stereotype rebound occurs under some circumstances, we suggest that a complete understanding of this phenomenon requires consideration of the full array of possible moderating influences. Copyright © 1998 by Lawrence Erlbaum Associates, Inc.},
	number = {1},
	journal = {Personality and Social Psychology Review},
	author = {Monteith, Margo J. and Sherman, Jeffrey W. and Devine, Patricia G.},
	year = {1998},
	pages = {63--82},
}

@article{Bojanowski2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	volume = {5},
	issn = {10450823},
	url = {http://arxiv.org/abs/1607.04606},
	doi = {1511.09249v1},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	year = {2017},
	pmid = {1000303116},
	note = {arXiv: 1607.04606
ISBN: 9781577357384},
	pages = {135--146},
}

@article{wu_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	issn = {2162-237X},
	url = {https://ieeexplore.ieee.org/abstract/document/9046288?casa_token=wfkvJGUX3j8AAAAA:PUNwEgf3JPZgmdMxbI827rokwRe4LsvfYeWNJbvqOVBRwFtXOd2kYcP0NJBprBkMBgL1Edkxzg},
	doi = {10.1109/tnnls.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	year = {2020},
	note = {arXiv: 1901.00596
Publisher: IEEE},
	pages = {1--21},
}

@article{vashishth_graph-based_2020,
	title = {Graph-based deep learning in natural language processing},
	doi = {10.1145/3371158.3371232},
	abstract = {This tutorial aims to introduce recent advances in graph-based deep learning techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides a brief introduction to deep learning methods on non-Euclidean domains such as graphs and justifies their relevance in NLP. It then covers recent advances in applying graph-based deep learning methods for various NLP tasks, such as semantic role labeling, machine translation, relationship extraction, and many more.},
	journal = {ACM International Conference Proceeding Series},
	author = {Vashishth, Shikhar and Yadati, Naganand and Talukdar, Partha},
	year = {2020},
	note = {ISBN: 9781450377386},
	pages = {371--372},
}

@article{zhou_graph_2018,
	title = {Graph {Neural} {Networks}: {A} {Review} of {Methods} and {Applications}},
	url = {http://arxiv.org/abs/1812.08434},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2018},
	note = {arXiv: 1812.08434},
	pages = {1--22},
}

@article{guillory_combating_2020,
	title = {Combating {Anti}-{Blackness} in the {AI} {Community}},
	abstract = {In response to a national and international awakening on the issues of anti-Blackness and systemic discrimination, we have penned this piece to serve as a resource for allies in the AI community who are wondering how they can more effectively engage with dismantling racist systems. This work aims to help elucidate areas where the AI community actively and passively contributes to anti-Blackness and offers actionable items on ways to reduce harm.},
	author = {Guillory, Devin},
	year = {2020},
	pages = {13},
}

@inproceedings{aharoni_unsupervised_2020,
	title = {Unsupervised {Domain} {Clusters} in {Pretrained} {Language} {Models}},
	doi = {10.18653/v1/2020.acl-main.692},
	abstract = {The notion of "in-domain data" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.},
	author = {Aharoni, Roee and Goldberg, Yoav},
	year = {2020},
	note = {arXiv: 2004.02105},
	pages = {7747--7763},
}

@misc{noauthor_hill_nodate,
	title = {Hill voices of {Don} {Gabriel}.pdf},
}

@article{reyes_inventing_2017,
	title = {Inventing {Postcolonial} {Elites}: {Race}, {Language}, {Mix}, {Excess}},
	volume = {27},
	issn = {15481395},
	doi = {10.1111/jola.12156},
	abstract = {This article illustrates how semiotic processes that form and circulate ideologies about race, language, and the elite are central to questions of coloniality. Considering the historical and contemporary context of the Philippines, I examine how notions of linguistic and racial “mix” and “excess” get linked to elite social figures and how one elite figure in particular—the “conyo elite”—is reportedly heard and seen by a private school–educated listening subject that is constituted, in contrast, as “middle-class elite.” I consider how iconizations of mixedness and excessiveness invent distinctions among Philippine elite types, producing an “elite bifurcation” that recursively constitutes colonial hierarchies: positioning conyo elites as acting as colonists whose supposedly mixed and excessive qualities are regarded as immoral, overly modern, and a national betrayal.},
	number = {2},
	journal = {Journal of Linguistic Anthropology},
	author = {Reyes, Angela},
	year = {2017},
	keywords = {Philippines, coloniality, elite, race, register},
	pages = {210--231},
}

@article{reagan_emotional_2016,
	title = {The emotional arcs of stories are dominated by six basic shapes},
	volume = {5},
	issn = {21931127},
	url = {http://dx.doi.org/10.1140/epjds/s13688-016-0093-1},
	doi = {10.1140/epjds/s13688-016-0093-1},
	abstract = {Advances in computing power, natural language processing, and digitization of text now make it possible to study a culture’s evolution through its texts using a ‘big data’ lens. Our ability to communicate relies in part upon a shared emotional experience, with stories often following distinct emotional trajectories and forming patterns that are meaningful to us. Here, by classifying the emotional arcs for a filtered subset of 1,327 stories from Project Gutenberg’s fiction collection, we find a set of six core emotional arcs which form the essential building blocks of complex emotional trajectories. We strengthen our findings by separately applying matrix decomposition, supervised learning, and unsupervised learning. For each of these six core emotional arcs, we examine the closest characteristic stories in publication today and find that particular emotional arcs enjoy greater success, as measured by downloads.},
	number = {1},
	journal = {EPJ Data Science},
	author = {Reagan, Andrew J. and Mitchell, Lewis and Kiley, Dilan and Danforth, Christopher M. and Dodds, Peter Sheridan},
	year = {2016},
	note = {arXiv: 1606.07772
Publisher: Reagan et al.},
	keywords = {language, narratology, sentiment mining, society, stories},
	pages = {1--12},
}

@article{dietterich_approximate_1998,
	title = {Approximate {Statistical} {Tests} for {Comparing} {Supervised} {Classification} {Learning} {Algorithms}},
	volume = {10},
	issn = {08997667},
	doi = {10.1162/089976698300017197},
	abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 × 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
	number = {7},
	journal = {Neural Computation},
	author = {Dietterich, Thomas G.},
	year = {1998},
	pmid = {9744903},
	pages = {1895--1923},
}

@article{dror_hitchhikers_2018,
	title = {The hitchhiker's guide to testing statistical significance in natural language processing},
	volume = {1},
	abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
	year = {2018},
	note = {ISBN: 9781948087322},
	pages = {1383--1392},
}

@article{valentinsson_english_2018,
	title = {English and {Bivalent} {Class} {Indexicality} in {Buenos} {Aires} ,},
	author = {Valentinsson, Mary-caitlyn},
	year = {2018},
	keywords = {argentina, cheto, class, english, nerd},
	pages = {1--14},
}

@article{chaparro_fresas_2016,
	title = {Fresas, {Nacos} y {Lo} que le {Sigue}: {Towards} a {Sketch} of {Two} {Mexican} {Emblematic} {Models} of {Personhood}},
	volume = {31},
	number = {1},
	journal = {Working Papers in Educational Linguistics},
	author = {Chaparro, Sofía},
	year = {2016},
	pages = {43--68},
}

@article{reyes_ontology_2017,
	title = {Ontology of fake: {Discerning} the philippine elite},
	volume = {5},
	issn = {23264497},
	doi = {10.1086/690067},
	abstract = {Hilary Putnam (1975) proposes that a “natural kind” term relies on a division of linguistic labor in which experts discern what is or is not a member of a kind. Centering on a Philippine-elite social kind term, this essay examines how self-appointed experts develop and share “scientific” instruments, or tests, that discern whether someone is a “real” or “fake” elite. These tests report about signs of realness and fakeness by assigning “gentle” and “rough” qualities to speech and body of differentiated social types. This essay demonstrates that such qualia are central to shaping ontologies of social types; that the discerning subject, who speaks from an elevated social position as reflexive expert, is critical to this process; and that the discerner shares expertise by developing tests that rest on different ontologies of emblem. The essay argues that discerning Philippine elite types and their aspiring subtypes and countertypes presupposes an overarching ontology of fake that already renders real elites as fakes.},
	number = {S1},
	journal = {Signs and Society},
	author = {Reyes, Angela},
	year = {2017},
	pages = {S100--S127},
}

@article{mohamed_decolonial_2020,
	title = {Decolonial {AI}: {Decolonial} {Theory} as {Sociotechnical} {Foresight} in {Artificial} {Intelligence}},
	issn = {22105441},
	url = {https://link.springer.com/article/10.1007/s13347-020-00405-8},
	doi = {10.1007/s13347-020-00405-8},
	abstract = {This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. While the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us, ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.},
	number = {May},
	journal = {Philosophy and Technology},
	author = {Mohamed, Shakir and Png, Marie Therese and Isaac, William},
	year = {2020},
	note = {arXiv: 2007.04068},
	keywords = {Affective community, Artificial intelligence, Coloniality, Critical technical practice, Decolonisation, Intercultural ethics, Sociotechnical foresight},
}

@inproceedings{ogbonnaya2020critical,
	title = {Critical {Race} {Theory} for {HCI}},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Ogbonnaya-Ogburu, Ihudiya Finda and Smith, Angela D R and To, Alexandra and Toyama, Kentaro},
	year = {2020},
	pages = {1--16},
}

@article{card_neural_2018,
	title = {Neural models for documents with metadata},
	volume = {1},
	doi = {10.18653/v1/p18-1189},
	abstract = {Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	year = {2018},
	note = {arXiv: 1705.09296
ISBN: 9781948087322},
	pages = {2031--2040},
}

@article{george_simultaneity_2020,
	title = {Simultaneity and the refusal to choose: {The} semiotics of {Serbian} youth identity on {Facebook}},
	volume = {49},
	issn = {14698013},
	doi = {10.1017/S004740451900099X},
	abstract = {Although the importance of linguistic simultaneity has long been recognized (Woolard 1998), the concept is underexamined in recent analyses of language use in globalized, digital contexts such as social media. Drawing from an analysis of everyday Facebook posts from youth in Belgrade, Serbia, the article proposes that recognizing four types of simultaneity-of linguistic features, indexical operations, effects, and scale-is key for making sense of social media utterances in political and historical context. On Facebook, Serbian youth mix languages and writing systems in complex ways, adhering to dominant ideologies of language and identity in some ways and flouting them in others. Using the Serbian case as a springboard, along with the four types of simultaneity proposed, I suggest a framework for analyzing language and identity on social media. (Serbia, indexicality, simultaneity, social media, superdiversity, bivalency, youth)∗.},
	number = {3},
	journal = {Language in Society},
	author = {George, Rachel},
	year = {2020},
	pages = {399--423},
}

@article{blodgett_demographic_2016,
	title = {Demographic dialectal variation in social media: {A} case study of {African}-{American} {English}},
	doi = {10.18653/v1/d16-1120},
	abstract = {Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.},
	journal = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
	year = {2016},
	note = {arXiv: 1608.08868
ISBN: 9781945626258},
	pages = {1119--1130},
}

@article{hofstra_diversityinnovation_2020,
	title = {The diversity–innovation paradox in science},
	volume = {117},
	issn = {10916490},
	doi = {10.1073/pnas.1915378117},
	abstract = {Prior work finds a diversity paradox: Diversity breeds innovation, yet underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-complete population of ∼1.2 million US doctoral recipients from 1977 to 2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: For example, novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity’s role in innovation and partly explains the underrepresentation of some groups in academia.},
	number = {17},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Hofstra, Bas and Kulkarni, Vivek V. and Galvez, Sebastian Munoz Najar and He, Bryan and Jurafsky, Dan and McFarland, Daniel A.},
	year = {2020},
	note = {ISBN: 1915378117},
	pages = {9284--9291},
}

@article{cushing_power_2020,
	title = {Power, policing, and language policy mechanisms in schools: {A} response to {Hudson}},
	volume = {49},
	issn = {14698013},
	doi = {10.1017/S004740452000038X},
	abstract = {This discussion is a response to Richard Hudson's response to my article, 'The policy and policing of language in schools' (Cushing 2019). Hudson argues that current education policy in England generally rejects and avoids prescriptivism and sets out to illustrate this in reference to a number of policy documents. As in my original article, I conceive of language policy as p/Political and one way in which language ideologies get turned into practices, through a series of policy mechanisms such as curricula, tests, and guidance for teachers. I show how these mechanisms do not 'reject' prescriptivism, but explicitly perpetuate it, and thus act as a system of coercion which can lead teachers into reproducing these ideologies in their practice. I argue that Hudson's argument is limited because of its depoliticised stance and understanding of key sociolinguistic concepts and issues, such as 'Standard English', 'linguistic correctness', and language education itself. (Language education policy, language ideologies, critical applied linguistics, schools, England)∗.},
	number = {3},
	journal = {Language in Society},
	author = {Cushing, Ian},
	year = {2020},
	note = {ISBN: 0047404520000},
	pages = {461--475},
}

@article{hudson_comment_2020,
	title = {Comment on '{The} policy and policing of language in schools' by {Ian} {Cushing}},
	volume = {49},
	issn = {14698013},
	doi = {10.1017/S0047404520000366},
	abstract = {Cushing argues that government policy in the UK is prescriptive and encourages similar policies at school level (as reported in the press), which in turn encourage the 'policing' of language by school teachers. I offer an alternative reading of the evidence in which government policy, as stated in official documents, generally avoids prescriptivism, as do an unknown number of schools and school teachers; where prescriptivism persists it reflects a prescriptive culture in society, not government policy. The conclusion is that government policy is only one influence on teachers' behaviour, so if government wants to eliminate prescriptivism it needs to take a stronger position than simply avoiding prescriptivism in its own documents. (Education, prescriptivism, policy, Britain).},
	number = {3},
	journal = {Language in Society},
	author = {Hudson, Richard},
	year = {2020},
	note = {ISBN: 0047404520000},
	pages = {451--460},
}

@article{cushing_policy_2019,
	title = {The policy and policing of language in schools},
	issn = {14698013},
	doi = {10.1017/S0047404519000848},
	abstract = {This study investigates cases of language 'policing' as educational language policies, and the way that these are represented across different policy levels. Focusing on UK schools and using discursive approaches to language policy as a theoretical framework, I critically examine the motivations and justifications that institutions provide for designing and implementing policies whereby nonstandardised forms are 'banned', and how these are reported in metalinguistic discourse. Drawing on a range of data including media discourse, policy documents, teacher interviews and linguistic landscapes, I textually trace how educational language policies (re)produce prescriptive and linguicist ideologies, often using metaphors of crime, and often using language as a proxy for social factors such as academic achievement, employability, and standards. Overall, I argue that micro- and meso-level language policies are a partial product of the linguistic conservatism as found within current macro-level educational policy. (Language policy, language policing, schools, language ideologies)∗},
	journal = {Language in Society},
	author = {Cushing, Ian},
	year = {2019},
	pages = {425--450},
}

@article{joseph_when_2020,
	title = {When do {Word} {Embeddings} {Accurately} {Reflect} {Surveys} on our {Beliefs} {About} {People}?},
	url = {http://arxiv.org/abs/2004.12043},
	abstract = {Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.},
	author = {Joseph, Kenneth and Morgan, Jonathan H.},
	year = {2020},
	note = {arXiv: 2004.12043},
	pages = {4392--4415},
}

@article{ribeiro_beyond_2020,
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} models with {CheckList}},
	url = {http://arxiv.org/abs/2005.04118},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	year = {2020},
	note = {arXiv: 2005.04118},
	pages = {4902--4912},
}

@article{schlesinger_intersectional_2017,
	title = {Intersectional {HCI}},
	doi = {10.1145/3025453.3025766},
	abstract = {Understanding users becomes increasingly complicated when we grapple with various overlapping attributes of an individual's identity. In this paper we introduce intersectionality as a framework for engaging with the complexity of users'—and authors'—identities, and situating these identities in relation to their contextual surroundings. We conducted a meta-review of identity representation in the CHI proceedings, collecting a corpus of 140 manuscripts on gender, ethnicity, race, class, and sexuality published between 1982-2016. Drawing on this corpus, we analyze how identity is constructed and represented in CHI research to examine intersectionality in a human-computer interaction (HCI) context. We find that previous identity-focused research tends to analyze one facet of identity at a time. Further, research on ethnicity and race lags behind research on gender and socio-economic class. We conclude this paper with recommendations for incorporating intersectionality in HCI research broadly, encouraging clear reporting of context and demographic information, inclusion of author disclosures, and deeper engagement with identity complexities.},
	author = {Schlesinger, Ari and Edwards, W. Keith and Grinter, Rebecca E.},
	year = {2017},
	note = {ISBN: 9781450346559},
	keywords = {Intersectional HCI, class, socio-economic status, ethnicity, gender, identity, intersectionality, race},
	pages = {5412--5427},
}

@article{oneal_beyond_2019,
	title = {Beyond intelligibility: ‘{Transintelligibility}’ phenomena in {English} as a {Lingua} {Franca} interactions},
	volume = {29},
	issn = {14734192},
	doi = {10.1111/ijal.12236},
	abstract = {Through conversation analysis, this study analyzes the negotiation of pronunciation in English as a lingua franca (ELF) contexts among students at a Japanese university. Previous research has demonstrated that interactants in ELF contexts can repair segmental pronunciation so that they can maintain mutually intelligible pronunciation in situ (e.g., Matsumoto 2011; O'Neal 2015). The present study nevertheless claims that the phonetic negotiation of pronunciation in ELF interactions can at times go beyond the goal of restoring or maintaining mutually intelligible pronunciation. That is, phonetic negotiations can continue in ELF interactions even after the restoration or maintenance of mutual intelligibility is achieved. Although most ELF phonology studies (e.g., Deterding, 2013; Jenkins, 2000) have hitherto focused on the issue of intelligibility, interactants in ELF contexts might also orient to states beyond the maintenance of mutual intelligibility as an interactional goal. Namely, ‘transintelligibility’, or the negotiation of individually favored ways of pronouncing, can be an alternative aim of phonetic negotiations in ELF interactions. Based on the analysis, this study provides pedagogical implications regarding how to negotiate pronunciations for English language users in intercultural communication.},
	number = {1},
	journal = {International Journal of Applied Linguistics (United Kingdom)},
	author = {O'Neal, George and Matsumoto, Yumi},
	year = {2019},
	keywords = {English as a Lingua Franca, Intelligibility, Pronunciation},
	pages = {44--60},
}

@article{marcus_building_2016,
	title = {Building a better description},
	volume = {135},
	issn = {07346018},
	doi = {10.1525/rep.2016.135.1.1},
	abstract = {Universally practiced across the disciplines, description is also consistently devalued or overlooked. In this introduction to the special issue "Description Across Disciplines," Sharon Marcus, Heather Love, and Stephen Best propose that description is a critical practice more complex (and less contradictory) than its detractors have taken it to be. They argue that turning critical attention toward description's nuances gives us access to the ways that scholars conventionally assign and withhold value and prestige. The authors set forth a number of principles (using their contributors' essays as a guide) toward the end of "building a better description.".},
	number = {1},
	journal = {Representations},
	author = {Marcus, Sharon and Love, Heather and Best, Stephen},
	year = {2016},
	pages = {1--21},
}

@article{love_close_2010,
	title = {Close but not {Deep}: {Literary} {Ethics} and the {Descriptive} {Turn}},
	volume = {41},
	number = {2},
	journal = {New Literary History},
	author = {Love, Heather},
	year = {2010},
	pages = {371--391},
}

@incollection{deleuze_introduction_1980,
	title = {Introduction: {Rhizome}},
	booktitle = {A {Thousand} {Plateaus}},
	author = {{Deleuze} and {Guattari}},
	year = {1980},
	pages = {3--25},
}

@incollection{jackson_fantasy_1981,
	title = {Fantasy: {The} {Literature} of {Subversion}},
	booktitle = {Fantasy: {The} {Literature} of {Subversion}},
	author = {Jackson, Rosemary},
	year = {1981},
	pages = {48--53},
}

@book{Azuma2009,
	title = {Otaku: {Japan}'s database animals},
	isbn = {978-0-8166-5351-5},
	publisher = {University of Minnesota Press},
	author = {Azuma, Hiroki},
	year = {2009},
}

@article{Clark2015,
	title = {Entity-centric coreference resolution with model stacking},
	volume = {1},
	doi = {10.3115/v1/p15-1136},
	abstract = {Mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coreference resolution, but do not make use of entity-level information. However, we show that the scores produced by such models can be aggregated to define powerful entity-level features between clusters of mentions. Using these features, we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally. The mention pair scores are also used to prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art.},
	journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
	author = {Clark, Kevin and Manning, Christopher D.},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1405--1415},
}

@article{im_still_2020,
	title = {Still out there: {Modeling} and {Identifying} {Russian} {Troll} {Accounts} on {Twitter}},
	doi = {10.1145/3394231.3397889},
	abstract = {There is evidence that Russia's Internet Research Agency attempted to interfere with the 2016 U.S. election by running fake accounts on Twitter - often referred to as "Russian trolls". In this work, we: 1) develop machine learning models that predict whether a Twitter account is a Russian troll within a set of 170K control accounts; and, 2) demonstrate that it is possible to use this model to find active accounts on Twitter still likely acting on behalf of the Russian state. Using both behavioral and linguistic features, we show that it is possible to distinguish between a troll and a non-troll with a precision of 78.5\% and an AUC of 98.9\%, under cross-validation. Applying the model to out-of-sample accounts still active today, we find that up to 2.6\% of top journalists' mentions are occupied by Russian trolls. These findings imply that the Russian trolls are very likely still active today. Additional analysis shows that they are not merely software-controlled bots, and manage their online identities in various complex ways. Finally, we argue that if it is possible to discover these accounts using externally - accessible data, then the platforms - with access to a variety of private internal signals - should succeed at similar or better rates.},
	author = {Im, Jane and Chandrasekharan, Eshwar and Sargent, Jackson and Lighthammer, Paige and Denby, Taylor and Bhargava, Ankit and Hemphill, Libby and Jurgens, David and Gilbert, Eric},
	year = {2020},
	note = {arXiv: 1901.11162
ISBN: 9781450379892},
	keywords = {misinformation, political manipulation, russian troll, social media},
	pages = {1--10},
}

@article{fishelov_types_1990,
	title = {Types of {Character}, {Characteristics} of {Types}},
	volume = {24},
	number = {3},
	journal = {Style},
	author = {Fishelov, David},
	year = {1990},
	pages = {422--439},
}

@book{digangi2011sexual,
	title = {Sexual {Types}: {Embodiment}, {Agency}, and {Dramatic} {Character} from {Shakespeare} to {Shirley}},
	isbn = {978-0-8122-0515-2},
	url = {https://books.google.com/books?id=JikfCci1y3MC},
	publisher = {University of Pennsylvania Press, Incorporated},
	author = {DiGangi, M},
	year = {2011},
}

@article{chatzakou_user_2020,
	title = {User {Identity} {Linkage} in {Social} {Media} {Using} {Linguistic} and {Social} {Interaction} {Features}},
	doi = {10.1145/3394231.3397920},
	author = {Chatzakou, Despoina and Soler-Company, Juan and Tsikrika, Theodora and Wanner, Leo and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	year = {2020},
	note = {ISBN: 9781450379892},
	keywords = {abusive and illegal content, acm reference format, actor identity resolution, despoina chatzakou, juan soler-company, leo wanner, theodora tsikrika, twitter},
	pages = {295--304},
}

@article{vashishth_attention_2019,
	title = {Attention {Interpretability} {Across} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/1909.11218},
	abstract = {The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain \& Wallace, 2019; Vig \& Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.},
	author = {Vashishth, Shikhar and Upadhyay, Shyam and Tomar, Gaurav Singh and Faruqui, Manaal},
	year = {2019},
	note = {arXiv: 1909.11218},
	pages = {1--10},
}

@article{mena__2020,
	title = {‘ {Converse} racialization ’ and ‘ un = marking ’ language : {The} making of a bilingual university in a neoliberal world},
	author = {Mena, Mike and García, Ofelia},
	year = {2020},
	note = {ISBN: 0047404520000},
	pages = {1--22},
}

@inproceedings{august2020exploring,
	title = {Exploring the {Effect} of {Author} and {Reader} {Identity} in {Online} {Story} {Writing}: the {STORIESINTHEWILD} {Corpus}.},
	booktitle = {Proceedings of the {First} {Joint} {Workshop} on {Narrative} {Understanding}, {Storylines}, and {Events}},
	author = {August, Tal and Sap, Maarten and Clark, Elizabeth and Reinecke, Katharina and Smith, Noah A},
	year = {2020},
	pages = {46--54},
}

@article{gal_scale-making_2016,
	title = {{SCALE}-{MAKING} : {Comparison} and {Perspective} as {Ideological} {Projects}},
	number = {May 2019},
	author = {Gal, Susan},
	year = {2016},
}

@article{field_unsupervised_2020,
	title = {Unsupervised {Discovery} of {Implicit} {Gender} {Bias}},
	url = {http://arxiv.org/abs/2004.08361},
	abstract = {Despite their prevalence in society, social biases are difficult to define and identify, primarily because human judgements in this domain can be unreliable. Therefore, we take an unsupervised approach to identifying gender bias at a comment or sentence level, and present a model that can surface text likely to contain bias. The main challenge in this approach is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, the core of our methodology relies on reducing the influence of confounds through propensity score matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms and references to their spouses, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.},
	author = {Field, Anjalie and Tsvetkov, Yulia},
	year = {2020},
	note = {arXiv: 2004.08361},
}

@inproceedings{Pradhan2012,
	title = {{CoNLL}-2012 {Shared} {Task} : {Modeling} {Multilingual} {Unrestricted} {Coreference} in {OntoNotes}},
	isbn = {978-1-937284-08-4},
	abstract = {The CoNLL-2011 shared task involved predicting coreference using OntoNotes data. Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ace entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.},
	booktitle = {Proceedings ofthe {Joint} {Conference} on {EMNLP} and {CoNLL}: {Shared} {Task}},
	author = {Pradhan, Sameer and Moschitti, Alessandro and Uryupina, Olga},
	year = {2012},
	pages = {1--40},
}

@article{sukthanker_anaphora_2018,
	title = {Anaphora and {Coreference} {Resolution}: {A} {Review}},
	url = {http://arxiv.org/abs/1805.11824},
	abstract = {Entity resolution aims at resolving repeated references to an entity in a document and forms a core component of natural language processing (NLP) research. This field possesses immense potential to improve the performance of other NLP fields like machine translation, sentiment analysis, paraphrase detection, summarization, etc. The area of entity resolution in NLP has seen proliferation of research in two separate sub-areas namely: anaphora resolution and coreference resolution. Through this review article, we aim at clarifying the scope of these two tasks in entity resolution. We also carry out a detailed analysis of the datasets, evaluation metrics and research methods that have been adopted to tackle this NLP problem. This survey is motivated with the aim of providing the reader with a clear understanding of what constitutes this NLP problem and the issues that require attention.},
	author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
	year = {2018},
	note = {arXiv: 1805.11824},
	keywords = {anaphora resolution, coreference resolution, deep learning, entity resolution, natural language, processing, sentiment analysis},
}

@article{niculae_quotus_2015,
	title = {{QUOTUS}: {The} structure of political media coverage as revealed by quoting patterns},
	doi = {10.1145/2736277.2741688},
	abstract = {Given the extremely large pool of events and stories available, media outlets need to focus on a subset of issues and aspects to convey to their audience. Outlets are often accused of exhibiting a systematic bias in this selection process, with different outlets portraying different versions of reality. However, in the absence of objective measures and empirical evidence, the direction and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns for quantifying and characterizing the degree to which media outlets exhibit systematic bias. We apply this framework to a massive dataset of news articles spanning the six years of Obama's presidency and all of his speeches, and reveal that a systematic pattern does indeed emerge from the outlet's quoting behavior. Moreover, we show that this pattern can be successfully exploited in an unsupervised prediction setting, to determine which new quotes an outlet will select to broadcast. By encoding bias patterns in a low-rank space we provide an analysis of the structure of political media coverage. This reveals a latent media bias space that aligns surprisingly well with political ideology and outlet type. A linguistic analysis exposes striking differences across these latent dimensions, showing how the different types of media outlets portray different realities even when reporting on the same events. For example, outlets mapped to the mainstream conservative side of the latent space focus on quotes that portray a presidential persona disproportionately characterized by negativity.},
	journal = {WWW 2015 - Proceedings of the 24th International Conference on World Wide Web},
	author = {Niculae, Vlad and Suen, Caroline and Zhang, Justine and Danescu-Niculescu-Mizil, Cristian and Leskovec, Jure},
	year = {2015},
	note = {arXiv: 1504.01383
ISBN: 9781450334693},
	keywords = {Media bias, News media, Political science, Quotes},
	pages = {798--808},
}

@inproceedings{Heuser2017,
	title = {Word {Vectors} in the {Eighteenth} {Century}},
	url = {https://dh2017.adho.org/program/abstracts/},
	abstract = {This talk explores how new vector-based approaches to computational semantics both afford new methods to digital humanities research, and raise interesting questions for eighteenth-century literary studies in particular. Given the scarcity of digital-humanities research on word vectors, work that seeks equally to explain, interpret, and demonstrate their potential seems particularly useful. With these goals in mind, this paper attempts first to unpack for a digital-humanities audience how word vectors work, with reference to the canonical analogy cited above: “man is to woman as king is to queen.” Second, in order to interpret word vectors’ conceptual implications for eighteenth-century literature, I move away from this canonical analogy to one central to a particularly influential argument in the period: “Learning is to Genius as Riches are to Virtue.” Lastly, I turn from this close reading of word vectors to methods of distant-reading analogies that lie implicit in eighteenth-century literature.},
	booktitle = {Digital {Humanities}},
	author = {Heuser, Ryan James},
	year = {2017},
}

@inproceedings{Kim2019,
	title = {Frowning {Frodo}, {Wincing} {Leia}, and a {Seriously} {Great} {Friendship}: {Learning} to {Classify} {Emotional} {Relationships} of {Fictional} {Characters}},
	url = {http://arxiv.org/abs/1903.12453},
	abstract = {The development of a fictional plot is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of stories and their development is the emotion between characters. In this paper, we combine these aspects into a unified framework to classify emotional relationships of fictional characters. We formalize it as a new task and describe the annotation of a corpus, based on fan-fiction short stories. The extraction pipeline which we propose consists of character identification (which we treat as given by an oracle here) and the relation classification. For the latter, we provide results using several approaches previously proposed for relation identification with neural methods. The best result of 0.45 F1 is achieved with a GRU with character position indicators on the task of predicting undirected emotion relations in the associated social network graph.},
	booktitle = {Proceedings of the {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({NAACL}-{HLT})},
	author = {Kim, Evgeny and Klinger, Roman},
	year = {2019},
	note = {arXiv: 1903.12453},
	pages = {647--653},
}

@inproceedings{Kim2017,
	title = {Investigating the {Relationship} between {Literary} {Genres} and {Emotional} {Plot} {Development}},
	doi = {10.18653/v1/w17-2203},
	abstract = {Literary genres are commonly viewed as being defined in terms of content and style. In this paper, we focus on one particular type of content feature, namely lexical ex-pressions of emotion, and investigate the hypothesis that emotion-related informa-tion correlates with particular genres. Us-ing genre classification as a testbed, we compare a model that computes lexicon-based emotion scores globally for complete stories with a model that tracks emotion arcs through stories on a subset of Project Gutenberg with five genres. Our main findings are: (a), the global emo-tion model is competitive with a large-vocabulary bag-of-words genre classifier (80 \% F 1); (b), the emotion arc model shows a lower performance (59 \% F 1) but shows complementary behavior to the global model, as indicated by a very good performance of an oracle model (94 \% F 1) and an improved performance of an ensem-ble model (84 \% F 1); (c), genres differ in the extent to which stories follow the same emotional arcs, with particularly uniform behavior for anger (mystery) and fear (ad-ventures, romance, humor, science fiction).},
	booktitle = {Proceedings of the {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	author = {Kim, Evgeny and Padó, Sebastian and Klinger, Roman},
	year = {2017},
	pages = {17--26},
}

@article{Lee2007,
	title = {A computational model of text reuse in ancient literary texts},
	abstract = {We propose a computational model of text reuse tailored for ancient literary texts, available to us often only in small and noisy samples. The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament. © 2007 Association for Computational Linguistics.},
	number = {June},
	journal = {ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
	author = {Lee, John},
	year = {2007},
	note = {ISBN: 9781932432862},
	pages = {472--479},
}

@inproceedings{leskovec2009meme,
	title = {Meme-tracking and the dynamics of the news cycle},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Leskovec, Jure and Backstrom, Lars and Kleinberg, Jon},
	year = {2009},
	pages = {497--506},
}

@article{Mohammad2018,
	title = {Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 {English} words},
	volume = {1},
	doi = {10.18653/v1/p18-1017},
	abstract = {Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD). We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words. We use Best-Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation. We show that the ratings obtained are vastly more reliable than those in existing lexicons. We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Mohammad, Saif M.},
	year = {2018},
	note = {ISBN: 9781948087322},
	pages = {174--184},
}

@article{Eckert2019,
	title = {The limits of meaning : {Social} indexicality, variation, and the cline of interiority},
	volume = {95},
	number = {4},
	journal = {Language},
	author = {Eckert, Penelope},
	year = {2019},
	keywords = {13, a successful linguistic theory, i dedicate it to, i have resisted the, iconicity, indexicality, labov 1972, not social, on my 2019 lsa, or practice which is, pragmatics, presidential address in new, semantics, semiotics, since it implies that, social meaning, term sociolinguistics for many, the memory of, there can be, this article is based, variation, years, york},
	pages = {751--776},
}

@book{Fairclough1993,
	title = {Discourse and {Social} {Change}},
	isbn = {0-7456-0674-1},
	abstract = {Today individuals working in a variety of disciplines are coming to recognize the ways in which changes in language use are linked to wider social and cultural processes, and hence are coming to appreciate the importance of using language analysis as a method for studying social change. But there does not yet exist a method of language analysis which is both theoretically adequate and practically usable. My main objective in this book, therefore, is to develop an approach to language analysis which can contribute to filling this gap - an approach which will be particularly useful for investigating change in language, and will be usable in studies of social and cultural change. To},
	publisher = {Polity Press},
	author = {Fairclough, Norman},
	year = {1992},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
ISSN: 1098-6596},
}

@article{Abu-Jbara2012,
	title = {Subgroup {Detection} in {Ideological} {Discussions}},
	url = {http://www.aclweb.org/anthology/P12-1042},
	abstract = {The rapid and continuous growth of social {\textbackslash}nnetworking sites has led to the emergence of {\textbackslash}nmany communities of communicating groups. {\textbackslash}nMany of these groups discuss ideological and {\textbackslash}npolitical topics. It is not uncommon that the {\textbackslash}nparticipants in such discussions split into two {\textbackslash}nor more subgroups. The members of each subgroup {\textbackslash}nshare the same opinion toward the discussion {\textbackslash}ntopic and are more likely to agree with {\textbackslash}nmembers of the same subgroup and disagree {\textbackslash}nwith members from opposing subgroups. In {\textbackslash}nthis paper, we propose an unsupervised approach {\textbackslash}nfor automatically detecting discussant {\textbackslash}nsubgroups in online communities. We analyze {\textbackslash}nthe text exchanged between the participants of {\textbackslash}na discussion to identify the attitude they carry {\textbackslash}ntoward each other and towards the various aspects {\textbackslash}nof the discussion topic. We use attitude {\textbackslash}npredictions to construct an attitude vector for {\textbackslash}neach discussant. We use clustering techniques {\textbackslash}nto cluster these vectors and, hence, determine {\textbackslash}nthe subgroup membership of each participant. {\textbackslash}nWe compare our methods to text clustering {\textbackslash}nand other baselines, and show that our method {\textbackslash}nachieves promising results.},
	number = {July},
	journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Abu-Jbara, Amjad and Dasigi, Pradeep and Diab, Mona and Radev, Dragomir},
	year = {2012},
	note = {ISBN: 9781937284244},
	pages = {399--409},
}

@incollection{ngai_introduction_nodate,
	title = {Introduction},
	booktitle = {Ugly feelings},
	author = {Ngai, Sianne},
	pages = {1--37},
}

@article{aslan_cash_2018,
	title = {‘{Cash} me ousside’: {A} citizen sociolinguistic analysis of online metalinguistic commentary},
	volume = {22},
	issn = {14679841},
	doi = {10.1111/josl.12303},
	abstract = {This study examines online metalinguistic commentary related to an Internet meme (i.e. ‘Cash me ousside/howbow dah’), in order to explore Internet users’ language ideologies. The meme, and its related YouTube metacommentary, places at its center a ‘non-standard’ utterance produced by a young teenage girl on a U.S. television talk show, which went viral. Drawing on citizen sociolinguistics – a means to explore how everyday citizens make sense of the world of language around them – the study offers an analysis of metalinguistic evaluations made by YouTube commenters about this particular utterance and its speaker. Our findings reveal that the teenager's sociolinguistically ambiguous manner of speaking is perceived as indexing multiple social categories including race, region, education, and class-linked imagined ‘spaces’ (e.g. ghetto, hood, the streets) – and that these categories overlap in complex, and not always predictable, configurations. Our analysis also highlights how evaluations regarding the authenticity and intelligibility of the speaker's performance interact with several of the aforementioned social categories.},
	number = {4},
	journal = {Journal of Sociolinguistics},
	author = {Aslan, Erhan and Vásquez, Camilla},
	year = {2018},
	keywords = {Internet memes, Web 2.0, citizen sociolinguistics, crossing, language ideologies, metacommentary},
	pages = {406--431},
}

@article{Bamman2014,
	title = {A {Bayesian} {Mixed} {Effects} {Model} of {Literary} {Character}},
	url = {http://www.aclweb.org/anthology/P14-1035},
	abstract = {We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models.},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)},
	author = {Bamman, David and Underwood, Ted and Smith, Noah A.},
	year = {2014},
	note = {ISBN: 9781937284725},
	pages = {370--379},
}

@article{trouble_judith_2020,
	title = {Judith {Butler}: {Mourning} {Is} a {Political} {Act} {Amid} the {Pandemic} and {Its} {Disparities}},
	author = {Trouble, Gender and Life, Precarious},
	year = {2020},
	pages = {1--8},
}

@article{butler_capitalism_2020,
	title = {Capitalism {Has} its {Limits}},
	journal = {Verso},
	author = {Butler, Judith},
	year = {2020},
	pages = {1--4},
}

@article{amir_modelling_2016,
	title = {Modelling {Context} with {User} {Embeddings} for {Sarcasm} {Detection} in {Social} {Media}},
	abstract = {We introduce a deep neural network for automated sarcasm detection. Recent work has emphasized the need for mod-els to capitalize on contextual features, be-yond lexical and syntactic cues present in utterances. For example, different speak-ers will tend to employ sarcasm regard-ing different subjects and, thus, sarcasm detection models ought to encode such speaker information. Current methods have achieved this by way of laborious feature engineering. By contrast, we pro-pose to automatically learn and then ex-ploit user embeddings, to be used in con-cert with lexical signals to recognize sar-casm. Our approach does not require elab-orate feature engineering (and concomi-tant data scraping); fitting user embed-dings requires only the text from their previous posts. The experimental results show that the our model outperforms a state-of-the-art approach leveraging an ex-tensive set of carefully crafted features.},
	journal = {Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)},
	author = {Amir, Silvio and Wallace, Byron C. and Lyu, Hao and Carvalho, Paula and Silva, Mario J.},
	year = {2016},
	note = {arXiv: 1607.00976},
	pages = {167--177},
}

@article{g_agamben_jl_nancy_r_esposito_s_benvenuto_d_dwivedi_s_mohan_r_ronchi_coronavirus_2020,
	title = {Coronavirus and {Philosophers}},
	url = {https://www.journal-psychoanalysis.eu/coronavirus-and-philosophers/%0Ahttps://antinomie.it/},
	journal = {European Journal of Psychoanalysis},
	author = {G. Agamben, J.L. Nancy, R. Esposito, S. Benvenuto, D. Dwivedi, S. Mohan, R. Ronchi, M. de Carolis},
	year = {2020},
	pages = {1--33},
}

@article{breitfeller_finding_2019,
	title = {Finding {Microaggressions} in the {Wild}: {A} {Case} for {Locating} {Elusive} {Phenomena} in {Social} {Media} {Posts}},
	doi = {10.18653/v1/d19-1176},
	abstract = {Microaggressions are subtle, often veiled, manifestations of human biases. These un-civil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microag-gressions and well-defined criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper , we devise a general but nuanced, com-putationally operationalizable typology of mi-croaggressions based on a small subset of mi-croaggression data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing mi-croaggressions. Finally, we analyze the trends that emerge from these new datasets.},
	author = {Breitfeller, Luke and Ahn, Emily and Jurgens, David and Tsvetkov, Yulia},
	year = {2019},
	pages = {1664--1674},
}

@article{newell_assessing_2017,
	title = {Assessing the {Verifiability} of {Attributions} in {News} {Text}},
	url = {https://www.aclweb.org/anthology/I17-1076},
	abstract = {When reporting the news, journalists rely on the statements of stakeholders, experts, and officials. The attribution of such a statement is verifiable if its fidelity to the source can be confirmed or denied. In this paper, we develop a new NLP task: determining the verifiability of an attribution based on linguistic cues. We operationalize the notion of verifiability as a score between 0 and 1 using human judgments in a comparison-based approach. Using crowdsourcing, we create a dataset of verifiability-scored attributions, and demonstrate a model that achieves an RMSE of 0.057 and Spearman\{'\}s rank correlation of 0.95 to human-generated scores. We discuss the application of this technique to the analysis of mass media.},
	journal = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Newell, Edward and Schang, Ariane and Margolin, Drew and Ruths, Derek},
	year = {2017},
	pages = {754--763},
}

@article{dean_biopolitics_2012,
	title = {The biopolitics of pleasure},
	volume = {111},
	issn = {00382876},
	doi = {10.1215/00382876-1596245},
	abstract = {This essay reconsiders biopolitical theory in relation to Michel Foucault's pursuit of the problematic of pleasure during the final decade of his work. The question of pleasure straddles the temporal and methodological gulf that separates the first volume of his History of Sexuality from the second and third volumes eight years later. In seeking to retrieve the full complexity of Foucault's account of pleasure, my essay offers a critique of Giorgio Agamben's elision of pleasure from biopolitical theory, on one hand, and queer theoretical reductions of pleasure to sexual pleasure, on the other. Arguing that the category of pleasure is recruited to heterogeneous positions in Foucault's work, I trace the various purposes that it is enlisted to serve in his thinking, as well as the topological structure he evokes to conceptualize pleasure - that of the spiral. His famous "spirals of power and pleasure" provide an image of the inseparability of power not only from knowledge but also from pleasure. By way of Georges Canguilhem's reading of James Watson and Francis Crick, I contend that Foucault borrowed the model of those spirals from the double helix of DNA - and that the iconography of the life sciences thus provided the philosopher of biopower with a model for conceptualizing how power takes hold of life even at the "subindividual" level. The double helix gave Foucault not only a model for the imbrication of pleasure with power but also a biopolitical intuition about the molecularization of life. What Nikolas Rose calls the "molecularization of vitality" has displaced the human body as the principal unit of biopolitical interest, in favor less of the macro-effects characteristic of populations than of the micro-processes of subindividual life. Yet, in multiplying the sites of power's penetration, this biopolitical disaggregation of human anatomy has multiplied sites of possible pleasure too. I suggest that his preoccupation with rendering bodies "infinitely more susceptible to pleasure" derived from Foucault's conviction that the qualitative instability of pleasure makes it difficult for systems of knowledge to capture. Precisely insofar as pleasure thus may appear incompatible with philosophical or political seriousness, it remained a vital concern in Foucault's thinking. © 2012 Duke University Press.},
	number = {3},
	journal = {South Atlantic Quarterly},
	author = {Dean, Tim},
	year = {2012},
	pages = {477--496},
}

@article{brown_foucault_2015,
	title = {Foucault ’ s {Birth} of {Biopolitics} {Lectures} : {Charting} {Neoliberal} {Political} {Rationality}},
	number = {May 2019},
	journal = {Undoing the Demos},
	author = {Brown, Wendy},
	year = {2015},
	pages = {47--78},
}

@article{bhatia_semantic_2018,
	title = {Semantic incompleteness in privacy policy goals},
	doi = {10.1109/RE.2018.00025},
	abstract = {Companies that collect personal information online often maintain privacy policies that are required to accurately reflect their data practices and privacy goals. To be comprehensive and flexible for future practices, policies contain ambiguity that summarize practices over multiple types of products and business contexts. Ambiguity in data practice descriptions undermines policies as an effective way to communicate system design choices to users, and as a reliable regulatory mechanism. In this paper, we report an investigation to identify incompleteness by representing data practice descriptions as semantic frames. The approach is a grounded analysis to discover which data actions and semantic roles correspond are needed to construct complete data practice descriptions. Our results include 281 data action instances obtained from 202 manually annotated statements across five privacy policies. Therein, we identified 878 instances of 17 types of semantic roles. Incomplete data practice descriptions undermine user comprehension, and can affect the user's perceived privacy risk, which we measure using factorial vignette surveys. We observed that user perception of risk decreases when two roles are present in a statement: the condition under which a data action is performed, and the purpose for which the user's information is used.},
	journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
	author = {Bhatia, Jaspreet and Breaux, Travis D.},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538674185},
	keywords = {Natural language processing, Privacy, Privacy risk, Semantic frames, Semantic roles},
	pages = {159--169},
}

@article{annals_when_nodate,
	title = {When {Chi}-tzu of {Yen}-ling {Hangs} {Up} {His} {Sword} 1},
	author = {Annals, Autumn},
	pages = {5--6},
}

@incollection{dean_homosexuality_2001,
	title = {Homosexuality and psychoanalysis: an introduction},
	booktitle = {Homosexuality and {Psychoanalysis}},
	author = {Dean, Tim and Lane, Christopher},
	year = {2001},
}

@article{bekker_parodies_2020,
	title = {Parodies of whiteness: {Die} {Antwoord} and the politics of race, gender, and class in {South} {Africa}},
	volume = {49},
	issn = {14698013},
	doi = {10.1017/S0047404519000630},
	abstract = {The dramatic reconfiguration of the social, political, and ideological order in South Africa since 1990/1994 has demanded a concomitant reconceptualization of (white) Afrikaner notions of self and belonging in the (new) nation. In this article, we draw on recent developments in the study of varidirectional voicing (polyphony), performance, and mediatization to examine how the South African rap-rave group Die Antwoord makes use of parody and metaparody in their music to critique emerging 'new Afrikaner' identities and the racial, class, and gender configurations on which they are based. We also discuss the structural limits of these critiques and the political potential of (meta)parodic performance more generally.},
	number = {1},
	journal = {Language in Society},
	author = {Bekker, Ian and Levon, Erez},
	year = {2020},
	keywords = {(Meta)parody, South Africa, class, gender, performance, polyphony, race},
	pages = {115--147},
}

@article{wen_face_2019,
	title = {Face {Reconstruction} from {Voice} using {Generative} {Adversarial} {Networks}},
	url = {https://github.com/cmu-mlsp/reconstructing_faces_from_voices},
	abstract = {Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling-reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task-cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in https://github.com/cmu-mlsp/reconstructing\_faces\_from\_voices},
	number = {NeurIPS},
	journal = {Conference on Neural Information Processing Systems},
	author = {Wen, Yandong and Singh, Rita and Raj, Bhiksha},
	year = {2019},
	pages = {5265--5274},
}

@article{mathet_unified_2015,
	title = {The {Unified} and {Holistic} {Method} {Gamma} for {Inter}-{Annotator} {Agreement} {Measure} and {Alignment}},
	volume = {41},
	issn = {04194217},
	doi = {10.1162/COLI},
	abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
	number = {3},
	journal = {Computational Linguistics},
	author = {Mathet, Yann and Widlöcher, Antoine and Métivier, Jean-Phillipe},
	year = {2015},
	note = {ISBN: 9781608459858},
	pages = {437--479},
}

@article{Vedres2019,
	title = {Gendered behavior as a disadvantage in open source software development},
	volume = {8},
	issn = {21931127},
	url = {http://dx.doi.org/10.1140/epjds/s13688-019-0202-z},
	doi = {10.1140/epjds/s13688-019-0202-z},
	abstract = {Women are severely marginalized in software development, especially in open source. In this article we argue that disadvantage is more due to gendered behavior than to categorical discrimination: women are at a disadvantage because of what they do, rather than because of who they are. Using data on entire careers of users from GitHub.com, we develop a measure to capture the gendered pattern of behavior: We use a random forest prediction of being female (as opposed to being male) by behavioral choices in the level of activity, specialization in programming languages, and choice of partners. We test differences in success and survival along both categorical gender and the gendered pattern of behavior. We find that 84.5\% of women’s disadvantage (compared to men) in success and 34.8\% of their disadvantage in survival are due to the female pattern of their behavior. Men are also disadvantaged along their interquartile range of the female pattern of their behavior, and users who don’t reveal their gender suffer an even more drastic disadvantage in survival probability. Moreover, we do not see evidence for any reduction of these inequalities in time. Our findings are robust to noise in gender recognition, and to taking into account particular programming languages, or decision tree classes of gendered behavior. Our results suggest that fighting categorical gender discrimination will have a limited impact on gender inequalities in open source software development, and that gender hiding is not a viable strategy for women.},
	number = {1},
	journal = {EPJ Data Science},
	author = {Vedres, Balazs and Vasarhelyi, Orsolya},
	year = {2019},
	note = {arXiv: 1810.03005
Publisher: The Author(s)},
	keywords = {Gender inequality, Gendered behavior, Open source, Software development},
}

@article{nguyen_why_2014,
	title = {Why gender and age prediction from tweets is hard: {Lessons} from a crowdsourcing experiment},
	abstract = {There is a growing interest in automatically predicting the gender and age of authors from texts. However, most research so far ignores that language use is related to the social identity of speakers, which may be different from their biological identity. In this paper, we combine insights from sociolinguistics with data collected through an online game, to underline the importance of approaching age and gender as social variables rather than static biological variables. In our game, thousands of players guessed the gender and age of Twitter users based on tweets alone. We show that more than 10\% of the Twitter users do not employ language that the crowd associates with their biological sex. It is also shown that older Twitter users are often perceived to be younger. Our findings highlight the limitations of current approaches to gender and age prediction from texts.},
	journal = {COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers},
	author = {Nguyen, Dong and Trieschnigg, Dolf and Dǒgrüoz, A. Seza and Gravel, Rilana and Theune, Marïet and Meder, Theo and De Jong, Franciska},
	year = {2014},
	note = {ISBN: 9781941643266},
	pages = {1950--1961},
}

@article{Kosinski2018,
	title = {Deep {Neural} {Networks} {Are} {More} {Accurate} {Than} {Humans} at {Detecting} {Sexual} {Orientation} {From} {Facial} {Images}},
	volume = {114},
	issn = {00223514},
	url = {https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual},
	doi = {http://dx.doi.org/10.1037/pspa0000098},
	abstract = {We show that faces contain much more information about sexual orientation than can be perceived or interpreted by the human brain. We used deep neural networks to extract features from 35,326 facial images. These features were entered into a logistic regression aimed at classifying sexual orientation. Given a single facial image, a classifier could correctly distinguish between gay and heterosexual men in 81\% of cases, and in 71\% of cases for women. Human judges achieved much lower accuracy: 61\% for men and 54\% for women. The accuracy of the algorithm increased to 91\% and 83\%, respectively, given five facial images per person. Facial features employed by the classifier included both fixed (e.g., nose shape) and transient facial features (e.g., grooming style). Consistent with the prenatal hormone theory of sexual orientation, gay men and women tended to have gender-atypical facial morphology, expression, and grooming styles. Prediction models aimed at gender alone allowed for detecting gay males with 57\% accuracy and gay females with 58\% accuracy. Those findings advance our understanding of the origins of sexual orientation and the limits of human perception. Additionally, given that companies and governments are increasingly using computer vision algorithms to detect people’s intimate traits, our findings expose a threat to the privacy and safety of gay men and women.},
	number = {2},
	journal = {Journal of Personality and Social Psychology},
	author = {Wang, Yilun and Kosinski, Michal},
	year = {2018},
	note = {arXiv: 10.17605/OSF.IO/HV28A},
	keywords = {computational social science, facial morphology, prenatal hormone theory, sexual orientation},
	pages = {246--257},
}

@article{miura_author_2017,
	title = {Author profiling with word+character neural attention network: {Notebook} for {PAN} at {CLEF} 2017},
	volume = {1866},
	issn = {16130073},
	abstract = {This paper describes neural network models that we prepared for the author profiling task of PAN@CLEF 2017. In previous PAN series, statistical models using a machine learning method with a variety of features have shown superior performances in author profiling tasks. We decided to tackle the author profiling task using neural networks. Neural networks have recently shown promising results in NLP tasks. Our models integrate word information and character information with multiple neural network layers. The proposed models have marked joint accuracies of 64-86\% in the gender identification and the language variety identification of four languages.},
	journal = {CEUR Workshop Proceedings},
	author = {Miura, Yasuhide and Taniguchi, Tomoki and Taniguchi, Motoki and Ohkuma, Tomoko},
	year = {2017},
}

@article{chang_dont_2020,
	title = {Don't {Let} {Me} {Be} {Misunderstood} : {Comparing} {Intentions} and {Perceptions} in {Online} {Discussions}},
	journal = {Proc. of WWW},
	author = {Chang, Jonathan P and Cheng, Justin and Danescu-niculescu-mizil, Cristian},
	year = {2020},
	note = {ISBN: 9781450370233},
}

@article{shang_demographic_2018,
	title = {Demographic {Inference} {Via} {Knowledge} {Transfer} in {Cross}-{Domain} {Recommender} {Systems}},
	volume = {2018-Novem},
	issn = {15504786},
	doi = {10.1109/ICDM.2018.00162},
	abstract = {User demographics such as age and gender are very useful in recommender systems for applications such as personalization services and marketing, but may not always be available for individual users. Existing approaches can infer users' private demographics based on ratings, given labeled data from users who share demographics. However, such labeled information is not always available in many e-commerce services, particularly small online retailers and most media sites, for which no user registration is required. We introduce a novel probabilistic matrix factorization model for demographic transfer that enables knowledge transfer from the source domain, in which users' ratings and the corresponding demographics are available, to the target domain, in which we would like to infer unknown user demographics from ratings. Our proposed method is based on two observations: (1) Items from different but related domains may share the same latent factors such as genres and styles, and (2) Users who share similar demographics are likely to prefer similar genres across domains. This approach can align latent factors across domains that share neither common users nor common items, associating user demographics with latent factors in a unified framework. Experiments on cross-domain datasets demonstrate that the proposed method consistently improves demographic classification accuracy over existing methods.},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	author = {Shang, Jin and Sun, Mingxuan and Collins-Thompson, Kevyn},
	year = {2018},
	note = {ISBN: 9781538691588},
	keywords = {Demographic inference, Matrix factorization, Recommender systems},
	pages = {1218--1223},
}

@article{wang_demographic_2019,
	title = {Demographic inference and representative population estimates from multilingual social media data},
	doi = {10.1145/3308558.3313684},
	abstract = {Social media provide access to behavioural data at an unprecedented scale and granularity. However, using these data to understand phenomena in a broader population is difficult due to their non-representativeness and the bias of statistical inference tools towards dominant languages and groups. While demographic attribute inference could be used to mitigate such bias, current techniques are almost entirely monolingual and fail to work in a global environment. We address these challenges by combining multilingual demographic inference with post-stratification to create a more representative population sample. To learn demographic attributes, we create a new multimodal deep neural architecture for joint classification of age, gender, and organization-status of social media users that operates in 32 languages. This method substantially outperforms current state of the art while also reducing algorithmic bias. To correct for sampling biases, we propose fully interpretable multilevel regression methods that estimate inclusion probabilities from inferred joint population counts and ground-truth population counts. In a large experiment over multilingual heterogeneous European regions, we show that our demographic inference and bias correction together allow for more accurate estimates of populations and make a significant step towards representative social sensing in downstream applications with multilingual social media.},
	journal = {The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019},
	author = {Wang, Zijian and Hale, Scott A. and Adelani, David and Grabowicz, Przemyslaw A. and Hartmann, Timo and Flöck, Fabian and Jurgens, David},
	year = {2019},
	note = {arXiv: 1905.05961
ISBN: 9781450366748},
	keywords = {Deep Learning, Demographics, Inclusion Probabilities, Latent Attribute Inference, Multilingual, Post-stratification, Social Media},
	pages = {2056--2067},
}

@article{preot_user-level_2018,
	title = {User-{Level} {Race} and {Ethnicity} {Predictors} from {Twitter} {Text}},
	url = {https://www.aclweb.org/anthology/C18-1130},
	abstract = {User demographic inference from social media text has the potential to improve a range of downstream applications, including real-time passive polling or quantifying demographic bias. This study focuses on developing models for user-level race and ethnicity prediction. We introduce a data set of users who self-report their race/ethnicity through a survey, in contrast to previous approaches that use distantly supervised data or perceived labels. We develop predictive models from text which accurately predict the membership of a user to the four largest racial and ethnic groups with up to .884 AUC and make these available to the research community},
	journal = {Proceedings of the 27th International Conference on Computational Linguistics},
	author = {Preot, Daniel and Ungar, Lyle},
	year = {2018},
	pages = {1534--1545},
}

@article{ghazouani_assessing_2019,
	title = {Assessing socioeconomic status of {Twitter} users: {A} survey},
	volume = {2019-Septe},
	issn = {13138502},
	doi = {10.26615/978-954-452-056-4_046},
	abstract = {Every day, the emotion and opinion of different people across the world are reflected in the form of short messages using microblogging platforms. Despite the existence of enormous potential introduced by this data source, the Twitter community is still ambiguous and is not fully explored yet. While there are a huge number of studies examining the possibilities of inferring gender and age, there exist hardly researches on socioeconomic status (SES) inference of Twitter users. As socioeconomic status is essential to treating diverse questions linked to human behavior in several fields (sociology, demography, public health, etc.), we conducted a comprehensive literature review of SES studies, inference methods, and metrics. With reference to the research on literature's results, we came to outline the most critical challenges for researchers. To the best of our knowledge, this paper is the first review that introduces the different aspects of SES inference. Indeed, this article provides the benefits for practitioners who aim to process and explore Twitter SES inference.},
	journal = {International Conference Recent Advances in Natural Language Processing, RANLP},
	author = {Ghazouani, Dhouha and Lancieri, Luigi and Ounelli, Habib and Jebari, Chaker},
	year = {2019},
	note = {ISBN: 9789544520557},
	pages = {388--398},
}

@article{zhang_multi-source_2019,
	title = {Multi-source user attribute inference based on hierarchical auto-encoder},
	doi = {10.1145/3338533.3366599},
	abstract = {With the rapid development of Online Social Networks (OSNs), it is crucial to construct users' portraits from their dynamic behaviors to address the increasing needs for customized information services. Previous work on user attribute inference mainly concentrated on developing advanced features/models or exploiting external information and knowledge but ignored the contradiction between dynamic behaviors and stable demographic attributes, which results in deviation of user understanding. To address the contradiction and accurately infer the user attributes, we propose a Multi-source User Attribute Inference algorithm based on Hierarchical Auto-encoder (MUAI-HAE). The basic idea is that: The shared patterns among the same individual's behaviors on different OSNs well indicate his/her stable demographic attributes. The hierarchical auto-encoder is introduced to realize this idea by discovering the underlying non-linear correlation between different OSNs. The unsupervised scheme in shared pattern learning alleviates the requirements for the cross-OSN user account and improves the practicability. Off-the-shelf classification methods are then utilized to infer user attributes from the derived shared behavior patterns. The experiments on the real-world datasets from three OSNs demonstrate the effectiveness of the proposed method.},
	journal = {1st ACM International Conference on Multimedia in Asia, MMAsia 2019},
	author = {Zhang, Boyu and Ding, Xiangguo and Huang, Xiaowen and Cao, Yang and Sang, Jitao and Yu, Jian},
	year = {2019},
	note = {ISBN: 9781450368414},
	keywords = {Demographic attributes, Dynamic behaviors, Hierarchical auto-encoder, Multi-source},
}

@article{inoue_what_2004,
	title = {What {Does} {Language} {Remember}?: {Indexical} {Inversion} and the {Naturalized} {History} of {Japanese} {Women}},
	volume = {14},
	issn = {1055-1360},
	doi = {10.1525/jlin.2004.14.1.39},
	abstract = {... and denaturalizing the politi- cally and culturally normalized indexical order (and thereby language ideology ). ... as “schoolgirl speech” are associated today with “women's language ” or the ... previously monopolized by boys and men, including foreign languages , classical Chinese ... {\textbackslash}n},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {Inoue, Miyako},
	year = {2004},
	pages = {39--56},
}

@inproceedings{sennrich_controlling_2016,
	title = {Controlling {Politeness} in {Neural} {Machine} {Translation} via {Side} {Constraints}},
	isbn = {978-1-941643-91-4},
	abstract = {Many languages use honorifics to express po-liteness, social distance, or the relative so-cial status between the speaker and their ad-dressee(s). In machine translation from a lan-guage without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of po-liteness in the output. In this paper, we per-form a pilot study to control honorifics in neu-ral machine translation (NMT) via side con-straints, focusing on English→German. We show that by marking up the (English) source side of the training data with a feature that en-codes the use of honorifics on the (German) target side, we can control the honorifics pro-duced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial im-provements are possible by constraining the translation to the desired level of politeness.},
	booktitle = {Proceedings of {NAACL}-{HLT} 2016},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {35--40},
}

@article{poesio_mategnome_2004,
	title = {The {MATE}/{GNOME} proposals for anaphoric annotation, revisited},
	abstract = {In the five years since it was proposed, the MATE scheme for anaphoric annotation has been used in a variety of annotation projects, and the resulting corpora have been used to study both anaphora resolution and NL gener- ation. Annotation tools inspired by the propos- als have been used in some of these projects. In this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest a few modifications. 1},
	journal = {Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue},
	author = {Poesio, Massimo},
	year = {2004},
}

@article{recasens_annotating_2012,
	title = {Annotating near-identity from coreference disagreements},
	abstract = {We present an extension of the coreference annotation in the English NP4E and the Catalan AnCora-CA corpora with near-identity relations, which are borderline cases of coreference. The annotated subcorpora have 50K tokens each. Near-identity relations, as presented by Recasens et al. (2010; 2011), build upon the idea that identity is a continuum rather than an either/or relation, thus introducing a middle ground category to explain currently problematic cases. The first annotation effort that we describe shows that it is not possible to annotate near-identity explicitly because subjects are not fully aware of it. Therefore, our second annotation effort used an indirect method, and arrived at near-identity annotations by inference from the disagreements between five annotators who had only a two-alternative choice between coreference and non-coreference. The results show that whereas as little as 2-6\% of the relations were explicitly annotated as near-identity in the former effort, up to 12-16\% of the relations turned out to be near-identical following the indirect method of the latter effort.},
	journal = {Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC 2012},
	author = {Recasens, Marta and Antonia Marti, M. and Orasany, Constantin},
	year = {2012},
	note = {ISBN: 9782951740877},
	keywords = {Coreference, Corpus annotation, Near-identity},
	pages = {165--172},
}

@article{pradhan_unrestricted_2007,
	title = {Unrestricted coreference: {Identifying} entities and events in ontonotes},
	doi = {10.1109/ICSC.2007.93},
	abstract = {Most research in the field of anaphora or coreference detection has been limited to noun phrase coreference, usually on a restricted set of entities, such as ACE entities. In part, this has been due to the lack of corpus resources tagged with general anaphoric coreference. The OntoNotes project is creating a large-scale, accurate corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a limited set of entity types. The coreference layer in OntoNotes constitutes one part of a multi-layer, integrated annotation of shallow semantic structure in text. This paper presents an initial model for unrestricted coreference based on this data that uses a machine learning architecture with state-of-the-art features. Significant improvements can be expected from using such cross-layer information for training predictive models. This paper describes the coreference annotation in OntoNotes, presents the baseline model, and provides an analysis of the contribution of this new resource in the context of recent MUC and ACE results. © 2007 IEEE.},
	journal = {ICSC 2007 International Conference on Semantic Computing},
	author = {Pradhan, Sameer S. and Ramshaw, Lance and Weischedel, Ralph and MacBride, Jessica and Micciulla, Linnea},
	year = {2007},
	note = {ISBN: 0769529976},
	pages = {446--453},
}

@inproceedings{hovy_ontonotes_2006,
	title = {{OntoNotes} : {The} 90\% {Solution}},
	author = {Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
	year = {2006},
	note = {Issue: June},
	pages = {57--60},
}

@inproceedings{vanmassenhove_getting_2018,
	title = {Getting gender right in neural machine translation},
	isbn = {978-1-948087-84-1},
	abstract = {Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying “I am happy” in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained/recovered. The same sentence would become either “Je suis heureux”, for a male speaker or “Je suis heureuse” for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or even on the level of syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is twofold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2018},
	author = {Vanmassenhove, Eva and Hardmeier, Christian and Way, Andy},
	year = {2018},
	pages = {3003--3008},
}

@article{tiedemann_neural_2018,
	title = {Neural {Machine} {Translation} with {Extended} {Context}},
	doi = {10.18653/v1/w17-4811},
	abstract = {We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.},
	author = {Tiedemann, Jörg and Scherrer, Yves},
	year = {2018},
	note = {arXiv: 1708.05943},
	pages = {82--92},
}

@article{gershon_breaking_2010,
	title = {Breaking {Up} {Is} {Hard} {To} {Do} : {Media}},
	volume = {20},
	issn = {00907324},
	doi = {10.1111/j.1548-1395.2010.01076.x.H},
	abstract = {When U.S. college students tell breakup stories, they often indicate what medium was used for each exchange. In this article, I explore what this practice reveals about people’s media ideologies. By extending previous scholarship on language ideologies to media, I trace how switching media or refusing to switch media contributes to the labor of disconnecting the relationship, determining whether phrases such as “it’s over” are effective or not.},
	number = {2},
	journal = {Journal of Linguistic Anthropology},
	author = {Gershon, Ilana},
	year = {2010},
	note = {ISBN: 9780255367233},
	pages = {389--405},
}

@article{milroy_language_2001,
	title = {Language ideologies and the consequences of standardization},
	volume = {5},
	issn = {1360-6441},
	doi = {10.1111/1467-9481.00163},
	abstract = {This paper explores the eects of the standard language ideology on attitudes to language of nonlinguists and of language specialists, and considers how far linguists themselves have been aected by ± and have contributed to ± this ideology. The primary de®nition of standardization is taken to be the imposition of uniformity upon a class of objects. Attitudes to language within standard language cultures are then reviewed and contrasted with unstandardized situ-ations, in which the boundaries of languages are indeterminate. It is therefore suggested that determinate languages, such as English, may be de®ned more by ideologies than by their internal structures. Some eects of standardization on the work of linguists are then reviewed. This is followed by a discussion of the importance of the process of legitimization in contributing to the standard language culture, and of the contribution of language specialists themselves to this process. Finally, certain matters arising are reviewed.},
	number = {4},
	journal = {Journal of Sociolinguistics},
	author = {Milroy, James},
	year = {2001},
	keywords = {correctness, elaboration of function, historicity, ideology, legitimization, standardization},
	pages = {530--555},
}

@article{gonen_lipstick_2019,
	title = {Lipstick on a {Pig}: {Debiasing} {Methods} {Cover} up {Systematic} {Gender} {Biases} in {Word} {Embeddings} {But} do not {Remove} {Them}},
	url = {http://arxiv.org/abs/1903.03862},
	abstract = {Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between "gender-neutralized" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
	author = {Gonen, Hila and Goldberg, Yoav},
	year = {2019},
	note = {arXiv: 1903.03862},
	pages = {609--614},
}

@article{zhao_gender_2019,
	title = {Gender {Bias} in {Contextualized} {Word} {Embeddings}},
	abstract = {In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2019},
	note = {arXiv: 1904.03310},
}

@incollection{Bucholtz2010,
	address = {Edinburgh},
	title = {Locating {Identity} in {Language}},
	isbn = {978-0-7486-3576-4},
	booktitle = {Language and {Identities}},
	publisher = {Edinburgh University Press},
	author = {Bucholtz, Mary and Hall, Kira},
	editor = {Llamas, Carmen and Watt, Dominic},
	year = {2010},
	keywords = {★},
	pages = {18--28},
}

@article{hazen_sociolinguistic_2018,
	title = {Sociolinguistic outreach for the {New} {South}: {Looking} back to move ahead},
	journal = {Language Variety in the New South: Contemporary Perspectives on Change and Variation},
	author = {Hazen, Kirk},
	year = {2018},
	note = {ISBN: 9781469638829},
	pages = {321--343},
}

@inproceedings{10.1145/3292522.3326013,
	address = {New York, NY, USA},
	title = {As the {Tweet}, so the {Reply}? {Gender} {Bias} in {Digital} {Communication} with {Politicians}},
	isbn = {978-1-4503-6202-3},
	url = {https://doi.org/10.1145/3292522.3326013},
	doi = {10.1145/3292522.3326013},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Web} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Mertens, Armin and Pradel, Franziska and Rozyjumayeva, Ayjeren and Wäckerle, Jens},
	year = {2019},
	note = {Series Title: WebSci ’19},
	keywords = {bias, dictionary analysis, gender, topic models, twitter},
	pages = {193--201},
}

@article{stier_social_2018,
	title = {Social {Media} {Monitoring} for the {German} federal election 2017},
	url = {https://dbk.gesis.org/dbksearch/sdesc2.asp?no=6926&db=e&doi=10.4232/1.12992},
	doi = {10.4232/1.12992},
	abstract = {Social Media Monitoring of the German Federal Election Campaign 2017 This dataset contains results from the social media monitoring of Facebook and Twitter for the German federal election campaign 2017. The project collected the tweets and Facebook posts of political candidates and organizations and the engagement of users with these contents – retweets and @-mentions on Twitter, comments, shares and likes on Facebook. Finally, all messages on Twitter containing at least one keyword denoting central political topics were collected. All data was publicly available at the time of data collection. The collected data is proprietary and owned by Facebook and Twitter. Due to this and with respect to privacy restrictions, only the following aspects of the data can be shared: (1) A list of all candidates that were considered in the project, their key attributes and the identification of their respective Twitter accounts and Facebook pages. Candidate dataset: Full surname, all first names of the candidate; academic title and name pre- or suffixes (if they exist); URL of the first Facebook account; URL of the second Facebook account; URL of the Twitter account; candidate is placed on a party list; candidate’s place on the party list; candidate is a direct candidate in one of the constituencies; official number and official name of the constituency in which the candidate is running for a direct mandate; state; candidate is a member of the federal parliament (Bundestag); party of the candidate; sex, age (year of birth); place of residence; place of birth; profession. Additionally coded was: unique ID. (2) Lists of organizations relevant during an election campaign, i.e. political parties and important gatekeepers, along with their respective Twitter and Facebook accounts. (3) A list of tweet IDs which can be used to retrieve the tweets we collected during our research period.},
	author = {Stier, Sebastian and Bleier, Arnim and Bonart, Malte and Mörsheim, Fabian and Bohlouli, Mahdi and Nizhegorodov, Margarita and Posch, Lisa and Maier, Jürgen and Rothmund, Tobias and Staab, Steffen},
	year = {2018},
	note = {ISBN: 9781450362023},
	keywords = {acm reference format, and jens wäck-, armin mertens, ayjeren rozyjumayeva, bias, dictionary analysis, franziska pradel, gender, topic models, twitter},
	pages = {193--201},
}

@article{nerghes_refugeemigrant_2018,
	title = {The refugee/migrant crisis dichotomy on twitter: {A} network and sentiment perspective},
	doi = {10.1145/3201064.3201087},
	abstract = {Media reports, political statements, and social media debates on the refugee/migrant crisis shape the ways in which people and societies respond to those displaced people arriving at their borders world wide. These current events are framed and experienced as a crisis, entering the media, capturing worldwide political attention, and producing diverse and contradictory discourses and responses. The labels “migrant” and “refugee” are frequently distinguished and conflated in traditional as well as social media when describing the same groups of people. In this paper, we focus on the simultaneous struggle over meaning, legitimization, and power in representations of the refugee crisis, through the specific lens of Twitter. The 369,485 tweets analyzed in this paper cover two days after a picture of Alan Kurdi - a three-year-old Syrian boy who drowned in the Mediterranean Sea while trying to reach Europe with his family - made global headlines and sparked wide media engagement. More specifically, we investigate the existence of the dichotomy between the “deserving” refugee versus the “undeserving” migrant, as well as the relationship between sentiment expressed in tweets, their influence, and the popularity of Twitter users involved in this dichotomous characterization of the crisis. Our results show that the Twitter debate was predominantly focused on refugee related hashtags and that those tweets containing such hashtags were more positive in tone. Furthermore, we find that popular Twitter users as well as popular tweets are characterized by less emotional intensity and slightly less positivity in the debate, contrary to prior expectations. Co-occurrence networks expose the structure underlying hashtag usage and reveal a refugee-centric core of meaning, yet divergent goals of some prominent users. As social media become increasingly prominent venues for debate over a crisis, how and why people express their opinions offer valuable insights into the nature and direction of these debates.},
	journal = {WebSci 2018 - Proceedings of the 10th ACM Conference on Web Science},
	author = {Nerghes, Adina and Lee, Ju Sung},
	year = {2018},
	note = {ISBN: 9781450355636},
	keywords = {Network analysis, Refugee crisis, Sentiment analysis, Twitter},
	pages = {271--280},
}

@book{burke1966language,
	title = {Language {As} {Symbolic} {Action}: {Essays} on {Life}, {Literature}, and {Method}},
	isbn = {978-0-520-00192-3},
	url = {https://books.google.com/books?id=HXF3HMi1zQ4C},
	publisher = {University of California Press},
	author = {Burke, Kenneth},
	year = {1966},
}

@article{Cheng2014,
	title = {Can cascades be predicted?},
	doi = {10.1145/2566486.2567997},
	abstract = {On many social networking web sites such as Facebook and Twitter, resharing or reposting functionality allows users to share others' content with their own friends or followers. As content is reshared from user to user, large cascades of reshares can form. While a growing body of research has focused on analyzing and characterizing such cascades, a recent, parallel line of work has argued that the future trajectory of a cascade may be inherently unpredictable. In this work, we develop a framework for addressing cascade prediction problems. On a large sample of photo reshare cascades on Facebook, we find strong performance in predicting whether a cascade will continue to grow in the future. We find that the relative growth of a cascade becomes more predictable as we observe more of its reshares, that temporal and structural features are key predictors of cascade size, and that initially, breadth, rather than depth in a cascade is a better indicator of larger cascades. This prediction performance is robust in the sense that multiple distinct classes of features all achieve similar performance. We also discover that temporal features are predictive of a cascade's eventual shape. Observing independent cascades of the same content, we find that while these cascades differ greatly in size, we are still able to predict which ends up the largest. Copyright is held by the International World Wide Web Conference Committee (IW3C2).},
	journal = {WWW 2014 - Proceedings of the 23rd International Conference on World Wide Web},
	author = {Cheng, Justin and Adamic, Lada A. and Dow, P. Alex and Kleinberg, Jon and Leskovec, Jure},
	year = {2014},
	note = {arXiv: 1403.4608
ISBN: 9781450327442},
	keywords = {Cascade prediction, Contagion, Information diffusion},
	pages = {925--935},
}

@inproceedings{he2016deep,
	title = {Deep residual learning for image recognition},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{sangeetha_syntheses_2006,
	title = {Syntheses of novel derivatives of 2-acetylfuro[2,3-a]carbazoles, benzo[1,2-b]-1,4-thiazepino[2,3-a]carbazoles and 1-acetyloxycarbazole-2- carbaldehydes},
	volume = {45},
	issn = {03764699},
	doi = {10.1002/chin.200650130},
	abstract = {An elegant one pot syntheses of the titled compounds 3a-d, 4a-d and 6a-d have been presented starting from 1-hydroxycarbazole-2-carbaldehydes 2a-d in good yields. Treatment of 1-hydroxycarbazole-2-carbaldehydes 2a-d with chloroacetone and with o-aminothiophenol have afforded the novel 2-acetylfuro[2,3-a]carbazoles 3a-d and benzo-[1,2-b]-1,4-thiazepino[2,3-a] carbazoles 4a-d respectively. Further carbazoles 2a-d are treated with phenyl acetic acid in an attempt to synthesize 3-phenyl-2-oxopyrano[2,3-a]-carbazoles 5a-d, but the reaction did not proceed in the anticipated direction and only acetyl derivatives 6a-d are obtained. All the products thus obtained from these reactions are well characterized by spectroscopic and analytical data.},
	number = {8},
	journal = {Indian Journal of Chemistry - Section B Organic and Medicinal Chemistry},
	author = {Sangeetha, V. and Prasad, K. J.Rajendra},
	year = {2006},
	keywords = {1-hydroxycarbazole-2-carbaldehydes, 2-acetylfuro carbazoles, Benzo carbazoles, Phenyl oxopyranocarbazoles, o-aminothiophenol},
	pages = {1951--1954},
}

@article{purschke_lorres_2020,
	title = {Lörres, {Möppes}, and the {Swiss}. ({Re}){Discovering} {Regional} {Patterns} in {Anonymous} {Social} {Media} {Data}},
	doi = {10.1017/jlg.2019.10},
	abstract = {1},
	number = {2019},
	journal = {Journal of Linguistic Geography},
	author = {Purschke, Christoph and Hovy, Dirk},
	year = {2020},
	keywords = {2019, accepted 8 july 2019, computational sociolinguistics, distributed representations, distributional semantics, german, language use, neural networks, received 8 november 2017, regional variation, representation learning, revise received 1 july, social media, social style, word embeddings},
	pages = {113--134},
}

@article{Nguyen2016,
	title = {Computational sociolinguistics: {A} survey},
	volume = {42},
	doi = {10.1016/j.jksus.2015.08.001},
	abstract = {Language is a social phenomenon and inherent to its social nature is that it is constantly changing. Recently, a surge of interest can be observed within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of "Computational Sociolinguistics" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions employed in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges.},
	number = {3},
	journal = {Computational Linguistics},
	author = {Nguyen, Dong and Doğruöz, A. Seza and Rosé, Carolyn P. and de Jong, Franciska},
	year = {2016},
	note = {arXiv: 1508.07544},
	keywords = {★},
	pages = {537--593},
}

@article{chandrasekharan_internets_2018,
	title = {The {Internet}’s hidden rules: {An} empirical study of {Reddit} norm violations at micro, meso, and macro scales},
	volume = {2},
	issn = {25730142},
	doi = {10.1145/3274301},
	abstract = {Norms are central to how online communities are governed. Yet, norms are also emergent, arise from interaction, and can vary significantly between communities—making them challenging to study at scale. In this paper, we study community norms on Reddit in a large-scale, empirical manner. Via 2.8M comments removed by moderators of 100 top subreddits over 10 months, we use both computational and qualitative methods to identify three types of norms: Macro norms that are universal to most parts of Reddit; meso norms that are shared across certain groups of subreddits; and micro norms that are specific to individual, relatively unique subreddits. Given the size of Reddit’s user base—and the wide range of topics covered by different subreddits—we argue this represents the first large-scale study of norms across disparate online communities. In other words, these findings shed light on what Reddit values, and how widely-held those values are. We conclude by discussing implications for the design of new and existing online communities.},
	number = {CSCW},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Chandrasekharan, Eshwar and Samory, Mattia and Jhaver, Shagun and Charvat, Hunter and Bruckman, Amy and Lampe, Cliff and Eisenstein, Jacob and Gilbert, Eric},
	year = {2018},
	keywords = {Community norms, Mixed methods, Moderation, online communities},
}

@article{dua_drop_2019,
	title = {{DROP}: {A} {Reading} {Comprehension} {Benchmark} {Requiring} {Discrete} {Reasoning} {Over} {Paragraphs}},
	url = {http://arxiv.org/abs/1903.00161},
	abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7\% F1 on our generalized accuracy metric, while expert human performance is 96.0\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0\% F1.},
	author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	year = {2019},
	note = {arXiv: 1903.00161},
	pages = {2368--2378},
}

@article{rajpurkar_know_2018,
	title = {Know what you don’t know: {Unanswerable} questions for {SQuAD}},
	volume = {2},
	doi = {10.18653/v1/p18-2124},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQUADRUN, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQUADRUN, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQUADRUN is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQUADRUN. We release SQUADRUN to the community as the successor to SQuAD.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	year = {2018},
	note = {arXiv: 1806.03822
ISBN: 9781948087346},
	pages = {784--789},
}

@article{pecina_lexical_2010,
	title = {Lexical association measures and collocation extraction},
	volume = {44},
	issn = {1574020X},
	doi = {10.1007/s10579-009-9101-4},
	abstract = {We present an extensive empirical evaluation of collocation extraction methods based on lexical association measures and their combination. The experiments are performed on three sets of collocation candidates extracted from the Prague Dependency Treebank with manual morphosyntactic annotation and from the Czech National Corpus with automatically assigned lemmas and part-of-speech tags. The collocation candidates were manually labeled as collocational or non-collocational. The evaluation is based on measuring the quality of ranking the candidates according to their chance to form collocations. Performance of the methods is compared by precision-recall curves and mean average precision scores. The work is focused on two-word (bigram) collocations only. We experiment with bigrams extracted from sentence dependency structure as well as from surface word order. Further, we study the effect of corpus size on the performance of the individual methods and their combination. © Springer Science+Business Media B.V. 2009.},
	number = {1-2},
	journal = {Language Resources and Evaluation},
	author = {Pecina, Pavel},
	year = {2010},
	keywords = {Collocations, Evaluation, Lexical association measures, Multiword expressions},
	pages = {137--158},
}

@article{geva_are_2019,
	title = {Are {We} {Modeling} the {Task} or the {Annotator}? {An} {Investigation} of {Annotator} {Bias} in {Natural} {Language} {Understanding} {Datasets}},
	doi = {10.18653/v1/d19-1107},
	abstract = {Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.},
	author = {Geva, Mor and Goldberg, Yoav and Berant, Jonathan},
	year = {2019},
	note = {arXiv: 1908.07898},
	pages = {1161--1166},
}

@article{daume_frustratingly_2009,
	title = {Frustratingly {Easy} {Domain} {Adaptation}},
	abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
	author = {Daumé, Hal},
	year = {2009},
	note = {arXiv: 0907.1815},
}

@article{inoue_listening_2003,
	title = {The listening subject of {Japanese} modernity and his auditory double: {Citing}, sighting, and siting the modern {Japanese} woman},
	volume = {18},
	issn = {08867356},
	doi = {10.1525/can.2003.18.2.156},
	number = {2},
	journal = {Cultural Anthropology},
	author = {Inoue, Miyako},
	year = {2003},
	pages = {156--193},
}

@article{wu_gendered_2018,
	title = {Gendered {Language} on the {Economics} {Job} {Market} {Rumors} {Forum}},
	volume = {108},
	issn = {2574-0768},
	doi = {10.1257/pandp.20181101},
	abstract = {This paper examines the existence of an unwelcoming or stereotypical culture using evidence on how women and men are portrayed in anonymous discussions on the Economics Job Market Rumors forum (EJMR). I use a Lasso-Logistic model to measure gendered language in EJMR postings, identifying the words that are most strongly associated with discussions about one gender or the other. I find that the words most predictive of a post about a woman are typically about physical appearance or personal information, whereas those most predictive of a post about a man tend to focus on academic or professional characteristics.},
	journal = {American Economic Association Papers and Proceedings},
	author = {Wu, Alice H.},
	year = {2018},
	pages = {175--79},
}

@article{may_measuring_2019,
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	year = {2019},
	note = {arXiv: 1903.10561v1},
	pages = {1--7},
}

@inproceedings{bolukbasi2016man,
	title = {Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
	booktitle = {30th {Conference} on {Neural} {Information} {Processing} {Systems} ({NIPS} 2016)},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year = {2016},
	pages = {4349--4357},
}

@article{nakassis_youth_2013,
	title = {Youth masculinity, 'style' and the peer group in {Tamil} {Nadu}, {India}},
	volume = {47},
	issn = {09730648},
	doi = {10.1177/0069966713482982},
	abstract = {This article examines young men's concepts of status in urban Tamil Nadu, India, focussing in particular on their concept of 'style'. The article shows how young men experience their position in the life cycle as between childhood and adulthood, and how this liminality mediates their concepts of status. In particular, I focus on the construction of the youth peer group as in distinction to, and transgressive of, the forms of adult respectability, propriety and authority from which young men are excluded by virtue of their age. I show how the peer group is marked by a productive tension between transgression and self-differentiation, and reciprocity, intimacy and peer pressure. The article then turns to two kinds of source material for young men's performances of status: English-Tamil slang and counterfeit global brands. I show how the tension between, and negotiation of, the mandates to status-raise and status-level in the peer group transform and revalourise these signs of status. The article concludes by arguing that while from afar, such youth practice seems to be negotiating globalisation, modernity and tradition, a close analysis of peer-group dynamics shows that youth practice is more centrally concerned with peer-group status negotiations. © SAGE Publications 2013.},
	number = {2},
	journal = {Contributions to Indian Sociology},
	author = {Nakassis, Constantine V.},
	year = {2013},
	keywords = {Tamil Nadu, brands, globalisation, slang, status, youth},
	pages = {245--269},
}

@article{kiritchenko_examining_2018,
	title = {Examining {Gender} and {Race} {Bias} in {Two} {Hundred} {Sentiment} {Analysis} {Systems}},
	doi = {10.18653/v1/s18-2005},
	abstract = {Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 'Affect in Tweets'. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.},
	author = {Kiritchenko, Svetlana and Mohammad, Saif},
	year = {2018},
	note = {arXiv: 1805.04508},
	pages = {43--53},
}

@incollection{irvine_ingredients_nodate,
	title = {Ingredients : {Signs} , {Conjectures} , {Perspectives}},
	author = {Irvine, Judith T and Gal, Susan},
}

@article{mostafazadeh_corpus_2016,
	title = {A corpus and cloze evaluation for deeper understanding of commonsense stories},
	doi = {10.18653/v1/n16-1098},
	abstract = {Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.},
	journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
	author = {Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
	year = {2016},
	note = {arXiv: 1604.01696
ISBN: 9781941643914},
	pages = {839--849},
}

@article{Zhong2017,
	title = {Wearing many (social) hats: {How} different are your different social network personae?},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15694/14828},
	abstract = {This paper investigates when users create profiles in different social networks, whether they are redundant expressions of the same persona, or they are adapted to each platform. Using the personal webpages of 116,998 users on About.me, we identify and extract matched user profiles on several major social networks including Facebook, Twitter, LinkedIn, and Instagram. We find evidence for distinct site-specific norms, such as differences in the language used in the text of the profile self-description, and the kind of picture used as profile image. By learning a model that robustly identifies the platform given a user's profile image (0.657--0.829 AUC) or self-description (0.608--0.847 AUC), we confirm that users do adapt their behaviour to individual platforms in an identifiable and learnable manner. However, different genders and age groups adapt their behaviour differently from each other, and these differences are, in general, consistent across different platforms. We show that differences in social profile construction correspond to differences in how formal or informal the platform is.},
	number = {Icwsm},
	journal = {Proceedings of the 11th International Conference on Web and Social Media, ICWSM 2017},
	author = {Zhong, Changtao and Chang, Hau Wen and Karamshuk, Dmytro and Lee, Dongwon and Sastry, Nishanth},
	year = {2017},
	note = {ISBN: 9781577357889},
	pages = {397--406},
}

@inproceedings{Schler2006,
	title = {Effects of {Age} and {Gender} on {Blogging}},
	volume = {86},
	isbn = {1-57735-264-5},
	doi = {10.1155/2015/862427},
	abstract = {Analysis of a corpus of tens of thousands of blogs incorporating close to 300 million words indicates significant differences in writing style and content between male and female bloggers as well as among authors of different ages. Such differences can be exploited to determine an unknown authors age and gender on the basis of a blogs vocabulary.},
	booktitle = {{AAAI} spring symposium: {Computational} approaches to analyzing weblogs},
	author = {Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James},
	year = {2006},
	pmid = {26090266},
	note = {ISSN: 0196-0202},
	keywords = {information retrieval \& textual information access, natural language processing},
	pages = {199--205},
}

@article{chevrot_editors_2018,
	title = {Editors’ {Introduction} and {Review}: {Sociolinguistic} {Variation} and {Cognitive} {Science}},
	volume = {10},
	issn = {17568765},
	doi = {10.1111/tops.12384},
	abstract = {Sociolinguists study the interaction between language and society. Variationist sociolinguistics — the subfield of sociolinguistics which is the focus of this issue — uses empirical and quantitative methods to study the production and perception of linguistic variation. Linguistic variation refers to how speakers choose between linguistic forms that say the same thing in different ways, with the variants differing in their social meaning. For example, how frequently someone says fishin’ or fishing depends on a number of factors, such as the speaker's regional and social background and the formality of the speech event. Likewise, if listeners are asked to use a rating scale make judgements about speakers who say fishin’ or fishing, their ratings depend on what other social characteristics are attributed to the speaker. This issue aims to reflect the growing number of interactions that bring variationist sociolinguistics into contact of different branches of cognitive science. After presenting current trends in sociolinguistics, we identify five areas of contact between the two fields: cognitive sociolinguistics, sociolinguistic cognition, acquisition of variation, computational modeling, and a comparative approach of variation in animal communication. We then explain the benefits of interdisciplinary work: fostering the study of variability and cultural diversity in cognition; bringing together data and modeling; understanding the cognitive mechanisms through which sociolinguistic variation is processed; examining indexical meaning; exploring links between different levels of grammar; and improving methods of data collection and analysis. Finally we explain how the articles in this issue contribute to each of these benefits. We conclude by suggesting that sociolinguistics holds a strategic position for facing the challenge of building theories of language through integrating its linguistic, cognitive, and social aspects at the collective and individual levels.},
	number = {4},
	journal = {Topics in Cognitive Science},
	author = {Chevrot, Jean Pierre and Drager, Katie and Foulkes, Paul},
	year = {2018},
	keywords = {Cognition, Language usage, Modeling, Sociolinguistics, Sociophonetics, Variation},
	pages = {679--695},
}

@inproceedings{schluter_glass_2018,
	title = {The glass ceiling in {NLP}},
	volume = {22},
	doi = {10.4018/978-1-59140-815-4.ch114},
	abstract = {In this paper, we provide empirical evidence based on a rigourously studied mathematical model for bi-populated networks , that a glass ceiling within the field of NLP has developed since the mid 2000s.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Schluter, Natalie},
	year = {2018},
	note = {Issue: 3
ISSN: 01635700},
	pages = {2793--2798},
}

@article{woodruff_qualitative_2018,
	title = {A qualitative exploration of perceptions of algorithmic fairness},
	volume = {2018-April},
	doi = {10.1145/3173574.3174230},
	abstract = {Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Woodruff, Allison and Fox, Sarah E. and Rousso-Schindler, Steven and Warshaw, Jeff},
	year = {2018},
	note = {ISBN: 9781450356206},
	keywords = {Algorithmic discrimination, Algorithmic fairness},
	pages = {1--14},
}

@incollection{sedgwick_touching_nodate,
	title = {Touching feeling},
	isbn = {978-85-7811-079-6},
	author = {Sedgwick, E.K.},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
ISSN: 1098-6596},
	keywords = {icle},
}

@article{von_hippel_linguistic_1997,
	title = {The {Linguistic} {Intergroup} {Bias} as an implicit indicator of prejudice},
	volume = {33},
	issn = {00221031},
	doi = {10.1006/jesp.1997.1332},
	abstract = {The Linguistic Intergroup Bias (LIB) is the tendency to describe stereotypic events in more abstract terms than counterstereotypic events. We examined whether a paper-and-pencil measure based on the LIB could be used as an implicit indicator of prejudice. Experiment 1 demonstrated that a measure of implicit racial prejudice based on the LIB predicted whether subjects evaluated an African-American or a Caucasian as threatening. Experiment 2 extended this finding with indirect measures of threat. In both of these experiments, an explicit measure of prejudice failed to predict subjects' evaluations. Experiment 3 demonstrated that a measure of implicit gender prejudice based on the LIB was correlated with an implicit prejudice measure based on biased attributional processing. Across these three experiments, implicit and explicit measures of prejudice were largely uncorrelated. © 1997 Academic Press.},
	number = {5},
	journal = {Journal of Experimental Social Psychology},
	author = {Von Hippel, William and Sekaquaptewa, Denise and Vargas, Patrick},
	year = {1997},
	pages = {490--509},
}

@article{peirce_questions_1868,
	title = {Questions {Concerning} {Certain} {Faculties} {Claimed} for},
	volume = {2},
	abstract = {Question 1. Whether by the simple con templation of a cognition, independently of any previous knowledge and without reason ing from signs, we are enabled rightly to judge whether that cognition has been de termined by a previous cognition or whether it refers immediately to its object. Throughout this paper, the term intui tion will be taken as a signifying cognition not determined by a previous cognition of the same object, and therefore so determ ined by something out of the conscious ness.* Let me request the reader to note this. Intuition here will be nearly the same as not itself a iepremise conclusion;" the only difference being that premises and conclusions are judgments, whereas an in tuition may, as far as its definition states, be any kind of cognition whatever. But as a conclusion just (good or bad) is de termined in the mind of the reasoner by its so premise, cognitions not judgments may be determined by previous cognitions ; and a cognition not so determined, and therefore determined directly by the trans * The word intuitus first occurs as a techni cal term in St. Anselm's Monoiogium. He wished to distinguish between our knowledge of God and our knowledge of finite things ("and, in the next world, of God, also); and thinking of the saying of St. Paul, Videmus nunc per speculum in cenigmate: tune autem facie ad faciem, he called the former speculation and " " the latter intuition. This use of speculation did not take root, because that word already had another exact and widely different mean In the middle the term " ing. ages, intuitive cognition" had two principal senses, 1st, as opposed to abstractive cognition, it meant the knowledge of the present as present, and this is its meaning in Anselm ; but 2d, as no intui tive cognition was allowed to be determined by a previous cognition, it came to be used as the opposite of discursive cognition (see Sco tus, In sentent. lib. \% dist. 3, qu. 9), and this is nearly the sense in which I employ it. This is also nearly the sense in which Kant ?ses it, the former distinction being expressed by his sensuous and non-sensuous. (See Werke, herausg. Rosenkrantz, Thl. 2, S. 713, 31, 41, 100, u. 8. w.) An enumeration of six mean ings of intuition may be found in Hamilton's Reid, p. 759. cendental is to be termed an intui object, tion. Now, it is plainly one thing to have an intuition and another to know intuitively that it is an intuition, and the question is whether these two things, distinguishable in con thought, are, in fact, invariably nected, so that we can always intuitively distinguish between an intuition and a cog nition determined by another. Every cog nition, as something present, is, of course, an intuition of itself. But the determ ination of a cognition by another cogni tion or by a transcendental object is not, at least so far as appears obviously at first, a part of the immediate content of that cognition, although it would appear to be an element of the action or passion of the transcendental ego, which is not, perhaps, in consciousness immediately; and yet this transcendental action or pas sion may invariably determine a cognition of itself, so that, in fact, the determina tion or non-determination of the cognition by another may be a part of the cognition. In this case, I should say that we had an intuitive},
	journal = {Journal of Speculative Philosophy},
	author = {Peirce, Charles Saunders},
	year = {1868},
	pages = {103--114},
}

@article{Clark2016,
	title = {Deep reinforcement learning for mention-ranking coreference models},
	doi = {10.18653/v1/d16-1245},
	abstract = {Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in a significant improvement over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.},
	journal = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Clark, Kevin and Manning, Christopher D.},
	year = {2016},
	note = {arXiv: 1609.08667
ISBN: 9781945626258},
	pages = {2256--2262},
}

@article{marquez_coreference_2013,
	title = {Coreference resolution: {An} empirical study based on {SemEval}-2010 shared {Task} 1},
	volume = {47},
	issn = {1574020X},
	doi = {10.1007/s10579-012-9194-z},
	abstract = {This paper presents an empirical evaluation of coreference resolution that covers several interrelated dimensions. The main goal is to complete the comparative analysis from the SemEval-2010 task on Coreference Resolution in Multiple Languages. To do so, the study restricts the number of languages and systems involved, but extends and deepens the analysis of the system outputs, including a more qualitative discussion. The paper compares three automatic coreference resolution systems for three languages (English, Catalan and Spanish) in four evaluation settings, and using four evaluation measures. Given that our main goal is not to provide a comparison between resolution algorithms, these are merely used as tools to shed light on the different conditions under which coreference resolution is evaluated. Although the dimensions are strongly interdependent, making it very difficult to extract general principles, the study reveals a series of interesting issues in relation to coreference resolution: the portability of systems across languages, the influence of the type and quality of input annotations, and the behavior of the scoring measures. © Springer Science+Business Media B.V. 2012.},
	number = {3},
	journal = {Language Resources and Evaluation},
	author = {Màrquez, Lluís and Recasens, Marta and Sapena, Emili},
	year = {2013},
	keywords = {Coreference resolution and evaluation, Discourse entities, Machine learning based NLP tools, NLP system analysis, SemEval-2010 (Task 1)},
	pages = {661--694},
}

@article{Moosavi2016,
	title = {Which coreference evaluation metric do you trust? {A} proposal for a link-based entity aware metric},
	volume = {2},
	doi = {10.18653/v1/p16-1060},
	abstract = {Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.},
	journal = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers},
	author = {Moosavi, Nafise Sadat and Strube, Michael},
	year = {2016},
	note = {ISBN: 9781510827585},
	pages = {632--642},
}

@inproceedings{manning-EtAl:2014:P14-5,
	title = {The {Stanford} {CoreNLP} {Natural} {Language} {Processing} {Toolkit}},
	url = {http://www.aclweb.org/anthology/P/P14/P14-5010},
	booktitle = {Association for {Computational} {Linguistics} ({ACL}) {System} {Demonstrations}},
	author = {Manning, Christopher D and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J and McClosky, David},
	year = {2014},
	pages = {55--60},
}

@article{Lee2014,
	title = {Low-dimensional embeddings for interpretable anchor-based topic inference},
	doi = {10.3115/v1/d14-1138},
	abstract = {The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words.},
	journal = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
	author = {Lee, Moontae and Mimno, David},
	year = {2014},
	note = {arXiv: 1711.06826
ISBN: 9781937284961},
	pages = {1319--1328},
}

@article{Roberts2019,
	title = {stm: {An} {R} package for structural topic models},
	volume = {91},
	issn = {15487660},
	doi = {10.18637/jss.v091.i02},
	abstract = {This paper demonstrates how to use the R package stm for structural topic modeling. The structural topic model allows researchers to flexibly estimate a topic model that includes document-level metadata. Estimation is accomplished through a fast variational approximation. The stm package provides many useful features, including rich ways to explore topics, estimate uncertainty, and visualize quantities of interest.},
	number = {2},
	journal = {Journal of Statistical Software},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin},
	year = {2019},
	keywords = {LDA, R, Stm, Structural topic model, Text analysis},
}

@article{Blondel2008,
	title = {Fast unfolding of communities in large networks},
	volume = {2008},
	issn = {17425468},
	doi = {10.1088/1742-5468/2008/10/P10008},
	abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .},
	number = {10},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Blondel, Vincent D. and Guillaume, Jean Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	year = {2008},
	keywords = {Networks, New applications of statistical mechanics, Random graphs},
}

@article{kulick_no_2003,
	title = {No},
	volume = {23},
	author = {Kulick, Don},
	year = {2003},
	keywords = {calls a performative approach, developed what she, homosexual panic defense, is most closely associated, language and sexuality, of well-known books has, performativity, performativity as a theory, pher judith butler, rape, sadomasochism, the cornerstone of this, to language and culture, who in a number, with the american philoso-},
	pages = {139--151},
}

@article{zeldes_different_2016,
	title = {Different {Flavors} of {GUM}: {Evaluating} {Genre} and {Sentence} {Type} {Effects} on {Multilayer} {Corpus} {Annotation} {Quality}},
	doi = {10.18653/v1/w16-1709},
	author = {Zeldes, Amir and Simonson, Dan},
	year = {2016},
	pages = {68--78},
}

@article{jurgens_incorporating_2017,
	title = {Incorporating dialectal variability for socially equitable language identification},
	volume = {2},
	doi = {10.18653/v1/P17-2009},
	abstract = {Language identification (LID) is a critical first step for processing multilingual text. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like Twitter, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our model achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of “socially inclusive” NLP tools.},
	journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Jurgens, David and Tsvetkov, Yulia and Jurafsky, Dan},
	year = {2017},
	note = {ISBN: 9781945626760},
	pages = {51--57},
}

@techreport{marcus_building_1993,
	title = {Building a {Large} {Annotated} {Corpus} of {English}: {The} {Penn} {Treebank}},
	abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
	author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
	year = {1993},
	note = {Volume: 19
Issue: 2
ISSN: 0891-2017},
	keywords = {Analyse automatique, Anglais, Automatic analysis, Computational linguistics, Construction, Corpus, Discours, Discourse, Linguistique informatique, Natural language processing, Partie du discours, Penn Treebank, Structure syntaxique, Syntax, Syntaxe, Traitement automatique des langues naturelles},
	pages = {313},
}

@misc{noauthor_gal_nodate,
	title = {Gal and {Irvine} (2019) {Signs} of difference intro.pdf},
}

@article{Yang2014,
	title = {Towards an integration of text and graph clustering methods as a lens for studying social interaction in {MOOCs}},
	volume = {15},
	issn = {14923831},
	abstract = {{\textless}p{\textgreater}In this paper, we describe a novel methodology, grounded in techniques from the field of machine learning, for modeling emerging social structure as it develops in threaded discussion forums, with an eye towards application in the threaded discussions of massive open online courses (MOOCs). This modeling approach integrates two simpler, well established prior techniques, namely one related to social network structure and another related to thematic structure of text. As an illustrative application of the integrated technique’s use and utility, we use it as a lens for exploring student dropout behavior in three different MOOCs. In particular, we use the model to identify twenty emerging subcommunities within the threaded discussions of each of the three MOOCs. We then use a survival model to measure the impact of participation in identified subcommunities on attrition along the way for students who have participated in the course discussion forums of the three courses. In each of three MOOCs we find evidence that participation in two to four subcommunities out of the twenty is associated with significantly higher or lower dropout rates than average. A qualitative post-hoc analysis illustrates how the learned models can be used as a lens for understanding the values and focus of discussions within the subcommunities, and in the illustrative example to think about the association between those and detected higher or lower dropout rates than average in the three courses. Our qualitative analysis demonstrates that the patterns that emerge make sense: It associates evidence of stronger expressed motivation to actively participate in the course as well as evidence of stronger cognitive engagement with the material in subcommunities associated with lower attrition, and the opposite in subcommunities associated with higher attrition. We conclude with a discussion of ways the modeling approach might be applied, along with caveats from limitations, and directions for future work.{\textless}/p{\textgreater}},
	number = {5},
	journal = {International Review of Research in Open and Distance Learning},
	author = {Yang, Diyi and Wen, Miaomiao and Kumar, Abhimanu and Xing, Eric P. and Rosé, Carolyn Penstein},
	year = {2014},
	pages = {215--234},
}

@article{Roder2015,
	title = {Exploring the space of topic coherence measures},
	doi = {10.1145/2684822.2685324},
	abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
	journal = {WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining},
	author = {Röder, Michael and Both, Andreas and Hinneburg, Alexander},
	year = {2015},
	note = {ISBN: 9781450333177},
	keywords = {Topic coherence, Topic evaluation, Topic model},
	pages = {399--408},
}

@inproceedings{Fast2016,
	title = {Shirtless and dangerous: {Quantifying} linguistic signals of gender bias in an online fiction writing community},
	isbn = {978-1-57735-758-2},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM16/paper/download/13112/12731},
	abstract = {Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors' use of gender stereotypes is associated with the community's ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Web} and {Social} {Media} ({ICWSM})},
	author = {Fast, Ethan and Vachovsky, Tina and Bernstein, Michael S},
	year = {2016},
	note = {arXiv: 1603.08832},
	keywords = {Full Papers},
	pages = {112--120},
}

@article{Walton2018,
	title = {The leaky canon: {Constructing} and policing heteronormativity in the {Harry} {Potter} fandom},
	volume = {15},
	abstract = {Though scholars have turned to the Harry Potter universe as a prime example of an explosive, international fan community and as a site of canon-building, few have drawn attention to the particular websites and individual web identities that were responsible for constructing and policing the HP fanon. This article looks closely at the websites MuggleNet and the Leaky Cauldron as they operated during the early millennium and argues that these spaces and those respected individuals who maintained them established the foundation for a gender normative and often conservative ‘mainstream’ Potter canon. Of interest is how these websites gained authority and power within the fandom and fan communities writ large, a significant feat considering they were centered in a children’s fictional world and, more notably, were sometimes created and managed by children (and at the very least frequented by them). These websites’ role in the HP community complicates understandings of children’s agency, identity construction, and power in large part because of the way gender and sexuality are presented in these sites’ fan-generated material. In short, MuggleNet and the Leaky Cauldron’s understanding of the Potter ‘canon’ as being heteronormative – as is most evident in their actions and reactions in the shipping wars –does more than simply police interpretations of Rowling’s books: it also reveals how gatekeeping practices can slip into toxicity, transforming a fandom community into one that is arguably hostile to multiple interpretations or, more worryingly, traditionally marginalized fans.},
	number = {1},
	journal = {Participations: Journal of Audience \& Reception Studies},
	author = {Walton, Sarah Schaefer},
	year = {2018},
	keywords = {and as i consider, as a child, beige ibm in our, canon, childhood, children, clunky, fan community, fandom, fanon, harry potter, heteronormativity, i primarily used the, it is only in, mugglenet, or portkey, playroom as a gateway, retrospect, s agency, the potter, to the harry potter, toxicity},
	pages = {21},
}

@article{Bamman2013,
	title = {Learning {Latent} {Personas} of {Film} {Characters}},
	url = {http://www.aclweb.org/anthology/P13-1035},
	abstract = {We present two latent variable models for learning character types, or personas, in film, in which a persona is defined as a set of mixtures over latent lexical classes. These lexical classes capture the stereotypical actions of which a character is the agent and patient, as well as attributes by which they are described. As the first attempt to solve this problem explicitly, we also present a new dataset for the text-driven analysis of film, along with a benchmark testbed to help drive future work in this area.},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)},
	author = {Bamman, David and O'Connor, Brendan and Smith, Noah A},
	year = {2013},
	note = {ISBN: 9781937284503},
	pages = {352--361},
}

@incollection{sedgwick_introduction:_nodate,
	title = {Introduction: {Axiomatic}},
	isbn = {0-520-074042-9},
	booktitle = {The {Epistemology} of the {Closet}},
	author = {Sedgwick, E.K.},
	pages = {1--66},
}

@article{gelman_philosophy_2013,
	title = {Philosophy and the practice of {Bayesian} statistics},
	volume = {66},
	issn = {00071102},
	doi = {10.1111/j.2044-8317.2011.02037.x},
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework. © 2012 The British Psychological Society.},
	number = {1},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	year = {2013},
	note = {arXiv: 1006.3868},
	pages = {8--38},
}

@article{chakraborty_tabloids_2017,
	title = {Tabloids in the {Era} of {Social} {Media}?},
	volume = {1},
	doi = {10.1145/3134665},
	number = {CSCW},
	journal = {CSCW},
	author = {Chakraborty, Abhijnan and Sarkar, Rajdeep and Mrigen, Ayushi and Ganguly, Niloy},
	year = {2017},
	keywords = {-  Human-centered computing  -{\textgreater}  Social content sh, Empirical studies in collaborative and social comp, Social media},
	pages = {1--21},
}

@article{fan_plain_2019,
	title = {In {Plain} {Sight}: {Media} {Bias} {Through} the {Lens} of {Factual} {Reporting}},
	url = {http://arxiv.org/abs/1909.02670},
	abstract = {The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions.},
	author = {Fan, Lisa and White, Marshall and Sharma, Eva and Su, Ruisi and Choubey, Prafulla Kumar and Huang, Ruihong and Wang, Lu},
	year = {2019},
	note = {arXiv: 1909.02670},
}

@incollection{foucault_domain_nodate,
	title = {Domain},
	booktitle = {The {History} of {Sexuality}},
	author = {Foucault, Michel},
}

@incollection{foucault_perverse_nodate,
	title = {The {Perverse} {Implantation}},
	booktitle = {The {History} of {Sexuality}},
	author = {Foucault, Michel},
	pages = {36--49},
}

@article{halperin_one_2001,
	title = {One hundred years of homosexuality},
	volume = {33},
	issn = {04866134},
	doi = {10.1177/048661340103300105},
	abstract = {Marxian socialism has bequeathed contradictory ideas about the course of capitalist development. There is “the proletariat,” the anticipation that workers were being simplified into a uniform mass, living at the bottom of society, and “having no country of their own.” On the other hand, there is an historical tendency for the relations and even the forces of production to become more deeply social in character, ultimately to burst the confining integument of private property and capitalism itself. Modem socialism continues to be influenced by echoes of that proletariat, but has tended to ignore opportunities provided by the increasingly social character of our mode of production, assuming that those are tasks to be taken up on some distant tomorrow. If the decline of socialism is to be radically reversed, the proletarian echo will have to be muted even further, and the increasing socialization of the mode of production taken up as an existing threat and a striking opportunity. The expectation of a revolutionary proletariat neither can nor does function as the underpinning for socialist strategies of change. Of course, that is not the same thing as socialist sensitivity to the very poor and to inequality generally, but the two are often confused. If there were such a thing as proletarianization, then a realistic and ultimately dominant political movement could be based upon it, but contemporary socialist sympathy for the poor and the denied has much more morality than strategy to it, and to that extent does not supply a sufficiently acute political perspective. I am not saying, “Devil take the poor,” nor denying their permanence while capitalism still exists. But an anticapitalist transformation of society now requires a different socialist fulcrum. © 2001, Sage Publications. All rights reserved.},
	number = {1},
	journal = {Review of Radical Political Economics},
	author = {{Halperin}},
	year = {2001},
	pages = {99--115},
}

@article{jagose_homosexual_1992,
	title = {Homosexual, lesbian or gay, queer},
	number = {1996},
	author = {Jagose, Annamarie},
	year = {1992},
	pages = {72--100},
}

@article{ebara_copper/zinc_2003,
	title = {The copper/zinc ratio in patients with hepatocellular carcinoma},
	volume = {38},
	issn = {09441174},
	doi = {10.1007/s005350300016},
	number = {1},
	journal = {Journal of Gastroenterology},
	author = {Ebara, Masaaki and Fukuda, Hiroyuki and Saisho, Hiromitsu},
	year = {2003},
	pages = {104--105},
}

@book{butler2002gender,
	title = {Gender trouble},
	publisher = {routledge},
	author = {Butler, Judith},
	year = {2002},
}

@article{is_l_nodate,
	title = {L anguage},
	number = {2018},
	author = {Is, Anguage and Described, Often and Rubicon, A S A},
}

@article{both_c_2019,
	title = {C u lt u r a l l e a r n i n g},
	number = {2018},
	author = {Both, O G Y I S},
	year = {2019},
}

@article{chapter_many_2019,
	title = {{MANY} {ANSWERS}},
	number = {2018},
	author = {Chapter, N This and Outline, I First and The, H O W},
	year = {2019},
	pages = {7--23},
}

@incollection{coupland_foucault_2016,
	title = {Foucault, {Gumperz} and governmentality: {Interaction}, power and subjectivity in the twenty-first century},
	booktitle = {Sociolinguistics: {Theoretical} {Debates}},
	editor = {Coupland, Nikolas},
	year = {2016},
}

@book{busemeyer_cognitive_2010,
	title = {Cognitive modeling},
	isbn = {978-0-203-49445-5},
	abstract = {An important goal of cognitive science is to understand human cognition. Good models of cognition can be predictive—describing how people are likely to react in different scenarios—as well as prescriptive— describing limitations in cognition and potentially ways in which the limitations might be overcome. In},
	author = {Busemeyer, Jerome R. and Diederich, Adele},
	year = {2010},
	note = {Publication Title: Computer Science Handbook, Second Edition},
}

@article{ballantyne_writing_2015,
	title = {Writing an article},
	volume = {29},
	issn = {20479018},
	doi = {10.7748/ns.29.35.49.s44},
	number = {35},
	journal = {Nursing standard (Royal College of Nursing (Great Britain) : 1987)},
	author = {Ballantyne, Helen},
	year = {2015},
	pages = {49},
}

@article{Milner2013,
	title = {Pop {Polyvocality}: {Internet} {Memes}, {Public} {Participation}, and the {Occupy} {Wall} {Street} {Movement}},
	volume = {7},
	issn = {1932-8036},
	abstract = {From the inception of Occupy Wall Street (OWS), participatory media played a key role in the movement. Members of the public engaged OWS on sites like Tumblr and reddit. Central to the discussion were Internet memes. Memes are multimodal artifacts remixed by countless participants, employing popular culture for public commentary. Analyzing the use of memes in political discourse can illuminate the nature of mediated commentary on public events. This article examines how memes were used to articulate perspectives on OWS. A corpus of memes commenting on OWS from multiple participatory media networks was analyzed using multimodal critical discourse analysis. Findings indicated memes facilitated conversation between diverse positions. OWS memes employed populist argument and popular texts, intertwining them into a vibrant polyvocal public discourse.},
	journal = {International Journal of Communication},
	author = {Milner, Ryan M},
	year = {2013},
	keywords = {conversation, critical discourse analysis, internet meme, participatory media, politics, popular culture, public participation, public relations, social science, sociology},
	pages = {2357--2390},
}

@article{Oakley2016,
	title = {Disturbing {Hegemonic} {Discourse}: {Nonbinary} {Gender} and {Sexual} {Orientation} {Labeling} on {Tumblr}},
	volume = {2},
	issn = {2056-3051},
	url = {http://journals.sagepub.com/doi/10.1177/2056305116664217},
	doi = {10.1177/2056305116664217},
	abstract = {In this article, I examine lesbian, gay, bisexual, trans, queer/questioning, intersex, and asexual (LGBTQIA) Tumblr bloggers’ bio boxes and “About Me” pages to show the ways gender and sexual orientation identities are constructed through community-regulated and community-generated labeling practices. Tumblr encourages counter-cultures (and labeling practices) to not only form but also to thrive due to its distinctive affordances including tagging and blog formatting. This article examines not only how these affordances shape usage and, subsequently, identity construction on Tumblr but also the ways in which Tumblr bloggers have embraced affordances to create community-accepted conventions of identity construction. Additionally, building upon online identity scholarship by Bargh, McKenna, and Fitzsimons and Tiidenberg, this article discusses true self and nonbinary gender and sexual orientation labeling as forms of identity construction that allows LGBTQIA identifying individuals a method for nuanced descriptions of feelings and desires. However, far from perfect, these labeling practices are also grounded in hegemonic female/male, feminine/masculine binary discourse. In a Foucauldian sense, bloggers construct discourse within existing power structures that ignore or erase LGBTQIA as sexual “abnormalities.” Although it is nearly impossible to fully break away from the dominant discourse, these labeling practices can be a useful starting point for conversations about genders and sexualities that lie outside of the hegemonic binary.},
	number = {3},
	journal = {Social Media + Society},
	author = {Oakley, Abigail},
	year = {2016},
	keywords = {affordances, are, determines what kind of, from gender in the, gender, identity, labeling, sense that, sexuality, sexuality does not follow, what gender you},
	pages = {1--12},
}

@article{Zeglin2014,
	title = {Using {Social} {Media} to {Assess} {Conceptualizations} of {Sexuality}},
	volume = {9},
	issn = {15546136},
	doi = {10.1080/15546128.2014.933994},
	abstract = {There is currently no validated model explaining the variability of sexual expression. This has created a scenario where sexuality, as a construct, is purely intuitive. Sexuality educators have frequently presented the Circles of Sexuality, a model that contends that sexuality is a combination of intimacy, sensuality, sexual health/behaviors, sexual identity, and sexualization. Adapting photovoice methodology and using the social media site Tumblr, the current analysis used this model to assess the social conceptualization of sexuality. Results indicate that the circles are unequally represented, with intimacy underrepresented and sexual identity overrepresented. Implications for sexuality educators, clinicians, and researchers are discussed. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
	number = {3},
	journal = {American Journal of Sexuality Education},
	author = {Zeglin, Robert J. and Mitchell, Julie},
	year = {2014},
	keywords = {Circles of Sexuality, Sexuality education, holistic sexuality, sexuality development, social media},
	pages = {276--291},
}

@inproceedings{Haimson2017,
	title = {Changes in {Social} {Media} {Affect}, {Disclosure}, and {Sociality} for a {Sample} of {Transgender} {Americans} in 2016's {Political} {Climate}.},
	isbn = {978-1-57735-788-9},
	abstract = {In the wake of 2016's divisive political climate in the US, media reports indicated that vulnerable people, such as the transgender population, may be experiencing lower than nor-mal rates of emotional wellbeing. To test these claims, we analyzed social media linguistic markers of affect, disclosure, and sociality in late 2016 as compared to the same month a year prior in a sample of US Tumblr blogs documenting peo-ple's gender transitions. We find that negative affect, and words related to anger in particular, increased for trans people in 2016. At the same time, social words used to describe fam-ily decreased, indicating that trans people may have inter-acted less with family and friends in late 2016. Self-disclo-sure decreased for trans women in 2016, potentially indicat-ing increased political language vs. personal content, or self-censorship in response to a hostile political environment. Re-sults highlight ways large-scale external political events may impact how people communicate and disclose on social me-dia. Additionally, our results indicate that social media data could be used to identify those most in need of mental well-being resources in response to a hostile political climate.},
	booktitle = {{ICWSM}},
	author = {Haimson, Oliver L. and Hayes, Gillian R.},
	year = {2017},
}

@article{Grbovic2015,
	title = {Gender and interest targeting for sponsored post advertising at tumblr},
	volume = {2015-Augus},
	doi = {10.1145/2783258.2788616},
	abstract = {As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and promote that content by sponsoring it to appear as an advertisement in the streams of Tumblr users. In this paper we present a framework that enabled one of the key targeted advertising components for Tumblr, specifically gender and interest targeting. We describe the main challenges involved in development of the framework, which include creating the ground truth for training gender prediction models, as well as mapping Tumblr content to an interest taxonomy. For purposes of inferring user interests we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6.8 billion user posts, with very limited amount of categorized keywords, and was shown to have superior performance over the bag-of-words model. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that cover more than 90\% of daily activities at Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20\% lift in user engagement with sponsored posts as compared to untargeted campaigns.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Grbovic, Mihajlo and Radosavljevic, Vladan and Djuric, Nemanja and Bhamidipati, Narayan and Nagarajan, Ananth},
	year = {2015},
	note = {arXiv: 1606.07189v1
ISBN: 9781450336642},
	keywords = {Audience modeling, Computational advertising, Data mining},
	pages = {1819--1828},
}

@inproceedings{Xie2017,
	title = {What's trending tomorrow, today: {Using} early adopters to discover popular posts on {Tumblr}},
	isbn = {978-1-5386-2714-3},
	doi = {10.1109/BigData.2017.8258165},
	abstract = {How well can we predict which posts on social media platforms will eventually become popular? What approaches should we use to accomplish this task? In an era where human attention has become a commodity, the early detection of trending posts in online social media platforms is a problem of special importance, with applications ranging from business marketing to political campaigns and beyond. In this work we provide new insight towards tackling this problem by performing in-depth experiments on discovering trending posts on the popular mixed-media microblogging platform Tumblr. Inspired by prior art, our approach involves examining the key traits of the early adopters of online posts and utilizes a variety of network and temporal features to assist in performing classification. We obtain the complete data coverage of Tumblr for two consecutive months in order to study the characteristics and emerging patterns of popular posts. Based on the data, we investigate the interplay between the observation window of early adopters, the performance of individual classifiers, and the importance of various features. We find consistent trends where the importance of network features increases as the observation window increases, while the importance of content and temporal features decreases. To the best of our knowledge, this work is the first attempt to carry out a large-scale study on the early detection of trending posts on Tumblr. © 2017 IEEE.},
	booktitle = {Proceedings - 2017 {IEEE} {International} {Conference} on {Big} {Data}, {Big} {Data} 2017},
	author = {Xie, Daniel and Xu, Jiejun and Lu, Tsai Ching},
	year = {2017},
	keywords = {Complex Network, Data Science, Online Social Media, Popularity Prediction, Tumblr},
	pages = {2168--2176},
}

@article{thompson_levy_2018,
	title = {Lévy {Flights} of the {Collective} {Imagination}},
	url = {http://arxiv.org/abs/1812.04013},
	abstract = {We present a structured random-walk model that captures key aspects of how people communicate in groups. Our model takes the form of a correlated L{\textbackslash}'\{e\}vy flight that quantifies the balance between focused discussion of an idea and long-distance leaps in semantic space. We apply our model to three cases of increasing structural complexity: philosophical texts by Aristotle, Hume, and Kant; four days of parliamentary debate during the French Revolution; and branching comment trees on the discussion website Reddit. In the philosophical and parliamentary cases, the model parameters that describe this balance converge under coarse-graining to limit regions that demonstrate the emergence of large-scale structure, a result which is robust to translation between languages. Meanwhile, we find that the political forum we consider on Reddit exhibits a debate-like pattern, while communities dedicated to the discussion of science and news show much less temporal order, and may make use of the emergent, tree-like topology of comment replies to structure their epistemic explorations. Our model allows us to quantify the ways in which social technologies such as parliamentary procedures and online commenting systems shape the joint exploration of ideas.},
	author = {Thompson, William H. W. and Wojtowicz, Zachary and DeDeo, Simon},
	year = {2018},
	note = {arXiv: 1812.04013},
	keywords = {culture analytics, evy flight, exploration-exploitation, information, l, random walk, rhetoric, search, so-, social cognition},
}

@article{murdock_exploration_2017,
	title = {Exploration and exploitation of {Victorian} science in {Darwin}'s reading notebooks},
	volume = {159},
	issn = {18737838},
	url = {http://dx.doi.org/10.1016/j.cognition.2016.11.012},
	doi = {10.1016/j.cognition.2016.11.012},
	abstract = {Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.},
	journal = {Cognition},
	author = {Murdock, Jaimie and Allen, Colin and DeDeo, Simon},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cognitive search, Exploration-exploitation, History of science, Information foraging, Scientific discovery, Topic modeling},
	pages = {117--126},
}

@inproceedings{Chancellor2017,
	title = {Multimodal {Classification} of {Moderated} {Online} {Pro}-{Eating} {Disorder} {Content}},
	isbn = {978-1-4503-4655-9},
	doi = {10.1145/3025453.3025985},
	abstract = {Social media sites are challenged by both the scale and variety of deviant behavior online. While algorithms can detect spam and obscenity, behaviors that break community guidelines on some sites are difficult because they have multimodal subtleties (images and/or text). Identifying these posts is often regulated to a few moderators. In this paper, we develop a deep learning classifier that jointly models textual and visual characteristics of pro-eating disorder content that violates community guidelines. Using a million Tumblr photo posts, our classifier discovers deviant content efficiently while also maintaining high recall (85\%). Our approach uses human sensitivity throughout to guide the creation, curation, and understanding of this approach to challenging, deviant content. We discuss how automation might impact community moderation, and the ethical and social obligations of this area.},
	author = {Chancellor, Stevie and Kalantidis, Yannis and Pater, Jessica A. and De Choudhury, Munmun and Shamma, David A.},
	year = {2017},
	pages = {3213--3226},
}

@inproceedings{xu_inferring_2015,
	title = {Inferring {User} {Interests} on {Tumblr}},
	isbn = {978-3-319-16267-6},
	doi = {10.1007/978-3-319-16268-3},
	abstract = {International conference proceedings. Includes author index. This book constitutes the refereed proceedings of the 8th International Conference on Social Computing, Behavioral-Cultural Modeling, and Prediction, SBP 2015, held in Washington, DC, USA, in March/April 2015. The 24 full papers presented together with 36 poster papers were carefully reviewed and selected from 118 submissions. The goal of the conference was to advance our understanding of human behavior through the development and application of mathematical, computational, statistical, simulation, predictive and other models that provide fundamental insights into factors contributing to human socio-cultural dynamics. The topical areas addressed by the papers are social and behavioral sciences, health sciences, engineering, computer and information science. Basic research on sociocultural and behavioral processes using SBP -- Methodological issues in SBP -- Health applications of SBP -- Other applications of SBP.},
	booktitle = {Social {Computing}, {Behavioral}-{Cultural} {Modeling}, and {Prediction}: 8th {International} {Conference}},
	author = {Xu, Jiejun and Lu, Tsai-Ching},
	year = {2015},
	note = {ISSN: 16113349},
	keywords = {bi-relational graph, multi-label learning, social media, tumblr, user interest modeling},
}

@article{Tiidenberg2014,
	title = {Bringing sexy back: {Reclaiming} the body aesthetic via self-shooting},
	volume = {8},
	issn = {18027962},
	doi = {10.5817/CP2014-1-3},
	abstract = {This paper is based on visual narrative analysis of cyber-ethnographic material from a 2.5 year field-research with 'not safe for work' [NSFW] bloggers and self-shooters on tumblr.com. I use Koskela's concept of 'empowering exhibitionism', Waskul's 'erotic looking glass', and Foucault's 'technologies of the self' to analyze self-shooting (taking photos of one-self). Constricting societal norms of sexuality, body shape and body practices influence how my participants (N=20, 10 female, 9 male, 1 transgender, ages 21 - 51, average age 34) live their embodied and sexual lives. Through self-shooting and by negotiating the community specific issues of control, power and the gaze, they are able to construct a new, empowered, embodied identity for themselves. I look at self-shooting and selfie-blogging as a practice of reclaiming control over one's embodied self AND over the body-aesthetic, thus appropriating what is and is not 'sexy'. The NSFW self-shooting community offers a safe space otherwise so hard to find within the body/sexuality-normative mainstream culture. This makes self-shooting a collective therapeutic activity. In their self-images participants construct themselves as 'beautiful', 'sexy', 'devious', 'more than just a mother and an employee' and as someone who 'likes their body instead of trying to not hate it'. The technologies of the self activated through diaristic blogging and selfie sharing, along with the empowerment from interactions with peers take bloggers on a path of sexual awakening and reintroduce them to their own bodies. [ABSTRACT FROM AUTHOR]},
	number = {1},
	journal = {Cyberpsychology},
	author = {Tiidenberg, Katrin},
	year = {2014},
	keywords = {Body, Self-identity, Selfies, Sexuality, Visual narrative analysis},
}

@article{barron_individuals_2018,
	title = {Individuals, institutions, and innovation in the debates of the {French} {Revolution}},
	volume = {115},
	issn = {10916490},
	doi = {10.1073/pnas.1717729115},
	abstract = {The French Revolution brought principles of “liberty, equality, fraternity” to bear on the day-to-day challenges of governing what was then the largest country in Europe. Its experiments provided a model for future revolutions and democracies across the globe, but this first modern revolution had no model to follow. Using reconstructed transcripts of debates held in the Revolution’s first parliament, we present a quantitative analysis of how this body managed innovation. We use information theory to track the creation, transmission, and destruction of word-use patterns across over 40,000 speeches and a thousand speakers. The parliament as a whole was biased toward the adoption of new patterns, but speakers’ individual qualities could break these overall trends. Speakers on the left innovated at higher rates, while speakers on the right acted to preserve prior patterns. Key players such as Robespierre (on the left) and Abbé Maury (on the right) played information-processing roles emblematic of their politics. Newly created organizational functions—such as the Assembly president and committee chairs—had significant effects on debate outcomes, and a distinct transition appears midway through the parliament when committees, external to the debate process, gained new powers to “propose and dispose.” Taken together, these quantitative results align with existing qualitative interpretations, but also reveal crucial information-processing dynamics that have hitherto been overlooked. Great orators had the public’s attention, but deputies (mostly on the political left) who mastered the committee system gained new powers to shape revolutionary legislation.},
	number = {18},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Barron, Alexander T.J. and Huang, Jenny and Spang, Rebecca L. and DeDeo, Simon},
	year = {2018},
	keywords = {Cognitive science, Computational social science, Cultural evolution, Digital history, Political science},
	pages = {4607--4612},
}

@article{Attu2017,
	title = {What people study when they study {Tumblr}: {Classifying} {Tumblr}-related academic research},
	volume = {73},
	issn = {00220418},
	doi = {10.1108/JD-08-2016-0101},
	abstract = {Access to this document was granted through an Emerald subscription provided by emerald-srm:235237 [] For Authors If you would like to write for this, or any other Emerald publication, then please use our Emerald for Authors service information about how to choose which publication to write for and submission guidelines are available for all. Please visit www.emeraldinsight.com/authors for more information. About Emerald www.emeraldinsight.com Emerald is a global publisher linking research and practice to the benefit of society. The company manages a portfolio of more than 290 journals and over 2,350 books and book series volumes, as well as providing an extensive range of online products and additional customer resources and services. Abstract Purpose – Since its launch in 2007, research has been carried out on the popular social networking website Tumblr. The purpose of this paper is to identify published Tumblr-based research, classify it to understand approaches and methods, and provide methodological recommendations for others. Design/methodology/approach – Research regarding Tumblr was identified. Following a review of the literature, a classification scheme was adapted and applied, to understand research focus. Papers were quantitatively classified using open coded content analysis of method, subject, approach, and topic. Findings – The majority of published work relating to Tumblr concentrates on conceptual issues, followed by aspects of the messages sent. This has evolved over time. Perceived benefits are the platform's long-form text posts, ability to track tags, and the multimodal nature of the platform. Severe research limitations are caused by the lack of demographic, geo-spatial, and temporal metadata attached to individual posts, the limited Advanced Programming Interface, restricted access to data, and the large amounts of ephemeral posts on the site. The classification derived here provides a framework that can be used to analyse social media research, and in which to position Tumblr-related work, with recommendations on benefits and limitations of the platform for researchers.},
	number = {3},
	journal = {Journal of Documentation},
	author = {Attu, Rose and Terras, Melissa},
	year = {2017},
	keywords = {Blog, Classification, Content analysis, Keyword analysis, Microblogging, Research methods, Social network analysis, Social network systems, Tumblr, Twitter},
	pages = {528--554},
}

@article{sims_literary_2019,
	title = {Literary {Event} {Detection}},
	url = {https://www.aclweb.org/anthology/P19-1353},
	abstract = {In this work we present a new dataset of literary events\{---\}events that are depicted as taking place within the imagined space of a novel. While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language. We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9. We then apply this model to a corpus of novels split across two dimensions\{---\}prestige and popularity\{---\}and demonstrate that there are statistically significant differences in the distribution of events for prestige.},
	journal = {Proceedings of the 57th Conference of the Association for Computational Linguistics},
	author = {Sims, Matthew and Park, Jong Ho and Bamman, David},
	year = {2019},
	pages = {3623--3634},
}

@article{naveed_bad_2011,
	title = {Bad news travel fast: {A} content-based analysis of interestingness on twitter},
	doi = {10.1145/2527031.2527052},
	abstract = {On the microblogging site Twitter, users can forward any message they receive to all of their followers. This is called a retweet and is usually done when users find a message particularly interesting and worth sharing with others. Thus, retweets reflect what the Twitter community considers interesting on a global scale, and can be used as a function of interestingness to generate a model to describe the content-based characteristics of retweets. In this paper, we analyze a set of high- and low-level content-based features on several large collections of Twitter messages. We train a prediction model to forecast for a given tweet its likelihood of being retweeted based on its contents. From the parameters learned by the model we de- duce what are the influential content features that contribute to the likelihood of a retweet. As a result we obtain insights into what makes a message on Twitter worth retweeting and, thus, interest- ing.},
	journal = {Proceedings of the 3rd International Web Science Conference, WebSci 2011},
	author = {Naveed, Nasir and Gottron, Thomas and Kunegis, Jérôme and Alhadi, Arifah Che},
	year = {2011},
	note = {ISBN: 9781450308557},
	keywords = {Interestingness, Microblog, Retweet, Sentiment analysis, Topic modeling, Twitter},
}

@article{blunsom_discriminative_2006,
	title = {Discriminative {Word} {Alignment} with {Conditional} {Random} {Fields}},
	doi = {10.3115/1220175.1220184},
	abstract = {In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of- the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.},
	number = {July},
	journal = {Acl-2006},
	author = {Blunsom, Phil and Cohn, Trevor},
	year = {2006},
	pages = {65--72},
}

@article{Blei2003,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	issn = {15324435},
	doi = {10.1162/jmlr.2003.3.4-5.993},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	year = {2003},
	pmid = {21362469},
	note = {arXiv: 1111.6189v1
ISBN: 9781577352815},
	pages = {993--1022},
}

@article{VanderMaaten2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {02624079},
	doi = {10.1007/s10479-011-0841-3},
	abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	journal = {Journal of Machine Learning Research},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	year = {2008},
	pmid = {20652508},
	note = {arXiv: 1307.1662
ISBN: 1532-4435},
	keywords = {dimensionality reduction, embedding algorithms, manifold learning, multidimensional scaling, visualization},
	pages = {2579--2605},
}

@inproceedings{Mikolov2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	isbn = {2150-8097},
	url = {http://www.crossref.org/deleted_DOI.html},
	doi = {10.1162/jmlr.2003.3.4-5.951},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	pmid = {903},
	note = {arXiv: 1310.4546
ISSN: 0003-6951},
	pages = {3111--3119},
}

@article{dey_emtagger:_2017,
	title = {{EmTaggeR}: {A} word embedding based novel method for hashtag recommendation on twitter},
	volume = {2017-Novem},
	issn = {23759259},
	doi = {10.1109/ICDMW.2017.145},
	abstract = {The hashtag recommendation problem addresses recommending (suggesting) one or more hashtags to explicitly tag a post made on a given social network platform, based upon the content and context of the post. In this work, we propose a novel methodology for hashtag recommendation for microblog posts, specifically Twitter. The methodology, EmTaggeR, is built upon a training-testing framework that builds on the top of the concept of word embedding. The training phase comprises of learning word vectors associated with each hashtag, and deriving a word embedding for each hashtag. We provide two training procedures, one in which each hashtag is trained with a separate word embedding model applicable in the context of that hashtag, and another in which each hashtag obtains its embedding from a global context. The testing phase constitutes computing the average word embedding of the test post, and finding the similarity of this embedding with the known embeddings of the hashtags. The tweets that contain the most-similar hashtag are extracted, and all the hashtags that appear in these tweets are ranked in terms of embedding similarity scores. The top-K hashtags that appear in this ranked list, are recommended for the given test post. Our system produces F1 score of 50.83\%, improving over the LDA baseline by around 6.53 times, outperforming the best-performing system known in the literature that provides a lift of 6.42 times. EmTaggeR is a fast, scalable and lightweight system, which makes it practical to deploy in real-life applications.},
	number = {i},
	journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
	author = {Dey, Kuntal and Shrivastava, Ritvik and Kaushik, Saroj and Subramaniam, L. Venkata},
	year = {2017},
	note = {ISBN: 9781538614808},
	keywords = {Hashtag recommendation, Twitter},
	pages = {1025--1032},
}

@article{zhang_hashtag_2017,
	title = {Hashtag recommendation for multimodal microblog using co-attention network},
	issn = {10450823},
	abstract = {In microblogging services, authors can use hashtags to mark keywords or topics. Many live social media applications (e.g., microblog retrieval, classification) can gain great benefits from these manually labeled tags. However, only a small portion of microblogs contain hashtags inputed by users. Moreover, many microblog posts contain not only textual content but also images. These visual resources also provide valuable information that may not be included in the textual content. So that it can also help to recommend hashtags more accurately. Motivated by the successful use of the attention mechanism, we propose a co-attention network incorporating textual and visual information to recommend hashtags for multimodal tweets. Experimental result on the data collected from Twitter demonstrated that the proposed method can achieve better performance than state-of-the-art methods using textual information only.},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Zhang, Qi and Wang, Jiawen and Huang, Haoran and Huang, Xuanjing and Gong, Yeyun},
	year = {2017},
	note = {ISBN: 9780999241103},
	pages = {3420--3426},
}

@article{Otsuka2016,
	title = {A hashtag recommendation system for twitter data streams},
	volume = {3},
	issn = {2197-4314},
	url = {http://computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-016-0028-9},
	doi = {10.1186/s40649-016-0028-9},
	abstract = {Background: Twitter has evolved into a powerful communication and information sharing tool used by millions of people around the world to post what is happening now. A hashtag, a keyword prefixed with a hash symbol (\#), is a feature in Twitter to organize tweets and facilitate effective search among a massive volume of data. In this paper, we propose an automatic hashtag recommendation system that helps users find new hashtags related to their interests on-demand. Methods: For hashtag ranking, we propose the Hashtag Frequency-Inverse Hashtag Ubiquity (HF-IHU) ranking scheme, which is a variation of the well-known TF-IDF, that considers hashtag relevancy, as well as data sparseness which is one of the key challenges in analyzing microblog data. Our system is built on top of Hadoop, a leading platform for distributed computing, to provide scalable performance using MapReduce. Experiments on a large Twitter data set demonstrate that our method successfully yields relevant hashtags for user’s interest and that recommendations are more stable and reliable than ranking tags based on tweet content similarity. Results and conclusions: Our results show that HF-IHU can achieve over 30 \% hashtag recall when asked to identify the top 10 relevant hashtags for a particular tweet. Furthermore, our method out-performs kNN, k-popularity, and Naïve Bayes by 69, 54, and 17 \%, respectively, on recall of the top 200 hashtags.},
	number = {1},
	journal = {Computational Social Networks},
	author = {Otsuka, Eriko and Wallace, Scott A. and Chiu, David},
	year = {2016},
	note = {Publisher: Springer International Publishing},
	keywords = {Recommendation, Twitter, recommendation, twitter},
	pages = {3},
}

@article{Huang2016,
	title = {Hashtag {Recommendation} {Using} {End}-{To}-{End} {Memory} {Networks} with {Hierarchical} {Attention}},
	abstract = {On microblogging services, people usually use hashtags to mark microblogs, which have a specific theme or content, making them easier for users to find. Hence, how to automatically recommend hashtags for microblogs has received much attention in recent years. Previous deep neural network-based hashtag recommendation approaches converted the task into a multi-class classification problem. However, most of these methods only took the microblog itself into consideration. Motivated by the intuition that the history of users should impact the recommendation procedure, in this work, we extend end-to-end memory networks to perform this task. We incorporate the histories of users into the external memory and introduce a hierarchical attention mechanism to select more appropriate histories. To train and evaluate the proposed method, we also construct a dataset based on microblogs collected from Twitter. Experimental results demonstrate that the proposed methods can significantly outperform state-of-the-art methods. By incorporating the hierarchical attention mechanism, the relative improvement in the proposed method over the state-of-the-art method is around 67.9\% in the F1-score.},
	author = {Huang, Haoran and Zhang, Qi and Gong, Yeyun and Huang, Xuanjing},
	year = {2016},
	pages = {943--952},
}

@article{an_political_2019,
	title = {Political {Discussions} in {Homogeneous} and {Cross}-{Cutting} {Communication} {Spaces}},
	url = {http://arxiv.org/abs/1904.05643},
	abstract = {Online platforms, such as Facebook, Twitter, and Reddit, provide users with a rich set of features for sharing and consuming political information, expressing political opinions, and exchanging potentially contrary political views. In such activities, two types of communication spaces naturally emerge: those dominated by exchanges between politically homogeneous users and those that allow and encourage cross-cutting exchanges in politically heterogeneous groups. While research on political talk in online environments abounds, we know surprisingly little about the potentially varying nature of discussions in politically homogeneous spaces as compared to cross-cutting communication spaces. To fill this gap, we use Reddit to explore the nature of political discussions in homogeneous and cross-cutting communication spaces. In particular, we develop an analytical template to study interaction and linguistic patterns within and between politically homogeneous and heterogeneous communication spaces. Our analyses reveal different behavioral patterns in homogeneous and cross-cutting communications spaces. We discuss theoretical and practical implications in the context of research on political talk online.},
	number = {Icwsm},
	author = {An, Jisun and Kwak, Haewoon and Posegga, Oliver and Jungherr, Andreas},
	year = {2019},
	note = {arXiv: 1904.05643},
}

@article{pavalanathan_multidimensional_2017,
	title = {A {Multidimensional} {Lexicon} for {Interpersonal} {Stancetaking}},
	url = {https://www.aclweb.org/anthology/P17-1082},
	doi = {10.18653/v1/p17-1082},
	abstract = {© 2017 Association for Computational Linguistics. The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself. Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity. We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions. We validate these dimensions intrinsically and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.},
	author = {Pavalanathan, Umashanthi and Fitzpatrick, Jim and Kiesling, Scott and Eisenstein, Jacob},
	year = {2017},
	pages = {884--895},
}

@article{leicht_community_2008,
	title = {Community structure in directed networks},
	volume = {100},
	issn = {00319007},
	doi = {10.1103/PhysRevLett.100.118703},
	abstract = {We consider the problem of finding communities or modules in directed networks. The most common approach to this problem in the previous literature has been simply to ignore edge direction and apply methods developed for community discovery in undirected networks, but this approach discards potentially useful information contained in the edge directions. Here we show how the widely used benefit function known as modularity can be generalized in a principled fashion to incorporate the information contained in edge directions. This in turn allows us to find communities by maximizing the modularity over possible divisions of a network, which we do using an algorithm based on the eigenvectors of the corresponding modularity matrix. This method is shown to give demonstrably better results than previous methods on a variety of test networks, both real and computer-generated.},
	number = {11},
	journal = {Physical Review Letters},
	author = {Leicht, E. A. and Newman, M. E.J.},
	year = {2008},
	note = {arXiv: 0709.4500v1},
	pages = {1--5},
}

@article{dugue_directed_2015,
	title = {Directed {Louvain} : maximizing modularity in directed networks {To} cite this version : {HAL} {Id} : hal-01231784 {Directed} {Louvain} : maximizing modularity in directed networks e},
	author = {Dugué, Nicolas and Perez, Anthony},
	year = {2015},
	pages = {0--14},
}

@article{nissim_fair_2019,
	title = {Fair is {Better} than {Sensational}:{Man} is to {Doctor} as {Woman} is to {Doctor}},
	url = {http://arxiv.org/abs/1905.09866},
	abstract = {Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also exposed how strongly human biases are encoded in vector spaces built on natural language. While finding that queen is the answer to man is to king as woman is to X leaves us in awe, papers have also reported finding analogies deeply infused with human biases, like man is to computer programmer as woman is to homemaker, which instead leave us with worry and rage. In this work we show that,often unknowingly, embedding spaces have not been treated fairly. Through a series of simple experiments, we highlight practical and theoretical problems in previous works, and demonstrate that some of the most widely used biased analogies are in fact not supported by the data. We claim that rather than striving to find sensational biases, we should aim at observing the data "as is", which is biased enough. This should serve as a fair starting point to properly address the evident, serious, and compelling problem of human bias in word embeddings.},
	author = {Nissim, Malvina and van Noord, Rik and van der Goot, Rob},
	year = {2019},
	note = {arXiv: 1905.09866},
}

@article{khan_network_2017,
	title = {Network {Community} {Detection}: {A} {Review} and {Visual} {Survey}},
	url = {http://arxiv.org/abs/1708.00977},
	abstract = {Community structure is an important area of research. It has received a considerable attention from the scientific community. Despite its importance, one of the key problems in locating information about community detection is the diverse spread of related articles across various disciplines. To the best of our knowledge, there is no current comprehensive review of recent literature which uses a scientometric analysis using complex networks analysis covering all relevant articles from the Web of Science (WoS). Here we present a visual survey of key literature using CiteSpace. The idea is to identify emerging trends besides using network techniques to examine the evolution of the domain. Towards that end, we identify the most influential, central, as well as active nodes using scientometric analyses. We examine authors, key articles, cited references, core subject categories, key journals, institutions, as well as countries. The exploration of the scientometric literature of the domain reveals that Yong Wang is a pivot node with the highest centrality. Additionally, we have observed that Mark Newman is the most highly cited author in the network. We have also identified that the journal, "Reviews of Modern Physics" has the strongest citation burst. In terms of cited documents, an article by Andrea Lancichinetti has the highest centrality score. We have also discovered that the origin of the key publications in this domain is from the United States. Whereas Scotland has the strongest and longest citation burst. Additionally, we have found that the categories of "Computer Science" and "Engineering" lead other categories based on frequency and centrality respectively.},
	author = {Khan, Bisma S. and Niazi, Muaz A.},
	year = {2017},
	note = {arXiv: 1708.00977},
	keywords = {citespace, community detection, complex networks, scientometric, visual survey},
}

@article{manjavacas_feasibility_2019,
	title = {On the {Feasibility} of {Automated} {Detection} of {Allusive} {Text} {Reuse}},
	volume = {2},
	url = {http://arxiv.org/abs/1905.02973},
	abstract = {The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely---commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.},
	author = {Manjavacas, Enrique and Long, Brian and Kestemont, Mike},
	year = {2019},
	note = {arXiv: 1905.02973},
	pages = {104--114},
}

@article{malliaros_clustering_2013,
	title = {Clustering and community detection in directed networks: {A} survey},
	volume = {533},
	issn = {03701573},
	url = {http://dx.doi.org/10.1016/j.physrep.2013.08.002},
	doi = {10.1016/j.physrep.2013.08.002},
	abstract = {Networks (or graphs) appear as dominant structures in diverse domains, including sociology, biology, neuroscience and computer science. In most of the aforementioned cases graphs are directed - in the sense that there is directionality on the edges, making the semantics of the edges nonsymmetric as the source node transmits some property to the target one but not vice versa. An interesting feature that real networks present is the clustering or community structure property, under which the graph topology is organized into modules commonly called communities or clusters. The essence here is that nodes of the same community are highly similar while on the contrary, nodes across communities present low similarity. Revealing the underlying community structure of directed complex networks has become a crucial and interdisciplinary topic with a plethora of relevant application domains. Therefore, naturally there is a recent wealth of research production in the area of mining directed graphs - with clustering being the primary method sought and the primary tool for community detection and evaluation. The goal of this paper is to offer an in-depth comparative review of the methods presented so far for clustering directed networks along with the relevant necessary methodological background and also related applications. The survey commences by offering a concise review of the fundamental concepts and methodological base on which graph clustering algorithms capitalize on. Then we present the relevant work along two orthogonal classifications. The first one is mostly concerned with the methodological principles of the clustering algorithms, while the second one approaches the methods from the viewpoint regarding the properties of a good cluster in a directed network. Further, we present methods and metrics for evaluating graph clustering results, demonstrate interesting application domains and provide promising future research directions. © 2013 Elsevier B.V.},
	number = {4},
	journal = {Physics Reports},
	author = {Malliaros, Fragkiskos D. and Vazirgiannis, Michalis},
	year = {2013},
	note = {Publisher: Elsevier B.V.},
	keywords = {Community detection, Complex networks, Directed networks, Graph clustering, Graph mining},
	pages = {95--142},
}

@article{chintalapudi_survey_2015,
	title = {A survey on community detection algorithms in large scale real world networks},
	abstract = {Community Structure is one of the most relevant features of real world networks. Detecting such structures in large scale networks is a challenging task in scientific world. These are similar to clusters in which intra cluster density is more than the inter cluster density. This paper reviews the prominent community detection algorithms that detect both disjoint and overlapped communities. These algorithms are experimented on benchmark dataset Zachary's karate club. Obtained number of communities is compared with the ground truth. The quality measures namely modularity and Normalized Mutual Information (NMI) are computed for all disjoint community detection algorithms. As a result of voluminous research done in this area the overlapped communities are come into the picture. Overlapped community means that a node in the network may be affiliated to more than one community. To test these algorithms Omega index is also included in this survey. After reviewing all these algorithms, this survey concludes that quality and scalability are the major issues in this area and also the measure used for detecting communities needs more computational power. So, one need to use either high performance computing framework with Graphical Processing Units (GPU) or Hadoop framework for distributed computing. Hence, this will balance the trade-off between running time and quality.},
	journal = {Computing for Sustainable Global Development (INDIACom), 2015 2nd International Conference on},
	author = {Chintalapudi, S.R. and Krishna Prasad, M.H.M.},
	year = {2015},
	note = {ISBN: 9789380544168},
	keywords = {Algorithm design and analysis, Benchmark testing, Communities, Community detection, Detection algorithms, GPU, Graph mining, Graphics processing units, Hadoop framework, Image edge detection, NMI, Omega index algorithms, Social network services, benchmark dataset Zachary karate club, cluster density, community detection algorithms, data mining, data mining clustering, directed graphs, directed networks, disjoint communities, disjoint community detection, distributed computing, graphical processing units, high performance computing framework, large scale networks, large scale real world networks, modularity mutual information, network theory (graphs), normalized mutual information, overlapped communities, overlapped community detection, pattern clustering, quality measures, undirected networks},
	pages = {1323--1327},
}

@article{zhao_pscan:_2013,
	title = {{PSCAN}: {A} parallel {Structural} {Clustering} {Algorithm} for big {Networks} in {MapReduce}},
	issn = {1550445X},
	doi = {10.1109/AINA.2013.47},
	abstract = {Big data such as complex networks with over millions of vertices and edges is infeasible to process using conventional computation. MapReduce is a programming model that empowers us to analyze big data in a cluster of computers. In this paper we propose a Parallel Structural Clustering Algorithm for big Networks (PSCAN) in MapReduce for the detection of clusters or community structures in big networks such as Twitter. PSCAN is based on the structural clustering algorithm of SCAN, which not only finds cluster accurately, but also identifies vertices playing special roles such as hubs and outliers. An empirical evaluation of PSCAN using both real and synthetic networks demonstrated an outstanding performance in terms of accuracy and running time. We analyzed a Twitter network with over 40 million users and 1.4 billion follower/following relationships by using PSCAN on a Hadoop cluster with 15 computers. The result shows that PSCAN successfully detected interesting communities of people who share common interests.},
	journal = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
	author = {Zhao, Weizhong and Martha, Venkata Swamy and Xu, Xiaowei},
	year = {2013},
	note = {Publisher: IEEE
ISBN: 9780769549538},
	keywords = {Big data, Community structures, Hadoop, MapReduce, Network clustering algorithms},
	pages = {862--869},
}

@article{ritter_data-driven_2011,
	title = {Data-{Driven} {Response} {Generation} in {Social} {Media}},
	url = {http://aclweb.org/anthology/D11-1054},
	abstract = {We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15\% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.},
	journal = {Proceedings of the Empirical Methods in Natural Language Processing},
	author = {Ritter, Alan and Cherry, Colin and Dolan, William B},
	year = {2011},
	pages = {583--593},
}

@article{posch_p621-posch_2013,
	title = {P621-{Posch}},
	author = {Posch, Lisa and Wagner, Claudia and Strohmaier, Markus},
	year = {2013},
	note = {ISBN: 9781450320382},
	keywords = {hashtags, semantics, social structure, twitter},
	pages = {621--628},
}

@article{pang_revisiting_2012,
	title = {Revisiting the {Predictability} of {Language}: {Response} {Completion} in {Social} {Media}},
	abstract = {The question " how predictable is English? " has long fascinated researchers. While prior work has focused on formal English typically used in news articles, we turn to texts gener-ated by users in online settings that are more informal in nature. We are motivated by a novel application scenario: given the difficulty of typing on mobile devices, can we help re-duce typing effort with message completion, especially in conversational settings? We pro-pose a method for automatic response comple-tion. Our approach models both the language used in responses and the specific context pro-vided by the original message. Our experi-mental results on a large-scale dataset show that both components help reduce typing ef-fort. We also perform an information-theoretic study in this setting and examine the entropy of user-generated content, especially in con-versational scenarios, to better understand pre-dictability of user generated English.},
	number = {July},
	author = {Pang, Bo and Ravi, Sujith},
	year = {2012},
	pages = {1489--1499},
}

@article{shoemark_topic_2018,
	title = {Topic and audience effects on distinctively {Scottish} vocabulary usage in {Twitter} data},
	doi = {10.18653/v1/w17-4908},
	abstract = {Sociolinguistic research suggests that speakers modulate their language style in response to their audience. Similar ef-fects have recently been claimed to occur in the informal written context of Twit-ter, with users choosing less region-specific and non-standard vocabulary when address-ing larger audiences. However, these stud-ies have not carefully controlled for the possible confound of topic: that is, tweets addressed to a broad audience might also tend towards topics that engender a more formal style. In addition, it is not clear to what extent previous results generalize to different samples of users. Using mixed-effects models, we show that audience and topic have independent effects on the rate of distinctively Scottish usage in two demo-graphically distinct Twitter user samples. However, not all effects are consistent be-tween the two groups,underscoring the im-portance of replicating studies on distinct user samples before drawing strong conclu-sions from social media data.},
	author = {Shoemark, Philippa and Kirby, James and Goldwater, Sharon},
	year = {2018},
	pages = {59--68},
}

@inproceedings{kennedy_social_2016,
	title = {Social {Computing}},
	volume = {623},
	isbn = {978-981-10-2052-0},
	url = {http://link.springer.com/10.1007/978-981-10-2053-7},
	doi = {10.1007/978-981-10-2053-7},
	author = {Kennedy, William G and Agarwal, Nitin and Yang, Shanchieh Jay},
	year = {2016},
}

@article{ryan_provided_2001,
	title = {Provided by the author(s) and {University} {College} {Dublin} {Library} in accordance with publisher policies. {Please} cite the published version when available.},
	volume = {40},
	issn = {13564765},
	doi = {http://dx.doi.org/10.1680/geot.2008.T.003},
	number = {1},
	journal = {Family Process},
	author = {Ryan, Dermot and Carr, Alan},
	year = {2001},
	note = {ISBN: 3531716735},
	pages = {67--77},
}

@article{liang_lambda_2013,
	title = {Lambda {Dependency}-{Based} {Compositional} {Semantics}},
	issn = {04194217},
	url = {http://arxiv.org/abs/1309.4408},
	doi = {10.1162/COLI},
	abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
	number = {April 2016},
	author = {Liang, Percy},
	year = {2013},
	pmid = {22251136},
	note = {arXiv: 1309.4408
ISBN: 9781608459858},
}

@phdthesis{Fazekas2014,
	title = {Queer and {Unusual} {Space}: {White} {Supremacy} in {Slash} {Fanfiction}},
	url = {https://qspace.library.queensu.ca/handle/1974/12609},
	abstract = {Thesis (Master, Gender Studies) -- Queen's University, 2014-11-05 15:48:44.304},
	school = {Queen's University},
	author = {Fazekas, Angela},
	year = {2014},
	keywords = {Affect Theory, Critical Race Theory, Fan Studies, Fandom, Fanfiction, Pop Culture, Queer Theory, Social Media, Thesis, Whiteness},
}

@article{raghavan_near_2007,
	title = {Near linear time algorithm to detect community structures in large-scale networks},
	volume = {76},
	issn = {15393755},
	doi = {10.1103/PhysRevE.76.036106},
	abstract = {Community detection and analysis is an important methodology for understanding the organization of various real-world networks and has applications in problems as diverse as consensus formation in social communities or the identification of functional modules in biochemical networks. Currently used algorithms that identify the community structures in large-scale real-world networks require a priori information such as the number and sizes of communities or are computationally expensive. In this paper we investigate a simple label propagation algorithm that uses the network structure alone as its guide and requires neither optimization of a pre-defined objective function nor prior information about the communities. In our algorithm every node is initialized with a unique label and at every step each node adopts the label that most of its neighbors currently have. In this iterative process densely connected groups of nodes form a consensus on a unique label to form communities. We validate the algorithm by applying it to networks whose community structures are known. We also demonstrate that the algorithm takes an almost linear time and hence it is computationally less expensive than what was possible so far.},
	number = {3},
	journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	author = {Raghavan, Usha Nandini and Albert, Réka and Kumara, Soundar},
	year = {2007},
	note = {arXiv: 0709.2938v1},
	pages = {1--12},
}

@article{gregory_finding_2010,
	title = {Finding overlapping communities in networks by label propagation},
	volume = {12},
	issn = {13672630},
	doi = {10.1088/1367-2630/12/10/103018},
	abstract = {We propose an algorithm for finding overlapping community structure in very large networks. The algorithm is based on the label propagation technique of Raghavan, Albert and Kumara, but is able to detect communities that overlap. Like the original algorithm, vertices have labels that propagate between neighbouring vertices so that members of a community reach a consensus on their community membership. Our main contribution is to extend the label and propagation step to include information about more than one community: each vertex can now belong to up to v communities, where v is the parameter of the algorithm. Our algorithm can also handle weighted and bipartite networks. Tests on an independently designed set of benchmarks, and on real networks, show the algorithm to be highly effective in recovering overlapping communities. It is also very fast and can process very large and dense networks in a short time.},
	journal = {New Journal of Physics},
	author = {Gregory, Steve},
	year = {2010},
}

@article{kuzmin_parallelizing_2015,
	title = {Parallelizing {SLPA} for {Scalable} {Overlapping} {Community} {Detection}},
	volume = {2015},
	issn = {1058-9244},
	doi = {10.1155/2015/461362},
	abstract = {Communities in networks are groups of nodes whose connections to the nodes in a community are stronger than with the nodes in the rest of the network. Quite often nodes participate in multiple communities; that is, communities can overlap. In this paper, we first analyze what other researchers have done to utilize high performance computing to perform efficient community detection in social, biological, and other networks. We note that detection of overlapping communities is more computationally intensive than disjoint community detection, and the former presents new challenges that algorithm designers have to face. Moreover, the efficiency of many existing algorithms grows superlinearly with the network size making them unsuitable to process large datasets. We use the Speaker-Listener Label Propagation Algorithm (SLPA) as the basis for our parallel overlapping community detection implementation. SLPA provides near linear time overlapping community detection and is well suited for parallelization. We explore the benefits of a multithreaded programming paradigm and show that it yields a significant performance gain over sequential execution while preserving the high quality of community detection. The algorithm was tested on four real-world datasets with up to 5.5 million nodes and 170 million edges. In order to assess the quality of community detection, at least 4 different metrics were used for each of the datasets.},
	journal = {Scientific Programming},
	author = {Kuzmin, Konstantinbo and Chen, Mingming and Szymanski, Boleslaw K.},
	year = {2015},
	pages = {1--18},
}

@article{xie_towards_2012,
	title = {Towards linear time overlapping community detection in social networks},
	volume = {7301 LNAI},
	issn = {03029743},
	doi = {10.1007/978-3-642-30220-6_3},
	abstract = {Membership diversity is a characteristic aspect of social networks in which a person may belong to more than one social group. For this reason, discovering overlapping structures is necessary for realistic social analysis. In this paper, we present a fast algorithm1, called SLPA, for overlapping community detection in large-scale networks. SLPA spreads labels according to dynamic interaction rules. It can be applied to both unipartite and bipartite networks. It is also able to uncover overlapping nested hierarchy. The time complexity of SLPA scales linearly with the number of edges in the network. Experiments in both synthetic and real- world networks show that SLPA has an excellent performance in identifying both node and community level overlapping structures.},
	number = {PART 2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Xie, Jierui and Szymanski, Boleslaw K.},
	year = {2012},
	note = {arXiv: 1202.2465v1
ISBN: 9783642302190},
	pages = {25--36},
}

@article{xie_overlapping_2011,
	title = {Overlapping {Community} {Detection} in {Networks}: the {State} of the {Art} and {Comparative} {Study}},
	volume = {45},
	url = {http://arxiv.org/abs/1110.5813%0Ahttp://dx.doi.org/10.1145/2501654.2501657},
	doi = {10.1145/2501654.2501657},
	abstract = {This paper reviews the state of the art in overlapping community detection algorithms, quality measures, and benchmarks. A thorough comparison of different algorithms (a total of fourteen) is provided. In addition to community level evaluation, we propose a framework for evaluating algorithms' ability to detect overlapping nodes, which helps to assess over-detection and under-detection. After considering community level detection performance measured by Normalized Mutual Information, the Omega index, and node level detection performance measured by F-score, we reached the following conclusions. For low overlapping density networks, SLPA, OSLOM, Game and COPRA offer better performance than the other tested algorithms. For networks with high overlapping density and high overlapping diversity, both SLPA and Game provide relatively stable performance. However, test results also suggest that the detection in such networks is still not yet fully resolved. A common feature observed by various algorithms in real-world networks is the relatively small fraction of overlapping nodes (typically less than 30\%), each of which belongs to only 2 or 3 communities.},
	number = {4},
	author = {Xie, Jierui and Kelley, Stephen and Szymanski, Boleslaw K.},
	year = {2011},
	note = {arXiv: 1110.5813},
	pages = {1--37},
}

@article{xie_slpa:_2011,
	title = {{SLPA}: {Uncovering} overlapping communities in social networks via a speaker-listener interaction dynamic process},
	issn = {15504786},
	doi = {10.1109/ICDMW.2011.154},
	abstract = {Overlap is one of the characteristics of social networks, in which a person may belong to more than one social group. For this reason, discovering overlapping structures is necessary for realistic social analysis. In this paper, we present a novel, general framework to detect and analyze both individual overlapping nodes and entire communities. In this framework, nodes exchange labels according to dynamic interaction rules. A specific implementation called Speaker-listener Label Propagation Algorithm (SLPA1) demonstrates an excellent performance in identifying both overlapping nodes and overlapping communities with different degrees of diversity.},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	author = {Xie, Jierui and Szymanski, Boleslaw K. and Liu, Xiaoming},
	year = {2011},
	note = {arXiv: 1109.5720v3
ISBN: 9780769544090},
	keywords = {Algorithm, Dynamic interaction, Label propagation, Overlapping community detection, Social network},
	pages = {344--349},
}

@article{fortunato_community_2016,
	title = {Community detection in networks: {A} user guide},
	volume = {659},
	issn = {03701573},
	url = {http://dx.doi.org/10.1016/j.physrep.2016.09.002},
	doi = {10.1016/j.physrep.2016.09.002},
	abstract = {Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.},
	journal = {Physics Reports},
	author = {Fortunato, Santo and Hric, Darko},
	year = {2016},
	note = {Publisher: Elsevier B.V.},
	keywords = {Clustering, Communities, Networks},
	pages = {1--44},
}

@article{ahmed_network_2011,
	title = {Network {Sampling} via {Edge}-based {Node} {Selection} with {Graph} {Induction}},
	url = {papers2://publication/uuid/913072F5-95AD-4215-A5C3-09DC35A4AD3C},
	abstract = {In order to efficiently study the characteristics of network domains and support development of network systems (e.g. algorithms, pro- tocols that operate on networks), it is often necessary to sample a representative subgraph from a large complex network. While prior research has shown that topological (e.g. random-walk based) sampling methods produce more accurate samples than approaches based on node or edge sampling, they still do not produce samples that closely match the distributions of graph properties (e.g., de- gree) found in the original graph. In this paper, we observe that part of the problem is that any sampling process fundamentally bi- ases the structure of the sampled subgraph, since all neighbors of a sample node may not be included in the sampled subgraph. We address this problem using a novel sampling algorithm called TIES that (1) aims to offset this bias by using edge-based node selection, which favors selection of high-degree nodes, and (2) uses a graph induction step to select additional edges between sampled nodes to restore connectivity and bring the structure closer to that of the original graph. To understand the properties of TIES we compare it analytically to random node and edge sampling. We also evaluate the efficacy of TIES empirically using several real-world data sets. Across all datasets, we found that TIES produces samples that bet- ter match the original distributions. In terms of two distributional distance metrics, KS distance and skew divergence, we found that samples produced by TIES consistently outperform other sampling algorithms—with up to 2× reduction in KS distance and up to 3- 7×reduction in skew divergence, compared to the current state-of- the-art algorithms. 1.},
	author = {Ahmed, Nesreen K and Neville, J and Kompella, R R},
	year = {2011},
	pages = {1--10},
}

@inproceedings{leskovec_sampling_nodate,
	title = {Sampling from {Large} {Graphs}},
	isbn = {1-59593-339-5},
	author = {Leskovec, Jure and Faloutsos, Christos},
	keywords = {graph mining, graph sampling, scaling laws},
}

@article{you_graphrnn:_2018,
	title = {{GraphRNN}: {Generating} {Realistic} {Graphs} with {Deep} {Auto}-regressive {Models}},
	url = {http://arxiv.org/abs/1802.08773},
	abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
	author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	year = {2018},
	note = {arXiv: 1802.08773},
}

@article{lancichinetti_finding_2011,
	title = {Finding statistically significant communities in networks},
	volume = {6},
	issn = {19326203},
	doi = {10.1371/journal.pone.0018961},
	abstract = {Community structure is one of the main structural features of networks, revealing both their internal organization and the similarity of their elementary units. Despite the large variety of methods proposed to detect communities in graphs, there is a big need for multi-purpose techniques, able to handle different types of datasets and the subtleties of community structure. In this paper we present OSLOM (Order Statistics Local Optimization Method), the first method capable to detect clusters in networks accounting for edge directions, edge weights, overlapping communities, hierarchies and community dynamics. It is based on the local optimization of a fitness function expressing the statistical significance of clusters with respect to random fluctuations, which is estimated with tools of Extreme and Order Statistics. OSLOM can be used alone or as a refinement procedure of partitions/covers delivered by other techniques. We have also implemented sequential algorithms combining OSLOM with other fast techniques, so that the community structure of very large networks can be uncovered. Our method has a comparable performance as the best existing algorithms on artificial benchmark graphs. Several applications on real networks are shown as well. OSLOM is implemented in a freely available software (http://www.oslom.org), and we believe it will be a valuable tool in the analysis of networks.},
	number = {4},
	journal = {PLoS ONE},
	author = {Lancichinetti, Andrea and Radicchi, Filippo and Ramasco, José J. and Fortunato, Santo},
	year = {2011},
}

@inproceedings{chancellor_thyghgapp:_2016,
	title = {\#thyghgapp: {Instagram} {Content} {Moderation} and {Lexical} {Variation} in {Pro}-{Eating} {Disorder} {Communities}},
	isbn = {978-1-4503-3592-8},
	doi = {10.1145/2818048.2819963},
	abstract = {Pro-eating disorder (pro-ED) communities on social media encourage the adoption and maintenance of disordered eat-ing habits as acceptable alternative lifestyles rather than threats to health. In particular, the social networking site Instagram has reacted by banning searches on several pro-ED tags and issuing content advisories on others. We pre-sent the first large-scale quantitative study investigating pro-ED communities on Instagram in the aftermath of mod-eration – our dataset contains 2.5M posts between 2011 and 2014. We find that the pro-ED community has adopted non-standard lexical variations of moderated tags to circumvent these restrictions. In fact, increasingly complex lexical vari-ants have emerged over time. Communities that use lexical variants show increased participation and support of pro-ED (15-30\%). Finally, the tags associated with content on these variants express more toxic, self-harm, and vulnerable content. Despite Instagram's moderation strategies, pro-ED communities are active and thriving. We discuss the effec-tiveness of content moderation as an intervention for com-munities of deviant behavior.},
	author = {Chancellor, Stevie and Pater, Jessica and Clear, Trustin and Gilbert, Eric and De Choudhury, Munmun},
	year = {2016},
	pages = {1199--1211},
}

@article{guntuku_studying_2019,
	title = {Studying {Cultural} {Differences} in {Emoji} {Usage} across the {East} and the {West}},
	number = {ICWSM},
	author = {Guntuku, Sharath Chandra and Li, Mingyang and Tay, Louis and Ungar, Lyle H},
	year = {2019},
}

@article{berry_chill_2019,
	title = {A chill intro to causal inference via propensity scores},
	author = {Berry, George},
	year = {2019},
	pages = {1--16},
}

@article{kowalczyk_scalable_1988,
	title = {Scalable {Privacy}-{Compliant} {Virality} {Prediction} on {Twitter}},
	number = {Cohen},
	author = {Kowalczyk, Damian Konrad and Larsen, Jan},
	year = {1988},
	note = {arXiv: 1812.06034v2},
}

@article{lipton_mythos_2016,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	number = {Whi},
	author = {Lipton, Zachary C.},
	year = {2016},
	note = {arXiv: 1606.03490},
}

@article{nguyen_language_2011,
	title = {Language use as a reflection of socialization in online communities},
	url = {http://dl.acm.org/citation.cfm?id=2021119},
	abstract = {In this paper we investigate the connection between language and community membership of long time community participants through computational modeling techniques. We report on findings from an analysis of language usage within a popular online discussion forum with participation of thousands of users spanning multiple years. We find community norms of long time participants that are characterized by forum specific jargon and a style that is highly informal and shows familiarity with specific other participants and high emotional involvement in the discussion. We also find quantitative evidence of persistent shifts in language usage towards these norms across users over the course of the first year of community participation. Our observed patterns suggests language stabilization after 8 or 9 months of participation.},
	number = {June},
	journal = {Proceedings of the Workshop on Languages in …},
	author = {Nguyen, Dong and Rosé, Carolyn P.},
	year = {2011},
	note = {ISBN: 9781932432961},
	pages = {76--85},
}

@article{tran_characterizing_2016,
	title = {Characterizing the {Language} of {Online} {Communities} and},
	journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)},
	author = {Tran, Trang and Ostendorf, Mari},
	year = {2016},
	note = {arXiv: 1609.04779v1},
	pages = {1030--1035},
}

@phdthesis{Ruhl2016,
	title = {Welcome to my {Twisted} {Thesis} : {An} {Analysis} of {Orthographic} {Conventions} on {Tumblr} {Pragmatics} of the {Keyboard} : {An} {Analysis} of {Orthographic} {Conventions} on {Tumblr}},
	url = {https://www.researchgate.net/publication/315828929_Welcome_to_my_Twisted_Thesis_An_Analysis_of_Orthographic_Conventions_on_Tumblr?enrichId=rgreq-cfc747055650951d3a0b6dec7f6b970a-XXX&enrichSource=Y292ZXJQYWdlOzMxNTgyODkyOTtBUzo0ODA5ODQ2MTQzNDY3NTJAMTQ5MTY4},
	school = {San Francisco State University},
	author = {Ruhl, Molly},
	year = {2016},
	doi = {10.13140/RG.2.2.15033.77929},
	note = {Issue: May},
}

@inproceedings{Rashkin2018,
	title = {Modeling {Naive} {Psychology} of {Characters} in {Simple} {Commonsense} {Stories}},
	url = {http://arxiv.org/abs/1805.06533},
	abstract = {Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people's mental states - a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Rashkin, Hannah and Bosselut, Antoine and Sap, Maarten and Knight, Kevin and Choi, Yejin},
	year = {2018},
	note = {arXiv: 1805.06533},
	pages = {2289--2299},
}

@inproceedings{Peters2018,
	title = {Deep contextualized word representations},
	isbn = {978-1-941643-32-7},
	url = {http://www.aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	booktitle = {Proceedings of {NAACL}-{HLT} 2018},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	pmid = {299497},
	note = {arXiv: 1802.05365
ISSN: 9781941643327},
	pages = {2227--2237},
}

@article{Sim2016,
	title = {Friends with {Motives} : {Using} {Text} to {Infer} {Influence} on {SCOTUS}},
	abstract = {We present a probabilistic model of the in-fluence of language on the behavior of the U.S. Supreme Court, specifically influence of amicus briefs on Court decisions and opin-ions. The approach assumes that amici are rational, utility-maximizing agents who try to win votes or affect the language of court opin-ions. Our model leads to improved predictions of justices' votes and perplexity of opinion language. It is amenable to inspection, allow-ing us to explore inferences about the persua-siveness of different amici and influenceability of different justices; these are consistent with earlier findings. " Language is the central tool of our trade.},
	journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)},
	author = {Sim, Yanchuan and Routledge, Bryan R and Smith, Noah A},
	year = {2016},
	pages = {1724--1733},
}

@article{Visser2018,
	title = {Intertextual {Correspondence} for {Integrating} {Corpora}},
	abstract = {We present intertextual correspondence (ITC) as an integrative technique for combining annotated text corpora. The topical correspondence between different texts can be exploited to establish new annotation connections between existing corpora. Although the general idea should not be restricted to one particular theoretical framework, we explain how the annotation of intertextual correspondence works for two corpora annotated with argumentative notions on the basis of Inference Anchoring Theory. The annotated corpora we take as examples are topically and temporally related: the first corpus comprises television debates leading up to the 2016 presidential elections in the United States, the second corpus consists of commentary on and discussion of those debates on the social media platform Reddit. The integrative combination enriches the existing corpora in terms of the argumentative density, conceived of as the number of inference, conflict and rephrase relations relative to the word count of the (sub-)corpus. ITC also affects the global properties of the corpus, such as the most divisive issue. Moreover, the ability to extend existing corpora whilst maintaining the level of internal cohesion is beneficial to the use of the integrated corpus as resource for text and argument mining based on machine learning.},
	journal = {Conference on Language Resources and Evaluation (LREC)},
	author = {Visser, Jacky and Duthie, Rory and Lawrence, John and Reed, Chris},
	year = {2018},
	keywords = {Reddit, US presidential elections, argument, corpus, debate, dialogue, intertextuality},
	pages = {3511--3517},
}

@article{Mayfield2013,
	title = {Recognizing {Rare} {Social} {Phenomena} in {Conversation}: {Empowerment} {Detection} in {Support} {Group} {Chatrooms}},
	url = {http://www.aclweb.org/anthology/P13-1011},
	abstract = {Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. Important behavioral categories, though, are often sparse and often appear only in specific subsections of a conversation. This makes supervised machine learning difficult, through a combination of noisy features and unbalanced class distributions. We propose within-instance content selection, using cue features to selectively suppress sections of text and biasing the remaining representation towards minority classes. We show the effectiveness of this technique in automated annotation of empowerment language in online support group chatrooms. Our technique is significantly more accurate than multiple baselines, especially when prioritizing high precision.},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
	author = {Mayfield, Elijah and Adamson, David and Rosé, Carolyn Penstein},
	year = {2013},
	note = {ISBN: 9781937284503},
	pages = {104--113},
}

@article{bender_achieving_2011,
	title = {On {Achieving} and {Evaluating} {Language}-{Independence} in {NLP}},
	volume = {6},
	url = {http://elanguage.net/journals/lilt/article/view/2624},
	abstract = {Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability. In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.},
	number = {3},
	journal = {Linguistic Issues in Language Technology},
	author = {Bender, Emily M},
	year = {2011},
	pages = {1--28},
}

@article{Tschuggnall2017,
	title = {Overview of the author identification task at {PAN}-2017: {Style} breach detection and author clustering},
	volume = {1866},
	issn = {16130073},
	abstract = {Several authorship analysis tasks require the decomposition of a multi-authored text into its authorial components. In this regard two basic prerequisites need to be addressed: (1) style breach detection, i.e., the segmenting of a text into stylistically homogeneous parts, and (2) author clustering, i.e., the grouping of paragraph-length texts by authorship. In the current edition of PAN we focus on these two unsupervised authorship analysis tasks and provide both benchmark data and an evaluation framework to compare different approaches. We received three submissions for the style breach detection task and six submissions for the author clustering task; we analyze the submissions with different baselines while highlighting their strengths and weaknesses.},
	journal = {CEUR Workshop Proceedings},
	author = {Tschuggnall, Michael and Stamatatos, Efstathios and Verhoeven, Ben and Daelemans, Walter and Specht, Günther and Stein, Benno and Potthast, Martin},
	year = {2017},
}

@inproceedings{Silva2016,
	title = {Analyzing the {Targets} of {Hate} in {Online} {Social} {Media}},
	isbn = {978-1-57735-758-2},
	url = {http://arxiv.org/abs/1603.07709},
	abstract = {Social media systems allow Internet users a congenial platform to freely express their thoughts and opinions. Although this property represents incredible and unique communication opportunities, it also brings along important challenges. Online hate speech is an archetypal example of such challenges. Despite its magnitude and scale, there is a significant gap in understanding the nature of hate speech on social media. In this paper, we provide the first of a kind systematic large scale measurement study of the main targets of hate speech in online social media. To do that, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both these systems. Our results identify online hate speech forms and offer a broader understanding of the phenomenon, providing directions for prevention and detection approaches.},
	booktitle = {Proceedings of the {Tenth} {International} {AAAI} {Conference} on {Web} and {Social} {Media} ({ICWSM} 2016)},
	author = {Silva, Leandro and Mondal, Mainack and Correa, Denzil and Benevenuto, Fabricio and Weber, Ingmar},
	year = {2016},
	note = {arXiv: 1603.07709},
	keywords = {Poster Papers},
	pages = {687--690},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {10959203},
	doi = {10.1126/science.aal4230},
	abstract = {Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the \{{\textbackslash}em status quo\} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.},
	number = {6334},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	year = {2017},
	pmid = {28408601},
	note = {arXiv: 1608.07187},
	keywords = {results even with, we replicated the iat},
	pages = {183--186},
}

@article{kim_interpretability_2017,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	url = {http://arxiv.org/abs/1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	year = {2017},
	note = {arXiv: 1711.11279},
}

@article{cai_human-centered_2019,
	title = {Human-{Centered} {Tools} for {Coping} with {Imperfect} {Algorithms} during {Medical} {Decision}-{Making}},
	url = {http://arxiv.org/abs/1902.02960},
	abstract = {Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.},
	number = {45},
	author = {Cai, Carrie J. and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S. and Stumpe, Martin C. and Terry, Michael},
	year = {2019},
	note = {arXiv: 1902.02960},
	keywords = {all of this work, clinical health, copies are, copies of part or, digital or har d, fee provi ded that, for, human -ai int eraction, is grant ed without, machin e learnin g, permission to mak e, personal or classroom use},
}

@article{navarretta_semantic_1999,
	title = {Semantic {Clustering} of {Adjectives} and {Verbs} {Based} on {Syntactic} {Patterns}},
	number = {1993},
	author = {Navarretta, Costanza},
	year = {1999},
	pages = {124--132},
}

@article{pavlick_language_2014,
	title = {The {Language} {Demographics} of {Amazon} {Mechanical} {Turk}},
	volume = {2},
	doi = {10.1162/tacl_a_00167},
	abstract = {We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers’ selfreported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Pavlick, Ellie and Post, Matt and Irvine, Ann and Kachaev, Dmitry and Callison-Burch, Chris},
	year = {2014},
	pages = {79--92},
}

@article{sun_how_2019,
	title = {How {Do} {Distance} {Learners} {Connect} ? {Shared} {Identity} , {Focused} {Work} and {Future} {Possibilities}},
	doi = {10.1145/3290605.3300662},
	number = {January},
	author = {Sun, Na and Wang, Xiying and Rosson, Mary Beth},
	year = {2019},
	note = {ISBN: 9781450359702},
	keywords = {Connection strategies, Distance learning, Online learning community, Shared identity, acm reference format, connection, distance learning, online learning community, shared identity, strategies},
	pages = {1--12},
}

@inproceedings{joseph_exploring_2016,
	title = {Exploring {Patterns} of {Identity} {Usage} in {Tweets}},
	isbn = {978-1-4503-4143-1},
	url = {https://dl.acm.org/citation.cfm?id=2883027},
	doi = {10.1145/2872427.2883027},
	abstract = {Sociologists have long been interested in the ways that iden-tities, or labels for people, are created, used and applied across various social contexts. The present work makes two contributions to the study of identity, in particular the study of identity in text. We first consider the following novel NLP task: given a set of text data (here, from Twitter), label each word in the text as being representative of a (possi-bly multi-word) identity. To address this task, we develop a comprehensive feature set that leverages several avenues of recent NLP work on Twitter and use these features to train a supervised classifier. Our model outperforms a sur-prisingly strong rule-based baseline by 33\%. We then use our model for a case study, applying it to a large corpora of Twitter data from users who actively discussed the Eric Garner and Michael Brown cases. Among other findings, we observe that the identities used by individuals differ in interesting ways based on social context measures derived from census data.},
	booktitle = {{WWW} '16},
	author = {Joseph, Kenneth and Wei, Wei and Carley, Kathleen M.},
	year = {2016},
	pages = {401--412},
}

@article{da_computational_nodate,
	title = {The {Computational} {Case} against {Computational} {Literary} {Studies}},
	volume = {45},
	author = {Da, Nan Z},
}

@article{imtiaz_investigating_nodate,
	title = {Investigating the {Effects} of {Gender} {Bias} on {GitHub}},
	author = {Imtiaz, Nasif and Middleton, Justin and Chakraborty, Joymallya and Robson, Neill and Bai, Gina and Murphy-hill, Emerson},
}

@article{wang_relevant_2018,
	title = {Relevant {Document} {Discovery} for {Fact}-{Checking} {Articles}},
	doi = {10.1145/3184558.3188723},
	abstract = {With the support of major search platforms such as Google and Bing, fact-checking articles, which can be identified by their adoption of the schema.org ClaimReview structured markup, have gained widespread recognition for their role in the fight against digital misinformation. A claim-relevant document is an online document that addresses, and potentially expresses a stance towards, some claim. The claim-relevance discovery problem, then, is to find claim-relevant documents. Depending on the verdict from the fact check, claim-relevance discovery can help identify online misinformation. In this paper, we provide an initial approach to the claim-relevance discovery problem by leveraging various information retrieval and machine learning techniques. The system consists of three phases. First, we retrieve candidate documents based on various features in the fact-checking article. Second, we apply a relevance classi-fier to filter away documents that do not address the claim. Third, we apply a language feature based classifier to distinguish docu-ments with different stances towards the claim. We experimentally demonstrate that our solution achieves solid results on a large-scale dataset and beats state-of-the-art baselines. Finally, we highlight a rich set of case studies to demonstrate the myriad of remaining challenges and that this problem is far from being solved.},
	author = {Wang, Xuezhi and Yu, Cong and Baumgartner, Simon and Korn, Flip},
	year = {2018},
	note = {ISBN: 9781450356404},
	keywords = {Claim-Relevance Discovery, Digital Misinformation, Fact Checking, claim-relevance discovery, digital misinformation, fact checking},
	pages = {525--533},
}

@article{procter_supporting_2017,
	title = {Supporting the {Use} of {User} {Generated} {Content} in {Journalistic} {Practice}},
	doi = {10.1145/3025453.3025892},
	abstract = {Social media and user-generated content (UGC) are increasingly important features of journalistic work in a number of different ways. However, their use presents major challenges, not least because information posted on social media is not always reliable and therefore its veracity needs to be checked before it can be considered as fit for use in the reporting of news. We report on the results of a series of in-depth ethnographic studies of journalist work practices undertaken as part of the requirements gathering for a prototype of a social media verification 'dashboard' and its subsequent evaluation. We conclude with some reflections upon the broader implications of our findings for the design of tools to support journalistic work.},
	author = {Procter, Rob and Zubiaga, Arkaitz and Tolmie, Peter and Liakata, Maria and Randall, David William and Rouncefield, Mark and Wong Sak Hoi, Geraldine and Burger, Christian},
	year = {2017},
	note = {ISBN: 9781450346559},
	pages = {3632--3644},
}

@inproceedings{joseph_relating_2016,
	title = {Relating semantic similarity and semantic association to how humans label other people},
	doi = {10.18653/v1/w16-5601},
	abstract = {Computational linguists have long relied on a distinction between semantic similarity and semantic association to explain and evaluate what is being learned by NLP models. In the present work, we take these same concepts and explore how they apply to an entirely different question-how individuals label other people. Leveraging survey data made public by NLP researchers, we develop our own survey to connect semantic similarity and semantic association to the process by which humans label other people. The result is a set of insights applicable to how we think of semantic similarity as NLP researchers and a new way of leveraging NLP models of semantic similarity and association as researchers of social science.},
	author = {Joseph, Kenneth and Carley, Kathleen M.},
	year = {2016},
	pages = {1--10},
}

@article{hovy_social_2016,
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	issn = {0271-6798},
	doi = {10.18653/v1/p16-2096},
	abstract = {Medical sciences have long since estab-lished an ethics code for experiments, to minimize the risk of harm to subjects. Nat-ural language processing (NLP) used to involve mostly anonymous corpora, with the goal of enriching linguistic analysis, and was therefore unlikely to raise ethi-cal concerns. As NLP becomes increas-ingly wide-spread and uses more data from social media, however, the situation has changed: the outcome of NLP experi-ments and applications can now have a di-rect effect on individual users' lives. Until now, the discourse on this topic in the field has not followed the technological devel-opment, while public discourse was often focused on exaggerated dangers. This po-sition paper tries to take back the initiative and start a discussion. We identify a num-ber of social implications of NLP and dis-cuss their ethical significance, as well as ways to address them.},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	year = {2016},
	pmid = {20104160},
	note = {ISBN: 0271-6798},
	pages = {591--598},
}

@article{lipton_does_2017,
	title = {Does mitigating {ML}'s impact disparity require treatment disparity?},
	issn = {10966323},
	url = {http://arxiv.org/abs/1711.07076},
	abstract = {Following related work in law and policy, two notions of disparity have come to shape the study of fairness in algorithmic decision-making. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups, even if the correlation arises unintentionally. Naturally, we can achieve impact parity through purposeful treatment disparity. In one thread of technical work, papers aim to reconcile the two forms of parity proposing disparate learning processes (DLPs). Here, the learning algorithm can see group membership during training but produce a classifier that is group-blind at test time. In this paper, we show theoretically that: (i) When other features correlate to group membership, DLPs will (indirectly) implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) When group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) In general, DLPs provide a suboptimal trade-off between accuracy and impact parity. Based on our technical analysis, we argue that transparent treatment disparity is preferable to occluded methods for achieving impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs vs. per-group thresholds.},
	number = {ML},
	author = {Lipton, Zachary C. and Chouldechova, Alexandra and McAuley, Julian},
	year = {2017},
	note = {arXiv: 1711.07076},
	pages = {1--11},
}

@article{Lin2018,
	title = {Mining {Cross}-{Cultural} {Differences} and {Similarities} in {Social} {Media}},
	url = {http://aclweb.org/anthology/P18-1066},
	number = {Section 5},
	journal = {Proceedings of ACL},
	author = {Lin, Bill Yuchen and Xu, Frank F and Zhu, Kenny and Hwang, Seung-won},
	year = {2018},
	pages = {709--719},
}

@inproceedings{An2018,
	title = {{SemAxis}: {A} {Lightweight} {Framework} to {Characterize} {Domain}-{Specific} {Word} {Semantics} {Beyond} {Sentiment}},
	url = {http://arxiv.org/abs/1806.05521},
	abstract = {Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SEMAXIS, a simple yet powerful framework to characterize word semantics using many semantic axes in word- vector spaces beyond sentiment. We demonstrate that SEMAXIS can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SEMAXIS outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {An, Jisun and Kwak, Haewoon and Ahn, Yong-Yeol},
	year = {2018},
	note = {arXiv: 1806.05521},
	pages = {2450--2461},
}

@article{nayan_named_2008,
	title = {Named {Entity} {Recognition} for {Indian} {Languages}},
	abstract = {This paper talks about a new approach to recognize named entities for Indian languages. Phonetic matching tech- nique is used to match the strings of differ- ent languages on the basis of their similar sounding property. We have tested our sys- tem with a comparable corpus of English and Hindi language data. This approach is language independent and requires only a set of rules appropriate for a language. 1},
	number = {January},
	author = {Nayan, Animesh and Ravi, B and Rao, Kiran and Singh, Pawandeep and Sanyal, Sudip and Sanyal, Ratna},
	year = {2008},
	pages = {97--104},
}

@article{bies_transliteration_2014,
	title = {Transliteration of {Arabizi} into {Arabic} {Orthography} : {Developing} a {Parallel} {Annotated} {Arabizi}-{Arabic} {Script} {SMS} / {Chat} {Corpus}},
	volume = {93},
	abstract = {Bies, Ann, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee, Jonathan Wright, Stephanie Strassel, Nizar Habash, Ramy Eskander, and Owen Rambow. "Transliteration of Arabizi into Arabic Orthography: Developing a Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus." ANLP 2014 (2014): 93.},
	number = {Ldc},
	journal = {Anlp 2014},
	author = {Bies, Ann and Song, Zhiyi and Maamouri, Mohamed and Grimes, Stephen and Lee, Haejoong and Wright, Jonathan and Strassel, Stephanie and Habash, Nizar and Eskander, Ramy and Rambow, Owen},
	year = {2014},
	pages = {93--103},
}

@article{allcott_welfare_2019,
	title = {The {Welfare} {Effects} of {Social} {Media}},
	url = {https://sites.google.com/site/allcott/research.},
	abstract = {The rise of social media has provoked both optimism about potential societal benefits and concern about harms such as addiction, depression, and political polarization. We present a randomized evaluation of the welfare effects of Facebook, focusing on US users in the run-up to the 2018 midterm election. We measured the willingness-to-accept of 2,844 Facebook users to deactivate their Facebook accounts for four weeks, then randomly assigned a subset to actually do so in a way that we verified. Using a suite of outcomes from both surveys and direct measurement, we show that Facebook deactivation (i) reduced online activity, including other social media, while increasing offline activities such as watching TV alone and socializing with family and friends; (ii) reduced both factual news knowledge and political polarization; (iii) increased subjective well-being; and (iv) caused a large persistent reduction in Facebook use after the experiment. We use participants' pre-experiment and post-experiment Facebook valuations to quantify the extent to which factors such as projection bias might cause people to overvalue Facebook, finding that the magnitude of any such biases is likely minor relative to the large consumer surplus that Facebook generates. JEL Codes: D12, D90, I31, L86, O33.},
	author = {Allcott, Hunt and Braghieri, Luca and Eichmeyer, Sarah and Gentzkow, Matthew and Baym, Nancy and Burke, Moira and Franco, Annie and Leavitt, Alex and Rogers, Todd},
	year = {2019},
	keywords = {Social media, consumer surplus, political polarization, projection bias, subjective well-being},
	pages = {1--114},
}

@article{ruder_survey_2017,
	title = {A {Survey} {Of} {Cross}-lingual {Word} {Embedding} {Models}},
	issn = {0964-6639},
	url = {http://arxiv.org/abs/1706.04902},
	doi = {10.1177/0964663912467814},
	abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
	author = {Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
	year = {2017},
	pmid = {877579},
	note = {arXiv: 1706.04902
ISBN: 0824701712},
	pages = {1--55},
}

@article{Bullingham2013,
	title = {'{The} presentation of self in the online world': {Goffman} and study of online identites},
	volume = {39},
	issn = {0165-5515},
	doi = {10.1177/016555150000000},
	abstract = {Dataspace systems constitute a recent data management approach that supports better cooperation among autonomous and heterogeneous data sources with which the user is initially unfamiliar. A central idea is to gradually increase the user's knowledge about the contents, structures, and semantics of the data sources in the dataspace. Without this knowledge, the user is not able to make sophisticated queries. The dataspace systems proposed so far are usually application-specific. The present paper introduces an application-independent dataspace system with versatile facilities based on XML data sources. Our XML dataspace system provides the user with a uniform visual interface, where various interactions can be combined in declarative way. This means, among others, that the user need not master programming techniques. Keywords},
	number = {1},
	journal = {Journal of Information Science},
	author = {Bullingham, Liam and Vasconcelos, Ana C.},
	year = {2013},
	pmid = {74193159},
	note = {ISBN: 0165551500000},
	keywords = {dataspaces, query languages, visual interfaces, xml},
	pages = {101--112},
}

@article{Hogan2010,
	title = {The {Presentation} of {Self} in the {Age} of {Social} {Media}: {Distinguishing} {Performances} and {Exhibitions} {Online}},
	volume = {30},
	issn = {0270-4676},
	url = {http://journals.sagepub.com/doi/10.1177/0270467610385893},
	doi = {10.1177/0270467610385893},
	abstract = {Presentation of self (via Goffman) is becoming increasingly popular as a means for explaining differences in meaning and activity of online participation. This article argues that self-presentation can be split into performances, which take place in synchronous “situations,” and artifacts, which take place in asynchronous “exhibitions.” Goffman’s dramaturgical approach (including the notions of front and back stage) focuses on situations. Social media, on the other hand, frequently employs exhibitions, such as lists of status updates and sets of photos, alongside situational activities, such as chatting. A key difference in exhibitions is the virtual “curator” that manages and redistributes this digital content. This article introduces the exhibitional approach and the curator and suggests ways in which this approach can extend present work concerning online presentation of self. It introduces a theory of “lowest common denominator” culture employing the exhibitional approach.},
	number = {6},
	journal = {Bulletin of Science, Technology \& Society},
	author = {Hogan, Bernie},
	year = {2010},
	pmid = {11331521},
	note = {arXiv: 1011.1669v3
ISBN: 0270467610},
	keywords = {all the world, and all the men, and one, and their entrances, and women merely, exhibition, goffman, online identity, performance, players, privacy, s a stage, symbolic interactionism, they have their exits},
	pages = {377--386},
}

@book{Goffman1959,
	address = {Oxford, England},
	title = {The {Presentation} of {Self} in {Everyday} {Life}},
	publisher = {Doubleday},
	author = {Goffman, Erving},
	year = {1959},
}

@incollection{Johnstone2010,
	title = {Locating {Language} in {Identity}},
	booktitle = {Language and {Identities}},
	publisher = {Edinburgh University Press},
	author = {Johnstone, Barbara},
	year = {2010},
	keywords = {★},
}

@article{Waugh1982,
	title = {Marked and unmarked: {A} choice between unequals in semiotic structure},
	volume = {38},
	number = {3-4},
	journal = {Semiotica},
	author = {Waugh, Linda R.},
	year = {1982},
	note = {ISBN: 1111111111},
	pages = {299--318},
}

@article{Chen2013,
	title = {Joint {Link} {Prediction} and {Attribute} {Inference} using a {Social}-{Attribute} {Network} {NEIL}},
	volume = {0},
	issn = {03600300},
	url = {http://arxiv.org/abs/1710.03346},
	doi = {10.1145/0000000.0000000},
	abstract = {Natural language place descriptions in everyday communication provide a rich source of spatial knowledge about places. An important step to utilize such knowledge in information systems is geo-referencing all the places referred to in these descriptions. Current techniques for geo-referencing places from text documents are using place name recognition and disambiguation; however, place descriptions often contain place references that are not known by gazetteers, or that are expressed in other, more flexible ways. Hence, the approach for geo-referencing presented in this paper starts from a place graph that contains the place references as well as spatial relationships extracted from place descriptions. Spatial relationships are used to constrain the locations of places and allow the later best-matching process for geo-referencing. The novel geo-referencing process results in higher precision and recall compared to state-of-art toponym resolution approaches on several tested place description datasets.},
	number = {0},
	journal = {ACM Transactions on Intelligent Systems and Technology,},
	author = {Chen, Hao and Vasardani, Maria and Winter, Stephan},
	year = {2013},
	pmid = {1000285845},
	note = {arXiv: 1710.03346
ISBN: 9781627480031},
	pages = {1--20},
}

@book{Gee2011,
	address = {New York},
	title = {An {Introduction} to {Discourse} {Analysis}: {Theory} and {Method}},
	publisher = {Routledge},
	author = {Gee, James Paul},
	year = {2011},
	note = {Publication Title: Introduction to Discourse Analysis},
}

@article{Vosoughi2018,
	title = {The spread of true and false news online},
	volume = {1151},
	number = {March},
	journal = {Science},
	author = {Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
	year = {2018},
	pages = {1146--1151},
}

@article{sim_discovering_2012,
	title = {Discovering factions in the computational linguistics community},
	url = {http://dl.acm.org/citation.cfm?id=2390511},
	abstract = {We present a joint probabilistic model of who cites whom in computational linguistics, and also of the words they use to do the citing. The model reveals latent factions, or groups of individuals whom we expect to collaborate more closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without. We conduct an exploratory data analysis on the ACL Anthology. We extend the model to reveal changes in some authors’ faction memberships over time.},
	number = {July},
	journal = {Proceedings of the ACL-2012 Special …},
	author = {Sim, Yanchuan and Smith, NA and Smith, DA},
	year = {2012},
	pages = {22--32},
}

@article{tang_learning_2014,
	title = {Learning {Sentiment}-{Specific} {Word} {Embedding}},
	issn = {03029743},
	url = {http://www.aclweb.org/anthology/P14-1146},
	doi = {10.3115/1220575.1220648},
	abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
	journal = {Acl},
	author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
	year = {2014},
	pmid = {18487783},
	note = {arXiv: 1011.1669v3
ISBN: 9781937284725},
	pages = {1555--1565},
}

@article{jurafsky_linguistic_2016,
	title = {Linguistic {Markers} of {Status} in {Food} {Culture}: {Bourdieu}’s {Distinction} in a {Menu} {Corpus}},
	issn = {23714549},
	url = {http://culturalanalytics.org/2016/10/linguistic-markers-of-status-in-food-culture-bourdieus-distinction-in-a-menu-corpus/},
	doi = {10.22148/16.007},
	journal = {Journal of Cultural Analytics},
	author = {Jurafsky, Dan and Chahuneau, Victor and Routledge, Bryan R. and Smith, Noah A.},
	year = {2016},
	pages = {1--24},
}

@article{thebault-spieker_simulation_2017,
	title = {Simulation {Experiments} on (the {Absence} of) {Ratings} {Bias} in {Reputation} {Systems}},
	volume = {1},
	issn = {25730142},
	url = {http://dl.acm.org/citation.cfm?doid=3171581.3134736},
	doi = {10.1145/3134736},
	abstract = {As the gig economy continues to grow and freelance work moves online, five-star reputation systems are becoming more and more common. At the same time, there are increasing accounts of race and gender bias in evaluations of gig workers, with negative impacts for those workers. We report on a series of four Mechanical Turk-based studies in which participants who rated simulated gig work did not show race-or gender bias, while manipulation checks showed they reliably distinguished between low-and high-quality work. Given prior research, this was a striking result. To explore further, we used a Bayesian approach to verify absence of ratings bias (as opposed to merely not detecting bias). This Bayesian test let us identify an upper-bound: if any bias did exist in our studies, it was below an average of 0.2 stars on a five-star scale. We discuss possible interpretations of our results and outline future work to better understand the results.},
	number = {CSCW},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Thebault-Spieker, Jacob and Kluver, Daniel and Klein, Maximilian A. and Halfaker, Aaron and Hecht, Brent and Terveen, Loren and Konstan, Joseph A.},
	year = {2017},
	note = {ISBN: 1542887720},
	keywords = {aaron halfaker, acm reference format, bayesian statistics, brent hecht, daniel kluver, gender bias, gig economy, jacob thebault-spieker, loren terveen and, maximillian klein, racial bias, reputation bias, reputation systems},
	pages = {1--25},
}

@article{zhao_gender_2018,
	title = {Gender {Bias} in {Coreference} {Resolution}: {Evaluation} and {Debiasing} {Methods}},
	url = {http://arxiv.org/abs/1804.06876},
	abstract = {We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2018},
	note = {arXiv: 1804.06876},
	pages = {15--20},
}

@article{busse_geek_2013,
	title = {Geek hierarchies, boundary policing, and the gendering of the good fan},
	volume = {10},
	abstract = {Even though mainstream cultural constructions of geeks (and, through it, fans) have been changing recently, they remain heavily gendered. I describe how fans internalize these concepts, and how gender and gendering of fannish activities continues to affect inter- and intra-fannish policing. What underlies much of this border policing is a clear sense of protecting one’s own sense of fan community and ascribing positive values to it while trying to exclude others. Fans replicate negative outsider notions of what constitutes fannishness, often using similar feminizing concepts. Accusations of being too attached, too obsessed, and too invested get thrown around readily, and all too often such affect is criticized for being too girly or like a teen. Particularly interesting here is the gender bias that not so subtly pervades much cultural conversation surrounding fan discourses and that is more often than not predicated on unruly sexualities and queer bodies, both of which get policed within and without fan spaces.},
	number = {1},
	journal = {Participations. Journal of Audience \& Reception Studies},
	author = {Busse, Kristina},
	year = {2013},
	keywords = {affect, border policing, fan representation, fanboy, fangirl, geek hierarchy},
	pages = {73--91},
}

@book{ehrlich_handbook_2014,
	title = {The {Handbook} of {Language}, {Gender}, and {Sexuality}},
	isbn = {3-08-234241-8},
	publisher = {Wiley-Blackwell},
	editor = {Ehrlich, Susan and Meyerhoff, Miriam and Holmes, Janet},
	year = {2014},
	pmid = {26840611},
	doi = {10.1002/ejoc.201200111},
	note = {arXiv: 1011.1669v3
ISSN: 0196-6553},
}

@inproceedings{phadke_framing_2018,
	title = {Framing {Hate} with {Hate} {Frames}: {Designing} the {Codebook}},
	isbn = {978-1-4503-6018-0},
	booktitle = {{CSCW}},
	author = {Phadke, Shruti and Lloyd, Jonathan and Hawdon, James and Samory, Mattia and Mitra, Tanushree},
	year = {2018},
}

@article{zhang_characterizing_2018,
	title = {Characterizing {Online} {Public} {Discussions} through {Patterns} of {Participant} {Interactions}},
	volume = {2},
	doi = {10.1145/3274467},
	number = {November},
	author = {Zhang, Justine and Danescu-Niculescu-Mizil, Cristian and Sauper, Christy and Taylor, Sean},
	year = {2018},
	keywords = {2018, Facebook, acm on human-, acm reference format, and sean j, characterizing, christina sauper, conversations, cristian danescu-niculescu-mizil, facebook, in proceedings of the, interaction patterns, justine zhang, online public discussions through, patterns of participant interactions, public discussions, taylor},
}

@article{Lothian2007,
	title = {Yearning void and infinite potential: {Online} slash fandom as queer female space},
	volume = {45},
	issn = {00138282},
	abstract = {To me, slash is about cracks and crevices in a text, a yearning void in both the text and the reader. So space is a vacuum—something that isn't there but could be. But in the larger terms of community, culture, politics, space is less about vacuum and more about potential.The slash space, to me, is remarkable in its fecundity. It is space that is never filled, potential that never runs out. No matter how many stories, how many writers, there's always more space. Slash as space, space as both yearning void and infinite potential. (Julad 2003) o date, work on women, queerness, and online communities has mainly focused on lesbian and queer-identified women's use of online space in the service of iden-tity and sexuality narratives played out in the physical world. 1 In this project, we expand the scope of such inquiries to include ways in which particular online spaces, cultures, and practices can queer women (and other gendered subjects) in ways not accounted for by most identity narratives. We are interested in the interactions between women which structure online media fandom, specifically the exchange of sexually explic-it slash stories which depict relationships between male characters and actors from films, books, and television shows. In the virtual spaces we invoke in this paper, such shared sex-ual fantasies bring people together from a wide array of identities and locations. Our expe-rience in slash fan communities on LiveJournal.com (LJ) suggests that participation in electronic social networks can induct us into new and unusual narratives of identity and sexuality, calling into question familiar identifications and assumptions. Slash fandom's dis-cursive sphere has been termed queer female space by some who inhabit and study it; we want to explore the function of this space in the lives of the people who occupy it, how it is structured, and what it can do. We have chosen not to pursue our exploration by producing an academic research essay which draws evidence from experts external to the community it discusses in order to con-struct an argument that will build to a final conclusion. Rather, we want to demonstrate the open-ended theorizing in which fan fiction writers and readers participate, bringing into a different sphere some conversations that continue to take place in spaces other than— though not always dissimilar to—the conferences and seminar rooms of academia. To do this, we created an online discussion space, which we used to invite some fellow fans to English Language Notes 45.2 Fall / Winter 2007 T English Language Notes 45.2 Fall / Winter 2007 104 address issues around sexual and gender identity, queer practices, and their politics and limitations, all questions which circulate widely in slash fandom. The authors of this essay participated in these discussions under fannish pseudonyms, but we have chosen not to identify which comments are ours. We instigated, facilitated, and collected discussions, and we edited parts of these conversations to present them as a linear text, which we repro-duce here. Some excerpts have been edited for length and clarity, but our collaborators were consulted at every stage, and the ideas expressed throughout the paper have taken shape from all our interactions. A longer edited version of the debate is available at http://slashroundtable.livejournal.com. We hope that this piece will open up important and interesting questions for both fan studies and queer studies. For example, can we consider slash fandom to be an anti-heteronormative space? Does it problematize the distinction between the normative and the deviant, the heteronormative and the queer? If slash fandom's discourses of queer-ness circulate outside queer theory's commonest academic and (sub)cultural locations, what contributions can they make to queer theoretical and political practices? Connectivity: Queerness in Fannish Space Cat: On LJ and in fandom, I've really found a home that integrates the various-ly queer or non-mainstream aspects of my personality in a way I never have in real life. For my friend J., fandom was the only way she could express her queerness and she had to keep that completely hidden from her family. But through fandom she met people who helped her become comfortable with that aspect of herself and then she met someone she was attracted to. It was during heady nights and days of complete immersion in cowriting a very, very smutty story that both our relationships really crossed over from the theoret-ical to the actual, the queerness of the writing and fannish interaction becom-ing manifest in our outside lives. For us, slash fandom has become a place where a young urban dyke shares erotic space},
	number = {2},
	journal = {English Language Notes},
	author = {Lothian, Alexis and Busse, Kristina and Reid, Robin Anne},
	year = {2007},
	pages = {103--111},
}

@inproceedings{Le2014,
	title = {Distributed {Representations} of {Sentences} and {Documents}},
	isbn = {978-1-63439-397-3},
	url = {http://arxiv.org/abs/1405.4053},
	doi = {10.1145/2740908.2742760},
	abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	author = {Le, Quoc V. and Mikolov, Tomas},
	year = {2014},
	pmid = {9377276},
	note = {arXiv: 1405.4053
ISSN: 10495258},
	pages = {1188--1196},
}

@inproceedings{hua_wikiconv_2018,
	title = {{WikiConv} : {A} {Corpus} of the {Complete} {Conversational} {History} of a {Large} {Online} {Collaborative} {Community}},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing},},
	author = {Hua, Yiqing and Danescu-Niculescu-Mizil, Cristian and Taraborelli, Dario and Thain, Nithum and Sorensen, Jeffrey and Dixon, Lucas},
	year = {2018},
	note = {arXiv: 1810.13181},
	pages = {2818--2823},
}

@article{peacock_streets_2018,
	title = {Streets for {People}: {Engaging} {Children} in {Placemaking} {Through} a {Socio}-technical {Process}},
	url = {http://doi.acm.org/10.1145/3173574.3173901},
	doi = {10.1145/3173574.3173901},
	abstract = {© 2018 Copyright is held by the owner/author(s). In this paper, we present a socio-technical process designed to engage children in an ongoing urban design project-Streets for People-in Newcastle, UK. We translated urban design proposals developed by residents and the local authority to enable children to contribute ideas to the project. Our process comprised three stages: situated explorations and evidence gathering through digitally supported neighbourhood walks; issue mapping and peer-to-peer discussions using an online engagement platform; and faceto-face dialogue between children, residents, and the local authority through a 'Town Hall' event. We report insights gained through our engagement and show how our activities facilitated issue advocacy and the development of children's capacities, but also surfaced tensions around the agency of children in political processes. We reflect on the challenges of working in this space, and discuss wider implications for technology design and ethical questions that 'scaling up' such work might pose.},
	journal = {In Proc. of CHI},
	author = {Peacock, Sean and Anderson, Robert and Crivellaro, Clara},
	year = {2018},
	note = {ISBN: 978-1-4503-5620-6},
	keywords = {children, community engagement, digital civics, urban design, urban planning},
	pages = {327:1--327:14},
}

@article{yang_investigating_2018,
	title = {Investigating {How} {Experienced} {UX} {Designers} {Effectively} {Work} with {Machine} {Learning}},
	url = {http://dl.acm.org/citation.cfm?doid=3196709.3196730},
	doi = {10.1145/3196709.3196730},
	abstract = {Machine learning (ML) plays an increasingly important role in improving a user's experience. However, most UX practitioners face challenges in understanding ML's capabilities or envisioning what it might be. We interviewed 13 designers who had many years of experience designing the UX of ML-enhanced products and services. We probed them to characterize their practices. They shared they do not view themselves as ML experts, nor do they think learning more about ML would make them better designers. Instead, our participants appeared to be the most successful when they engaged in ongoing collaboration with data scientists to help envision what to make and when they embraced a data-centric culture. We discuss the implications of these findings in terms of UX education and as opportunities for additional design research in support of UX designers working with ML.},
	number = {March},
	journal = {Proceedings of the 2018 on Designing Interactive Systems Conference 2018  - DIS '18},
	author = {Yang, Qian and Scuito, Alex and Zimmerman, John and Forlizzi, Jodi and Steinfeld, Aaron},
	year = {2018},
	note = {ISBN: 9781450351980},
	pages = {585--596},
}

@book{ehrlich2017handbook,
	title = {The handbook of language, gender, and sexuality},
	publisher = {John Wiley \& Sons},
	author = {Ehrlich, Susan and Meyerhoff, Miriam and Holmes, Janet},
	year = {2017},
}

@book{kerlinger_foundations_2000,
	address = {Forth Worth, TX},
	title = {Foundations of {Behavioral} {Research}},
	publisher = {Harcraft College Publishers},
	author = {Kerlinger, F and Lee, H},
	year = {2000},
}

@article{li_slacktivists_2018,
	title = {Slacktivists or {Activists}?: {Identity} {Work} in the {Virtual} {Disability} {March}},
	doi = {10.1145/3173574.3173799},
	journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI'18},
	author = {Li, Hanlin and Bora, Disha and Salvi, Sagar and Brady, Erin},
	year = {2018},
	note = {ISBN: 9781450356206},
	pages = {1--13},
}

@article{berry_how_2017,
	title = {How {Values} {Shape} {Collaboration} {Between} {Patients} with {Multiple} {Chronic} {Conditions} and {Spousal} {Caregivers}},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025923},
	doi = {10.1145/3025453.3025923},
	abstract = {Individuals with multiple chronic conditions (MCC) collaborate with spousal caregivers daily to pursue what is most important to their health and well-being. Previous research in human-computer interaction has supported individuals with chronic conditions or their caregivers, but little has supported both as a unit. We conducted a field study with 12 patient-caregiver dyads, all married and living together, to identify partners' values and how they shape collaborative management of MCC. Partners' coinciding values motivated them to empathize with and support each other in the face of challenges related to health and well-being. When their values were asymmetric, they perceived tensions between individual autonomy and their ability to coordinate with their partner. Systems to support partners in this context could help them overcome asymmetric values, but should balance this with support for individual autonomy.},
	journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems  - CHI '17},
	author = {Berry, Andrew B. L. and Lim, Catherine and Hartzler, Andrea L. and Hirsch, Tad and Wagner, Edward H. and Ludman, Evette and Ralston, James D.},
	year = {2017},
	note = {ISBN: 9781450346559},
	pages = {5257--5270},
}

@inproceedings{august_framing_2018,
	title = {Framing {Effect} : {Choice} of {Slogans} {Used} to {Advertise} {Online} {Experiments} {Can} {Boost} {Recruitment} and {Lead} to {Sample} {Biases}},
	volume = {2},
	booktitle = {Proc. {ACM} {Hum}.-{Comput}. {Interact}., {CSCW}},
	author = {August, T A L and Oliveira, Nigini and Tan, Chenhao and Smith, Noah A and Reinecke, Katharina},
	year = {2018},
	note = {Issue: November},
}

@article{hamidi_gender_2018,
	title = {Gender {Recognition} or {Gender} {Reductionism}?},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173582},
	doi = {10.1145/3173574.3173582},
	number = {April},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Hamidi, Foad and Scheuerman, Morgan Klaus and Branham, Stacy M.},
	year = {2018},
	note = {ISBN: 9781450356206},
	pages = {1--13},
}

@inproceedings{Maki2017,
	title = {Roles and {Success} in {Wikipedia} {Talk} {Pages}: {Identifying} {Latent} {Patterns} of {Behavior}},
	copyright = {All rights reserved},
	url = {http://www.aclweb.org/anthology/I17-1103},
	abstract = {In this work we investigate how role-based behavior profiles of a Wikipedia editor, considered against the backdrop of roles taken up by other editors in discussions, predict the success of the editor at achiev-ing an impact on the associated article. We first contribute a new public dataset in-cluding a task predicting the success of Wikipedia editors involved in discussion, measured by an operationalization of the lasting impact of their edits in the article. We then propose a probabilistic graphical model that advances earlier work induc-ing latent discussion roles using the light supervision of success in the negotiation task. We evaluate the performance of the model and interpret findings of roles and group configurations that lead to certain outcomes on Wikipedia.},
	booktitle = {Proc. of the 8th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Maki, Keith and Yoder, Michael Miller and Jo, Yohan and Rosé, Carolyn Penstein},
	year = {2017},
	pages = {1026--1035},
}

@inproceedings{Yoder2017,
	title = {Code-{Switching} as a {Social} {Act}: {The} {Case} of {Arabic} {Wikipedia} {Talk} {Pages}},
	copyright = {All rights reserved},
	url = {http://aclweb.org/anthology/W17-2911},
	abstract = {Code-switching has been found to have social motivations in addition to syntactic constraints. In this work, we explore the social effect of code-switching in an on-line community. We present a task from the Arabic Wikipedia to capture language choice, in this case code-switching be-tween Arabic and other languages, as a predictor of social influence in collabora-tive editing. We find that code-switching is positively associated with Wikipedia edi-tor success, particularly borrowing techni-cal language on pages with topics less di-rectly related to Arabic-speaking regions.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science}},
	author = {Yoder, Michael Miller and Rijhwani, Shruti and Rosé, Carolyn Penstein and Levin, Lori},
	year = {2017},
	pages = {73--82},
}

@incollection{Grice1975,
	address = {New York},
	title = {Logic and conversation},
	booktitle = {Syntax and {Semantics}: {Speech} {Acts}},
	publisher = {Academic Press},
	author = {Grice, H. Paul},
	editor = {Cole, Peter and Morgan, Jerry L.},
	year = {1975},
	pages = {41--58},
}

@article{conneau_what_2018,
	title = {What you can cram into a single vector: {Probing} sentence embeddings for linguistic properties},
	url = {http://arxiv.org/abs/1805.01070},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	year = {2018},
	note = {arXiv: 1805.01070},
	pages = {2126--2136},
}

@inproceedings{brekhus1996social,
	title = {Social marking and the mental coloring of identity: {Sexual} identity construction and maintenance in the {United} {States}},
	volume = {11},
	booktitle = {Sociological {Forum}},
	publisher = {Springer},
	author = {Brekhus, Wayne},
	year = {1996},
	note = {Issue: 3},
	pages = {497--522},
}

@article{jang_virality_nodate,
	title = {On the {Virality} of {Animated} {GIFs} on {Tumblr}},
	author = {Jang, Yunseok and Song, Yale and Kim, Gunhee},
}

@article{brekhus1998sociology,
	title = {A sociology of the unmarked: {Redirecting} our focus},
	volume = {16},
	number = {1},
	journal = {Sociological Theory},
	author = {Brekhus, Wayne},
	year = {1998},
	note = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {34--51},
}

@incollection{Nakayama1999,
	title = {Whiteness as strategic rhetoric},
	volume = {81},
	booktitle = {Whiteness: {The} communication of social identity},
	author = {Nakayama, T. K. and Krizek, R. L.},
	year = {1999},
	pages = {87--106},
}

@article{bucholtz_whiteness_1999,
	title = {The {Whiteness} of {Nerds} :},
	volume = {11},
	issn = {1055-1360},
	doi = {10.1525/jlin.2001.11.1.84},
	abstract = {Anthropological research has shown that identities that are "not white enough" may be racially marked. Yet marking may also be the result of being "too white." California high school students who embrace one such white identity, nerds, employ a superstandard language variety to reject the youth culture norm of coolness. These practices also ideologically position nerds as hyperwhite by distancing them from the African American underpinnings of European American youth culture},
	number = {1},
	journal = {Journal of Linguistic Anthopology},
	author = {Bucholtz, Mary},
	year = {1999},
	note = {ISBN: 1548-1395},
	pages = {84--100},
}

@article{trechter2001introduction,
	title = {White {Noise}: {Bringing} {Language} into {Whiteness} {Studies}},
	volume = {11},
	issn = {10551360},
	doi = {10.1525/jlin.2001.11.1.3},
	abstract = {An intorduction to a special issue of the Journal of Linguistic Antrhopology that "extends the contribution of anthroplogy to whiteness studies...by introducing the methods and theories of linguistic anthropology. In recognition of the fact that, as a social construction, race is a linguistic construction as well, the contributors to this issue make whiteness not only visible but also audible by calling attention to the production of whtie identities and ideologies in discourse" (4).},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {Bucholtz, Mary and Trechter, Sara},
	year = {2001},
	note = {Publisher: JSTOR},
	pages = {3--21},
}

@book{lakoff_metaphors_1980,
	address = {Chicago},
	title = {Metaphors we live by},
	isbn = {0-226-46800-3},
	publisher = {The University of Chicago Press},
	author = {Lakoff, George and Johnson, Mark},
	year = {1980},
}

@article{girouard_comparative_2014,
	title = {Comparative {Stylistic} {Fanfiction} {Analysis} : {Popular} and {Unpopular} {Fics} across {Eleven} {Fandoms}},
	number = {July},
	author = {Girouard, Vanessa and Rubin, Victoria L},
	year = {2014},
}

@article{livingston_how_2014,
	title = {How players value their characters in world of warcraft},
	url = {http://dl.acm.org/citation.cfm?doid=2531602.2531661},
	doi = {10.1145/2531602.2531661},
	abstract = {Characters in games such as World of Warcraft allow players to act in the game world and to interact with others. Game characters and avatars are a mediated form of self-representation for the player, but some research suggests that players also view characters in other ways that have to do with the kinds of value that the characters provide. To better understand the ways that players value their characters in an online environment, we carried out a semi-structured interview study of twenty World of Warcraft players. From our data we identify ten kinds of value that characters can provide -- including utility, investment, communication, memory, enjoyment, and representations of relationships, as well as value as an opportunity for experience, creativity, sociability, and self-expression. The analytical lens of value provides a new understanding of the ways that players appreciate characters in online multi-user worlds. Our results can help developers understand and enhance an element of multi-player games that contributes greatly to player experience and satisfaction.},
	journal = {Proceedings of the 17th ACM conference on Computer supported cooperative work \& social computing - CSCW '14},
	author = {Livingston, Ian J. and Gutwin, Carl and Mandryk, Regan L. and Birk, Max},
	year = {2014},
	note = {ISBN: 9781450325400},
	pages = {1333--1343},
}

@article{yee_introverted_2011,
	title = {Introverted elves \& conscientious gnomes},
	issn = {9781450302289},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979052},
	doi = {10.1145/1978942.1979052},
	abstract = {Personality inference can be used for dynamic personalization of content or system customization. In this study, we examined whether and how personality is expressed in Virtual Worlds (VWs). Survey data from 1,040 World of Warcraft players containing demographic and personality variables was paired with their VW behavioral metrics over a four-month period. Many behavioral cues in VWs were found to be related to personality. For example, Extraverts prefer group activities over solo activities. We also found that these behavioral indicators can be used to infer a player's personality.},
	journal = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
	author = {Yee, Nick and Ducheneaut, Nicolas and Nelson, Les and Likarish, Peter},
	year = {2011},
	note = {ISBN: 9781450302289},
	pages = {753},
}

@book{myers-scotton_social_1993,
	title = {Social {Motivations} for {Code} {Switching}: {Evidence} from {Africa}},
	publisher = {Clarendon Press},
	author = {Myers-Scotton, Carol},
	year = {1993},
}

@article{mohammad_using_2009,
	title = {Using citations to generate surveys of scientific paradigms},
	url = {papers2://publication/uuid/4AA20573-4B6C-46CD-BE01-7D1A051208D3},
	doi = {10.3115/1620754.1620839},
	abstract = {The number of research publications in var- ious disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of tech- nical material. In this paper we present the first steps in producing an automatically gen- erated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization tech- niques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the frame- work of multi-document survey creation, cita- tion texts can play a crucial role.},
	number = {June},
	journal = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Mohammad, S and Dorr, B and Egan, M and Hassan, a and Muthukrishan, P and Qazvinian, V and Radev, D and Zajic, D},
	year = {2009},
	note = {ISBN: 9781932432411},
	pages = {584--592},
}

@article{bollegala_learning_2014,
	title = {Learning to {Predict} {Distributions} of {Words} {Across} {Domains}},
	abstract = {Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been lim- ited to a single domain because the dis- tribution of a word can vary between do- mains as the word’s predominant mean- ing changes. However, if it were pos- sible to predict how the distribution of a word changes from one domain to an- other, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsuper- vised method to predict the distribution of a word in one domain, given its distribu- tion in another domain. We evaluate our method on two tasks: cross-domain part- of-speech tagging and cross-domain sen- timent classification. In both tasks, our method significantly outperforms compet- itive baselines and returns results that are statistically comparable to current state- of-the-art methods, while requiring no task-specific customisations.},
	journal = {Acl},
	author = {Bollegala, Danushka and Weir, David and Carroll, John},
	year = {2014},
	note = {ISBN: 9781937284725},
	pages = {613--623},
}

@article{aransa_improving_2015,
	title = {Improving {Continuous} {Space} {Language} {Models} using {Auxiliary} {Features}},
	abstract = {In this paper we introduce a novel method to improve the continuous space language models using auxiliary features. The suggested auxiliary features include text genre, line length, various types of context vector representations. We report perplexity improvements of around 7.5\% of the En-glish Penn Treebank data set. We also report an improve-ment on a translation task up to 1.1 BLEU point on test by re-scoring the n-best list generated by our phrase-based sta-tistical machine translation system.},
	journal = {Iwslt},
	author = {Aransa, Walid and Schwenk, Holger and Barrault, Loc},
	year = {2015},
	pages = {151--158},
}

@article{ji_document_2015,
	title = {Document {Context} {Language} {Models}},
	url = {http://arxiv.org/abs/1511.03962},
	abstract = {Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.},
	author = {Ji, Yangfeng and Cohn, Trevor and Kong, Lingpeng and Dyer, Chris and Eisenstein, Jacob},
	year = {2015},
	note = {arXiv: 1511.03962},
	pages = {1--9},
}

@inproceedings{Liu2018,
	title = {{Hashtag2Vec} : {Learning} {Hashtag} {Representation} with {Relational} {Hierarchical} {Embedding} {Model}},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI}-18) {Hashtag2Vec}:},
	author = {Liu, Jie and He, Zhicheng and Huang, Yalou},
	year = {2018},
	keywords = {Machine Learning Applications: Networks, Machine Learning: Unsupervised Learning, Natural Language Processing: Embeddings, Natural Language Processing: Text Classification},
	pages = {3456--3462},
}

@article{Choudhury2015,
	title = {Anorexia on {Tumblr}: {A} characterization study},
	doi = {10.1145/2750511.2750515},
	abstract = {Eating disorders, such as anorexia nervosa are amajor health concern affecting many young individuals. Given the exten- sive adoption of social media technologies in the anorexia affected demographic, we study behavioral characteristics of this population focusing on the social media Tumblr. Aligned with observations in prior literature, we find the presence of two prominent anorexia related communities on Tumblr — pro-anorexia and pro-recovery. Empirical analy- ses on several thousand Tumblr posts show use of the site as a media-rich platform replete with triggering content for enacting anorexia as a lifestyle choice. Through use of com- mon pro-anorexia tags, the prorecovery community however attempts to “permeate” into the pro-anorexia community to educate them of the health risks of anorexia. Further, the communities exhibit distinctive affective, social, cogni- tive, and linguistic style markers. Compared with recover- ing anorexics, pro-anorexics express greater negative affect, higher cognitive impairment, and greater feelings of social isolation and selfharm. We also observe that these character- istics may be used in a predictive setting to detect anorexia content with ∼80\% accuracy. Based on our findings, clinical implications of detecting anorexia related content on social media are discussed},
	journal = {Proceedings of DH'15: 5th ACM Digital Health Conference. DH'15.},
	author = {Choudhury, Munmun De},
	year = {2015},
	note = {ISBN: 9781450334921},
	keywords = {anorexia, eating disorder, health, social media, tumblr},
	pages = {43--50},
}

@incollection{labovwaletzky1967,
	address = {Seattle},
	title = {Narrative analysis: {Oral} versions of personal experience.},
	url = {http://www2.clarku.edu/~mbamberg/LabovWaletzky.htm},
	booktitle = {Essays on the {Verbal} and {Visual} {Arts}},
	publisher = {University of Washington Press},
	author = {Labov, William and Waletzky, Joshua},
	editor = {Helm, June},
	year = {1967},
	pages = {12--44},
}

@inproceedings{cai_community_2017,
	title = {From {Community} {Detection} to {Community} {Profiling}},
	volume = {10},
	url = {http://arxiv.org/abs/1701.04528},
	doi = {10.14778/3067421.3067430},
	abstract = {Most existing community-related studies focus on detection, which aim to find the community membership for each user from user friendship links. However, membership alone, without a complete profile of what a community is and how it interacts with other communities, has limited applications. This motivates us to consider systematically profiling the communities and thereby developing useful community-level applications. In this paper, we for the first time formalize the concept of community profiling. With rich user information on the network, such as user published content and user diffusion links, we characterize a community in terms of both its internal content profile and external diffusion profile. The difficulty of community profiling is often underestimated. We novelly identify three unique challenges and propose a joint Community Profiling and Detection (CPD) model to address them accordingly. We also contribute a scalable inference algorithm, which scales linearly with the data size and it is easily parallelizable. We evaluate CPD on large-scale real-world data sets, and show that it is significantly better than the state-of-the-art baselines in various tasks.},
	author = {Cai, Hongyun and Zheng, Vincent W. and Zhu, Fanwei and Chang, Kevin Chen-Chuan and Huang, Zi},
	year = {2017},
	note = {arXiv: 1701.04528
Issue: 7
ISSN: 21508097},
	pages = {817--828},
}

@incollection{kozlowski_capturing_2018,
	title = {Capturing the {Dynamics} of {Hashtag}-{Communities}},
	volume = {1},
	isbn = {978-3-319-72150-7},
	booktitle = {Complex {Networks} \& {Their} {Applications} {VI}, {Studies} in {Computational} {Intelligence}},
	author = {Kozlowski, Steve W J and Chang, Chu-hsiang Daisy and Biswas, Subir},
	year = {2018},
}

@article{shi_learning--rank_2016,
	title = {Learning-to-{Rank} for {Real}-{Time} {High}-{Precision} {Hashtag} {Recommendation} for {Streaming} {News}},
	url = {http://dl.acm.org/citation.cfm?doid=2872427.2882982},
	doi = {10.1145/2872427.2882982},
	abstract = {We address the problem of real-time recommendation of streaming Twitter hashtags to an incoming stream of news articles. The technical challenge can be framed as large scale topic classification where the set of topics (i.e., hashtags) is huge and highly dynamic. Our main applications come from digital journalism, e.g., promoting original content to Twit-ter communities and social indexing of news to enable better retrieval and story tracking. In contrast to the state-of-the-art that focuses on topic modelling approaches, we propose a learning-to-rank approach for modelling hashtag relevance. This enables us to deal with the dynamic nature of the prob-lem, since a relevance model is stable over time, while a topic model needs to be continuously retrained. We present the data collection and processing pipeline, as well as our methodology for achieving low latency, high precision rec-ommendations. Our empirical results show that our method outperforms the state-of-the-art, delivering more than 80\% precision. Our techniques are implemented in a real-time system that is currently under user trial with a big news organisation.},
	journal = {Proceedings of the 25th International Conference on World Wide Web - WWW '16},
	author = {Shi, Bichen and Ifrim, Georgiana and Hurley, Neil},
	year = {2016},
	note = {ISBN: 9781450341431},
	keywords = {dynamic topics, hash-, learning-to-rank, news, social indexing},
	pages = {1191--1202},
}

@inproceedings{wang_larger-context_2016,
	title = {Larger-{Context} {Language} {Modelling} with {Recurrent} {Neural} {Network}},
	isbn = {978-1-5108-2758-5},
	url = {http://arxiv.org/abs/1511.03729},
	abstract = {In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Wang, Tian and Cho, Kyunghyun},
	year = {2016},
	note = {arXiv: 1511.03729},
	pages = {1319--1329},
}

@inproceedings{Ozdikis2012,
	title = {Semantic {Expansion} of {Hashtags} for {Enhanced} {Event} {Detection} in {Twitter}},
	abstract = {In this work, we present an event detection method in Twitter based on clustering of hashtags and introduce an enhancement technique by using the semantic similarities between the hashtags. To this aim, we devised two methods for tweet vector generation and evaluated their effect on clustering and event detection performance in comparison to word-based vector generation methods. By analyzing the contexts of hashtags and their co-occurrence statistics with other words, we identify their paradigmatic relationships and similarities. We make use of this information while applying a lexico-semantic expansion on tweet contents before clustering the tweets based on their similarities. Our aim is to tolerate spelling errors and capture statements which actually refer to the same concepts. We evaluate our enhancement solution on a three-day dataset of tweets with Turkish content. In our evaluations, we observe clearer clusters, improvements in accuracy, and earlier event detection times.},
	booktitle = {2012 {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining} ({ASONAM})},
	author = {Ozdikis, Ozer and Senkul, Pinar and Oguztuzun, Halit},
	year = {2012},
	keywords = {clustering, event detection, hashtags, semantics, turkish, tweets in, twitter, word co-occurrences},
	pages = {20--24},
}

@article{Tsur2013,
	title = {Efficient {Clustering} of {Short} {Messages} into {General} {Domains}},
	abstract = {The ever increasing activity in social networks is mainly man- ifested by a growing stream of status updating or microblog- ging. The massive stream of updates emphasizes the need for accurate and efficient clustering of short messages on a large scale. Applying traditional clustering techniques is both inac- curate and inefficient due to sparseness. This paper presents an accurate and efficient algorithm for clustering Twitter tweets. We break the clustering task into two distinctive tasks/stages: (1) batch clustering of user annotated data, and (2) online clustering of a stream of tweets. In the first stage we rely on the habit of ‘tagging’, common in social media streams (e.g. hashtags), thus the algorithm can bootstrap on the tags for clustering of a large pool of hashtagged tweets. The stable clusters achieved in the first stage lend themselves for online clustering of a stream of (mostly) tagless messages. We evaluate our results against gold-standard classification and validate the results by employing multiple clustering eval- uation measures (information theoretic, paired, F and greedy). We compare our algorithm to a number of other clustering algorithms and various types of feature sets. Results show that the algorithm presented is both accurate and efficient and can be easily used for large scale clustering of sparse mes- sages as the heavy lifting is achieved on a sublinear number of documents.},
	journal = {Proceedings of the 7th International Conference on Weblogs and Social Media, (ICWSM'13)},
	author = {Tsur, Oren and Littman, Adi and Rappoport, Ari},
	year = {2013},
	note = {ISBN: 978-1-57735-610-3},
	keywords = {Full Paper},
	pages = {1--10},
}

@inproceedings{lorenz2017capturing,
	title = {Capturing the {Dynamics} of {Hashtag}-{Communities}},
	booktitle = {International {Workshop} on {Complex} {Networks} and their {Applications}},
	publisher = {Springer},
	author = {Lorenz, Philipp and Wolf, Frederik and Braun, Jonas and Conrad, Nataša Djurdjevac and Hövel, Philipp},
	year = {2017},
	pages = {401--413},
}

@article{Muntean2012,
	title = {Exploring the meaning behind twitter hashtags through clustering},
	volume = {127 LNBIP},
	issn = {18651348},
	doi = {10.1007/978-3-642-34228-8_22},
	journal = {Lecture Notes in Business Information Processing},
	author = {Muntean, Cristina Ioana and Morar, Gabriela Andreea and Moldovan, Darie},
	year = {2012},
	note = {ISBN: 9783642342271},
	keywords = {clustering, hashtag, k-means, twitter},
	pages = {231--242},
}

@article{Antenucci2011,
	title = {Classification of tweets via clustering of hashtags eecs 545 final project, fall, 2011},
	abstract = {We present two techniques to aid in the retrieval of information from Twit- ter. First we present a technique to cluster hashtags in meaningful topic groups using a combination of co-occurrence frequency, graph clustering and textual similarity. Second, we present a technique to classify a tweet in terms of these topic groups based on their word content using a combination of PCA dimensionality reduction and a variety of multi-class classi cation algorithms. We examine the relationship between the clustering step and the classi cation step to evaluate the performance of each.},
	author = {Antenucci, Dolan and Handy, Gregory and Modi, Akshay and Tinkerhess, Miller},
	year = {2011},
	keywords = {classification, clustering, hashtag, machine learning, twitter},
	pages = {1--11},
}

@article{carter_twitter_2011,
	title = {Twitter hashtags : {Joint} {Translation} and {Clustering}},
	url = {http://journal.webscience.org/529/},
	abstract = {The popularity of microblogging platforms, such as Twitter, renders them valuable real-time information resources for tracking various aspects of worldwide events, e.g., earthquakes, political elections, etc. Such events are usually characterized in microblog posts via the use of hashtags (). As microbloggers come from different backgrounds, and express themselves in different languages, we witness different translations of hashtags which, however, are about the same event. Language-dependent variants of hashtags can possibly lead to issues in content-analysis. In this paper, we propose a method for translating hashtags, which builds on methods from information retrieval. The method introduced is source and target language independent. Our method is desirable, either instead of, or complimentary, to the direct translation of the hashtag for three reasons. First we return a list of hashtags on the same topic, which takes into account the plurality and variability of hashtags used by microbloggers for assigning posts to a topic. Second, our framework accounts for the problem that microbloggers in different languages will refer to the same topic using different tokens. Finally, our method does not require special preprocessing of hashtags, reducing barriers to real-world implementation. We present proof-of-concept results for the given Spanish hashtag 33mineros.},
	journal = {Human Factors},
	author = {Carter, Simon},
	year = {2011},
	pages = {1--3},
}

@article{wang_topic_2011,
	title = {Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach},
	url = {http://dl.acm.org/citation.cfm?id=2063726},
	doi = {10.1145/2063576.2063726},
	abstract = {Twitter is one of the biggest platforms where massive instant messages (i.e. tweets) are published every day. Users tend to express their real feelings freely in Twitter, which makes it an ideal source for capturing the opinions towards various interesting topics, such as brands, products or celebrities, etc. Naturally, people may anticipate an approach to receiving the common sentiment tendency towards these topics directly rather than through reading the huge amount of tweets about them. On the other side, Hashtags, starting with a symbol "\#" ahead of keywords or phrases, are widely used in tweets as coarse-grained topics. In this paper, instead of presenting the sentiment polarity of each tweet relevant to the topic, we focus our study on hashtag-level sentiment classification. This task aims to automatically generate the overall sentiment polarity for a given hashtag in a certain time period, which markedly differs from the conventional sentence-level and document-level sentiment analysis. Our investigation illustrates that three types of information is useful to address the task, including (1) sentiment polarity of tweets containing the hashtag; (2) hashtags co-occurrence relationship and (3) the literal meaning of hashtags. Consequently, in order to incorporate the first two types of information into a classification framework where hashtags can be classified collectively, we propose a novel graph model and investigate three approximate collective classification algorithms for inference. Going one step further, we show that the performance can be remarkably improved using an enhanced boosting classification setting in which we employ the literal meaning of hashtags as semi-supervised information. Experimental results o},
	journal = {Proceedings of the 20th ACM international conference on Information and knowledge management},
	author = {Wang, Xiaolong and Wei, Furu and Liu, Xiaohua and Zhou, Ming and Zhang, Ming},
	year = {2011},
	note = {ISBN: 9781450307178},
	keywords = {Graph-based Classificatio, Hashtag, Sentiment Analysis, Twitter},
	pages = {1031--1040},
}

@article{hu_toward_2017,
	title = {Toward {Controlled} {Generation} of {Text}},
	url = {http://arxiv.org/abs/1703.00955},
	doi = {arXiv:1},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2017},
	note = {arXiv: 1703.00955},
}

@inproceedings{zhou_multi-space_2017,
	title = {Multi-space {Variational} {Encoder}-{Decoders} for {Semi}-supervised {Labeled} {Sequence} {Transduction}},
	isbn = {978-1-945626-75-3},
	url = {http://arxiv.org/abs/1704.01691},
	doi = {10.18653/v1/P17-1029},
	abstract = {Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Zhou, Chunting and Neubig, Graham},
	year = {2017},
	note = {arXiv: 1704.01691},
}

@article{hovy_user_2015,
	title = {User {Review} {Sites} as a {Resource} for {Large}-{Scale} {Sociolinguistic} {Studies}},
	url = {http://dl.acm.org/citation.cfm?doid=2736277.2741141},
	doi = {10.1145/2736277.2741141},
	abstract = {Sociolinguistic studies investigate the relation between language and extra-linguistic variables. This requires both representative text data and the associated socio-economic meta-data of the subjects. Traditionally, sociolinguistic studies use small ...},
	journal = {Proceedings of the 24th International Conference on World Wide Web - WWW '15},
	author = {Hovy, Dirk and Johannsen, Anders and Søgaard, Anders},
	year = {2015},
	note = {ISBN: 9781450334693},
	keywords = {insights from natural-language analysis, language-analysis techniques, lingual analysis and mining, multi-lingual and cross-, of, on social media, social science research based},
	pages = {452--461},
}

@article{chung_recurrent_2015,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	issn = {10495258},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
	journal = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	year = {2015},
	note = {arXiv: 1506.02216},
	pages = {8},
}

@article{kageback_word_2016,
	title = {Word {Sense} {Disambiguation} using a {Bidirectional} {LSTM}},
	abstract = {In this paper we present a clean, yet effective, model for word sense disambiguation. Our ap-proach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes ef-fective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no ex-ternal resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.},
	author = {Kågebäck, Mikael and Salomonsson, Hans},
	year = {2016},
	pages = {51--56},
}

@article{Ma2014,
	title = {Tagging {Your} {Tweets} : {A} {Probabilistic} {Modeling} of {Hashtag} {Annotation} in {Twitter}},
	url = {http://dl.acm.org/citation.cfm?id=2661903},
	doi = {10.1145/2661829.2661903},
	abstract = {The adoption of hashtags in major social networks including Twitter, Facebook, and Google+ is a strong evidence of its importance in facilitating information diffusion and social chatting. To understand the factors (e.g., user interest, posting time and tweet content) that may affect hashtag annotation in Twitter and to capture the implicit relations between latent topics in tweets and their corresponding hashtags, we propose two PLSA-style topic models to model the hashtag annotation behavior in Twitter. Content-Pivoted Model (CPM) assumes that tweet content guides the generation of hashtags while Hashtag-Pivoted Model (HPM) assumes that hashtags guide the generation of tweet content. Both models jointly incorporate user, time, hashtag and tweet content in a probabilistic framework. The PLSA-style models also enable us to verify the impact of social factor on hashtag annotation by introducing social network regularization in the two models. We evaluate the proposed models using perplexity and demonstrate their effectiveness in two applications: retrospective hashtag annotation and related hashtag discovery. Our results show that HPM outperforms CPM by perplexity and both user and time are important factors that affect model performance. In addition, incorporating social network regularization does not improve model performance. Our experimental results also demonstrate the effectiveness of our models in both applications compared with baseline methods.},
	number = {i},
	journal = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
	author = {Ma, Zongyang and Sun, Aixin and Yuan, Quan and Cong, Gao},
	year = {2014},
	note = {ISBN: 9781450325981},
	keywords = {hashtag, hashtag annotation, topic model, twitter},
	pages = {999--1008},
}

@article{Boyd2007,
	title = {Why {Youth} ({Heart}) {Social} {Network} {Sites}: {The} {Role} of {Networked} {Publics} in {Teenage} {Social} {Life}},
	volume = {7641},
	issn = {15292401},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Why+Youth+(Heart)+Social+Network+Sites:+The+Role+of+Networked+Publics+in+Teenage+Social+Life#1},
	doi = {10.1162/dmal.9780262524834.119},
	abstract = {The rapid adoption of social network sites by teenagers in the United States and in many other countries around the world raises some important questions. Why do teenagers flock to these sites? What are they expressing on them? How do these sites fit into their lives? What are they learning from their participation? Are these online activities like face-to-face friendships or are they different, or complementary? The goal of this chapter is to address these questions, and explore their implications for youth identities. While particular systems may come and go, how youth engage through social network sites today provides long-lasting insights into identity formation, status negotiation, and peer-to-peer sociality. To address the aforementioned questions, I begin by documenting key features of social network sites and the business decisions that lead to mass adoption, and then seek to situate social network sites in a broader discussion of what I call networked publics. I then examine how teens are modeling identity through social network profiles so that they can write themselves and their community into being. Building on this, I investigate how this process of articulated expression supports critical peer-based sociality because, by allowing youth to hang out amongst their friends and classmates, social network sites are providing teens with a space to work out identity and status, make sense of cultural cues, and negotiate public life. I argue that social network sites are a type of networked public with four properties that are not typically present in face-to-face public life: persistence, searchability, exact copyability, and invisible audiences. These properties fundamentally alter social dynamics, complicating the ways in which people interact. I conclude by reflecting on the social developments that have prompted youth to seek out networked publics, and considering the changing role that publics have in young peoples lives.},
	number = {41},
	journal = {MacArthur Foundation Series on Digital Learning – Youth, Identity, and Digital Media},
	author = {Boyd, Danah},
	year = {2007},
	pmid = {20943929},
	note = {arXiv: cond-mat/0402594v3
ISBN: 9780262524834},
	keywords = {2007-16, earch publication no},
	pages = {1--26},
}

@book{wenger1998communities,
	title = {Communities of practice: {Learning}, meaning, and identity},
	publisher = {Cambridge university press},
	author = {Wenger, Etienne},
	year = {1998},
}

@inproceedings{khodak_carte_2018,
	title = {A {La} {Carte} {Embedding}: {Cheap} but {Effective} {Induction} of {Semantic} {Feature} {Vectors}},
	url = {http://arxiv.org/abs/1805.05388},
	abstract = {Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Khodak, Mikhail and Saunshi, Nikunj and Liang, Yingyu and Ma, Tengyu and Stewart, Brandon and Arora, Sanjeev},
	year = {2018},
	note = {arXiv: 1805.05388},
	pages = {1--11},
}

@article{tixier_word_2016,
	title = {Word {Embeddings} for the {Construction} {Domain}},
	url = {http://arxiv.org/abs/1610.09333},
	abstract = {We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful construction-specific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.},
	number = {October},
	author = {Tixier, Antoine J. -P. and Vazirgiannis, Michalis and Hallowell, Matthew R.},
	year = {2016},
	note = {arXiv: 1610.09333},
	keywords = {-based compression of the, and the data set, classification, ited loss in performance, leads to a significant, lim-, reports also, speed-up with only a, we created for the, we release our corpus},
}

@article{raganato_neural_2017,
	title = {Neural {Sequence} {Learning} {Models} for {Word} {Sense} {Disambiguation}},
	abstract = {Word Sense Disambiguation models ex-ist in many flavors. Even though super-vised ones tend to perform best in terms of accuracy, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every disambiguation target. To bridge this gap we adopt a different perspective and rely on sequence learning to frame the disambiguation problem: we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models. Our extensive evaluation over standard bench-marks and in multiple languages shows that sequence learning enables more ver-satile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.},
	author = {Raganato, Alessandro and Delli Bovi, Claudio and Navigli, Roberto},
	year = {2017},
	pages = {1167--1178},
}

@inproceedings{zhang_earth_2017,
	title = {Earth {Mover}'s {Distance} {Minimization} for {Unsupervised} {Bilingual} {Lexicon} {Induction}},
	isbn = {978-1-945626-75-3},
	url = {http://aclweb.org/anthology/P17-1179},
	doi = {10.18653/v1/P17-1179},
	abstract = {Cross-lingual natural language processing hinges on the premise that there exists in-variance across languages. At the word level, researchers have identified such in-variance in the word embedding seman-tic spaces of different languages. How-ever, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as dis-tributions, we propose to minimize their earth mover's distance, a measure of diver-gence between distributions. We demon-strate the success on the unsupervised bilingual lexicon induction task. In addi-tion, we reveal an interesting finding that the earth mover's distance shows potential as a measure of language difference.},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Zhang, Meng and Liu, Yang and Luan, Huanbo and Sun, Maosong},
	year = {2017},
	pages = {1959--1970},
}

@inproceedings{jameel_unsupervised_2018,
	title = {Unsupervised {Learning} of {Distributional} {Relation} {Vectors}},
	abstract = {Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to cap-ture rich information about the relation-ships between different words, existing ap-proaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns re-lation vectors from co-occurrence statis-tics. To this end, we first introduce a vari-ant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
	year = {2018},
	pages = {1--11},
}

@article{iacobacci_sensembed:_2015,
	title = {{SensEmbed}: {Learning} {Sense} {Embeddings} for {Word} and {Relational} {Similarity}},
	url = {http://aclweb.org/anthology/P15-1010},
	doi = {10.3115/v1/P15-1010},
	abstract = {Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including seman-tic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multi-faceted approach that transforms word embeddings to the sense level and lever-ages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similar-ity frameworks, reporting state-of-the-art performance on multiple datasets.},
	number = {1},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {95--105},
}

@article{melamud_context2vec:_2016,
	title = {context2vec: {Learning} {Generic} {Context} {Embedding} with {Bidirectional} {LSTM}},
	url = {http://aclweb.org/anthology/K16-1006},
	doi = {10.18653/v1/K16-1006},
	abstract = {Context representations are central to vari-ous NLP tasks, such as word sense disam-biguation, named entity recognition, co-reference resolution, and many more. In this work we present a neural model for efficiently learning a generic context em-bedding function from large corpora, us-ing bidirectional LSTM. With a very sim-ple application of our context represen-tations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while sub-stantially outperforming the popular con-text representation of averaged word em-beddings. We release our code and pre-trained models, suggesting they could be useful in a wide variety of NLP tasks.},
	journal = {Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},
	author = {Melamud, Oren and Goldberger, Jacob and Dagan, Ido},
	year = {2016},
	pages = {51--61},
}

@article{neelakantan_efficient_2015,
	title = {Efficient {Non}-parametric {Estimation} of {Multiple} {Embeddings} per {Word} in {Vector} {Space}},
	issn = {2307-387X},
	url = {http://arxiv.org/abs/1504.06654},
	doi = {10.1007/978-3-540-87479-9},
	abstract = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.},
	author = {Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
	year = {2015},
	pmid = {1687737},
	note = {arXiv: 1504.06654
ISBN: 9781937284435},
	pages = {1059--1069},
}

@article{Vicient2015,
	title = {Unsupervised topic discovery in micro-blogging networks},
	volume = {42},
	issn = {09574174},
	url = {http://dx.doi.org/10.1016/j.eswa.2015.04.014},
	doi = {10.1016/j.eswa.2015.04.014},
	abstract = {Unsupervised automatic topic discovery in micro-blogging social networks is a very challenging task, as it involves the analysis of very short, noisy, ungrammatical and uncontextual messages. Most of the current approaches to this problem are basically syntactic, as they focus either on the use of statistical techniques or on the analysis of the co-occurrences between the terms. This paper presents a novel topic discovery methodology, based on the mapping of hashtags to WordNet terms and their posterior clustering, in which semantics plays a centre role. The paper also presents a detailed case study in the field of Oncology, in which the discovered topics are thoroughly compared to a golden standard, showing promising results.},
	number = {17-18},
	journal = {Expert Systems with Applications},
	author = {Vicient, Carlos and Moreno, Antonio},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Knowledge-based systems, Micro-blogging, Semantic Web, Twitter},
	pages = {6472--6485},
}

@article{doi:10.1162/COLI\_a\_00277,
	title = {Hashtag {Sense} {Clustering} {Based} on {Temporal} {Similarity}},
	volume = {43},
	url = {https://doi.org/10.1162/COLI_a_00277},
	doi = {10.1162/COLI_a_00277},
	abstract = {Hashtags are creative labels used in micro-blogs to characterize the topic of a message/discussion. Regardless of the use for which they were originally intended, hashtags cannot be used as a means to cluster messages with similar content. First, because hashtags are created in a spontaneous and highly dynamic way by users in multiple languages, the same topic can be associated with different hashtags, and conversely, the same hashtag may refer to different topics in different time periods. Second, contrary to common words, hashtag disambiguation is complicated by the fact that no sense catalogs (e.g., Wikipedia or WordNet) are available; and, furthermore, hashtag labels are difficult to analyze, as they often consist of acronyms, concatenated words, and so forth. A common way to determine the meaning of hashtags has been to analyze their context, but, as we have just pointed out, hashtags can have multiple and variable meanings. In this article, we propose a temporal sense clustering algorithm based on the idea that semantically related hashtags have similar and synchronous usage patterns.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Stilo, Giovanni and Velardi, Paola},
	year = {2017},
	pages = {181--200},
}

@article{Romero2011,
	title = {Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter},
	issn = {14602059},
	url = {http://dl.acm.org/citation.cfm?id=1963503},
	doi = {10.1145/1963405.1963503},
	abstract = {There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread. Our results show that this variation is not attributable simply to differences in "stickiness," the probability of adoption based on one or more exposures, but also to a quantity that could be viewed as a kind of "persistence" - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects. We find that hashtags on politically controversial topics are particularly persistent, with repeated exposures continuing to have unusually large marginal effects on adoption; this provides, to our knowledge, the first large-scale validation of the "complex contagion" principle from sociology, which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious. Among other findings, we discover that hashtags representing the natural analogues of Twitter idioms and neologisms are particularly non-persistent, with the effect of multiple exposures decaying rapidly relative to the first exposure. We also study the subgraph structure of the initial adopters for different widely-adopted hashtags, again finding structural differences across topics. We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads.},
	journal = {WWW'11 Proceedings of the 20th International Conference on World Wide Web},
	author = {Romero, Daniel M and Meeder, Brendan and Kleinberg, Jon},
	year = {2011},
	note = {ISBN: 9781450306324},
	keywords = {Social media, information diffusion, social contagion},
	pages = {695--704},
}

@article{huang_conversational_2010,
	title = {Conversational tagging in twitter},
	issn = {03638111},
	url = {http://portal.acm.org/citation.cfm?doid=1810617.1810647},
	doi = {10.1145/1810617.1810647},
	abstract = {Users on Twitter, a microblogging service, started the phenomenon of adding tags to their messages sometime around February 2008. These tags are distinct from those in other Web 2.0 systems because users are less likely to index messages for later retrieval. We compare tagging patterns in Twitter with those in Delicious to show that tagging behavior in Twitter is different because of its conversational, rather than organizational nature. We use a mixed method of statistical analysis and an interpretive approach to study the phenomenon. We find that tagging in Twitter is more about filtering and directing content so that it appears in certain streams. The most illustrative example of how tagging in Twitter differs is the phenomenon of the Twitter micro-meme: emergent topics for which a tag is created, used widely for a few days, then disappears. We describe the micro-meme phenomenon and discuss the importance of this new tagging practice for the larger real-time search context.},
	journal = {Proceedings of the 21st ACM conference on Hypertext and hypermedia - HT '10},
	author = {Huang, Jeff and Thornton, Katherine M. and Efthimiadis, Efthimis N.},
	year = {2010},
	note = {arXiv: 1111.1896
ISBN: 9781450300414},
	keywords = {memes, tagging, trends, twitter},
	pages = {173},
}

@incollection{Gee2000,
	title = {Chapter 3: {Identity} as an {Analytic} {Lens} for {Research} in {Education}},
	volume = {25},
	isbn = {0091732X},
	url = {http://rre.sagepub.com/cgi/doi/10.3102/0091732X025001099},
	booktitle = {Review of {Research} in {Education}},
	author = {Gee, J. P.},
	year = {2000},
	pmid = {25246403},
	doi = {10.3102/0091732X025001099},
	note = {arXiv: 1011.1669v3
Issue: 1
ISSN: 0091-732X},
	pages = {99--125},
}

@article{mcclellan2014redefining,
	title = {Redefining genderswap fan fiction: {A} {Sherlock} case study.},
	volume = {17},
	journal = {Transformative Works \& Cultures},
	author = {McClellan, Ann},
	year = {2014},
}

@article{Milli2016,
	title = {Beyond {Canonical} {Texts} : {A} {Computational} {Analysis} of {Fanfiction}},
	url = {http://www.aclweb.org/anthology/D16-1218},
	abstract = {While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique op- portunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illus- trate the range of affordances it presents to re- search in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories.},
	journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)},
	author = {Milli, Smitha and Bamman, David},
	year = {2016},
	pages = {2048--2053},
}

@article{Tosenberger2008,
	title = {Homosexuality at the {Online} {Hogwarts}: {Harry} {Potter} {Slash} {Fanfiction}},
	volume = {36},
	issn = {1543-3374},
	url = {http://muse.jhu.edu/content/crossref/journals/childrens_literature/v036/36.1.tosenberger.html},
	doi = {10.1353/chl.0.0017},
	abstract = {Fans of J.K. Rowling's Harry Potter series have written new stories featuring the characters; many of these stories, called "slash," are homoerotic. This article examines a number of online slash stories, and discusses the potential for fans, especially teens, to experiment with non-heteronormative discourses through the medium of Potter fanfiction.},
	number = {1},
	journal = {Children's Literature},
	author = {Tosenberger, Catherine},
	year = {2008},
	note = {ISBN: 1543-3374},
	pages = {185--207},
}

@phdthesis{Grant2015,
	title = {Tumblinguistics : innovation and variation in new forms of written {CMC}},
	url = {https://www.academia.edu/18612487/Tumblinguistics_innovation_and_variation_in_new_forms_of_written_CMC#_=_},
	author = {Grant, Harley},
	year = {2015},
}

@article{Fink2014,
	title = {Trans {Media} {Moments}},
	volume = {15},
	issn = {1527-4764},
	url = {http://journals.sagepub.com/doi/10.1177/1527476413505002},
	doi = {10.1177/1527476413505002},
	abstract = {For transgender, transsexual, genderqueer, and gender nonconforming people, emergent media technologies offer new outlets for self-representation, outlets that often last for only a brief moment. This article examines transculture on the website Tumblr during the period from March 2011, when the authors began researching the platform, to May 2013, when Yahoo! paid creator David Karp over a billion dollars for the site. Through auto-ethnographic dialogue about the loose social networks within Tumblr to which the authors contributed during this phase, the article explores ephemeral aspects of self-representation at the intersection of postmodern art practice, sexual politics, and queer subjectivities. From at least 2011 to 2013, people collectively oriented in opposition to dominant discourses of gender and sexuality used Tumblr to refashion straight cisgender norms and to create everyday art in a hybrid media space.},
	number = {7},
	journal = {Television \& New Media},
	author = {Fink, Marty and Miller, Quinn},
	year = {2014},
	note = {ISBN: 1527476413},
	keywords = {blog, new media, queer, sexuality, transgender, tumblr},
	pages = {611--626},
}

@article{feng_streamcube:_2015,
	title = {{STREAMCUBE}: {Hierarchical} spatio-temporal hashtag clustering for event exploration over the {Twitter} stream},
	volume = {2015-May},
	issn = {10844627},
	doi = {10.1109/ICDE.2015.7113425},
	abstract = {What is happening around the world? When and where? Mining the geo-tagged Twitter stream makes it possible to answer the above questions in real-time. Although a single tweet can be short and noisy, proper aggregations of tweets can provide meaningful results. In this paper, we focus on hierarchical spatio-temporal hashtag clustering techniques. Our system has the following features: (1) Exploring events (hashtag clusters) with different space granularity. Users can zoom in and out on maps to find out what is happening in a particular area. (2) Exploring events with different time granularity. Users can choose to see what is happening today or in the past week. (3) Efficient single-pass algorithm for event identification, which provides human-readable hashtag clusters. (4) Efficient event ranking which aims to find burst events and localized events given a particular region and time frame. To support aggregation with different space and time granularity, we propose a data structure called STREAMCUBE, which is an extension of the data cube structure from the database community with spatial and temporal hierarchy. To achieve high scalability, we propose a divide-and-conquer method to construct the STREAMCUBE. To support flexible event ranking with different weights, we proposed a top-k based index. Different efficient methods are used to speed up event similarity computations. Finally, we have conducted extensive experiments on a real twitter data. Experimental results show that our framework can provide meaningful results with high scalability.},
	journal = {Proceedings - International Conference on Data Engineering},
	author = {Feng, Wei and Zhang, Chao and Zhang, Wei and Han, Jiawei and Wang, Jianyong and Aggarwal, Charu and Huang, Jianbin},
	year = {2015},
	pmid = {7113425},
	note = {ISBN: 9781479979639},
	keywords = {[Electronic Manuscript]},
	pages = {1561--1572},
}

@article{hogan_changing_2005,
	title = {Changing {Racial} {Prejudice} {Through} {Diversity} {Education}},
	volume = {46},
	issn = {1543-3382},
	doi = {10.1353/csd.2005.0015},
	abstract = {The Modern Racism Scale (McConahay, 1986) was used to assess the impact of education and personality variables on college students' prejudicial attitudes toward African Americans. Prejudice was lower in students who completed a diversity course specifically addressing race and gender issues and in students who measured high in need for cognition (Cacioppo \& Petty, 1982). A weak correlation between the prejudice scale and a social desirability scale (Crowne \& Marlowe, 1960) suggested that students were not grossly underreporting their prejudicial beliefs. Diversity courses in higher education were effective in improving students' intergroup tolerance.},
	number = {2},
	journal = {Journal of College Student Development},
	author = {Hogan, D. E. and Mallott, M.},
	year = {2005},
	pages = {115--125},
}

@article{kulik_common_2008,
	title = {Common {Goals} and {Golden} {Evaluations} of {Opportunities} : {Education} in {Diversity} {Academic} and {Organizational} {Settings}},
	volume = {7},
	issn = {1537260X},
	doi = {10.5465/AMLE.2008.34251670},
	number = {3},
	journal = {Academy of Management Learning and Education},
	author = {Kulik, Carol T and Roberson, L},
	year = {2008},
	pmid = {34251670},
	note = {ISBN: 1537260X},
	pages = {309--331},
}

@article{herek1996some,
	title = {" {Some} of {My} {Best} {Friends}" {Intergroup} {Contact}, {Concealable} {Stigma}, and {Heterosexuals}' {Attitudes} {Toward} {Gay} {Men} and {Lesbians}},
	volume = {22},
	number = {4},
	journal = {Personality and Social Psychology Bulletin},
	author = {Herek, Gregory M and Capitanio, John P},
	year = {1996},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {412--424},
}

@article{Arnett1983,
	title = {The {Assumptive} {Roots} of {Empathic} {Listening}: {A} {Critique}},
	volume = {32},
	issn = {14795795},
	doi = {10.1080/03634528309378558},
	abstract = {Examines the aspect of emphatic listening applied in public speaking and mass media. Historical overview of empathic tradition; Dynamics of empathic listening; Concept of the interpersonal approach to listening.},
	number = {4},
	journal = {Communication Education},
	author = {Arnett, Ronald C. and Nakagawa, Gordon},
	year = {1983},
	pages = {368--378},
}

@article{deturk_intercultural_2001,
	title = {Intercultural empathy: {Myth}, competency, or possibility for alliance building?},
	volume = {50},
	issn = {0363-4523},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03634520109379262},
	doi = {10.1080/03634520109379262},
	abstract = {"Part of this paradox can be attributed to the difficulty of defining empathy. Is it a trait, a learned skill, or a contexrually emergent relational state?" p375},
	number = {4},
	journal = {Communication Education},
	author = {DeTurk, Sara},
	year = {2001},
	pmid = {5357452},
	note = {ISBN: 0363-4523{\textbackslash}r1479-5795},
	pages = {374--384},
}

@article{walby_intersectionality:_2012,
	title = {Intersectionality: {Multiple} inequalities in social theory},
	volume = {46},
	issn = {00380385},
	doi = {10.1177/0038038511416164},
	abstract = {The concept of intersectionality is reviewed and further developed for more effective use. Six dilemmas in the debates on the concept are disentangled, addressed and resolved: the distinction between structural and political intersectionality; the tension between ‘categories’ and ‘inequalities’; the significance of class; the balance between a fluidity and stability; the varyingly competitive, cooperative, hierarchical and hegemonic relations between inequalities and between projects; and the conundrum of ‘visibility’ in the tension between the ‘mutual shaping’ and the ‘mutual constitution’ of inequalities. The analysis draws on critical realism and on complexity theory in order to find answers to the dilemmas in intersectionality theory.},
	number = {2},
	journal = {Sociology},
	author = {Walby, Sylvia and Armstrong, Jo and Strid, Sofia},
	year = {2012},
	note = {ISBN: 0038-0385 1469-8684},
	keywords = {Gender, Inequality/inequalities, Intersectionality, Social theory},
	pages = {224--240},
}

@article{Aultman2018,
	title = {Keywords: {Cisgender}},
	volume = {1},
	number = {1},
	journal = {TSQ: Transgender Studies Quarterly},
	author = {Aultman, B},
	year = {2014},
	pages = {61--62},
}

@article{Abrams2007,
	title = {Teaching notes: {Reframing} multicultural education: {Teaching} white privilege in the social work curriculum},
	volume = {43},
	number = {1},
	journal = {Journal of Social Work Education},
	author = {Abrams, Laura S. and Gibson, Priscilla},
	year = {2007},
	pages = {147--160},
}

@incollection{landreman2013art,
	title = {From {Safe} {Spaces} to {Brave} {Spaces}: {A} {New} {Way} to {Frame} {Dialogue} {Around} {Diversity} and {Social} {Justice}},
	isbn = {978-1-57922-974-0},
	url = {https://books.google.com/books?id=9eFaAQAAQBAJ},
	booktitle = {The {Art} of {Effective} {Facilitation}: {Reflections} from {Social} {Justice} {Educators}},
	publisher = {Stylus Publishing, LLC},
	author = {Arao, Brian and Clemens, Kristi},
	editor = {Landreman, L M},
	year = {2013},
	note = {Series Title: ACPA Books Co-Published with Stylus Publishing Series},
	pages = {135--150},
}

@incollection{Dyer2005,
	title = {The {Matter} of {Whiteness}},
	booktitle = {White privilege: {Essential} readings on the other side of racism},
	publisher = {Worth Publishing},
	author = {Dyer, Richard},
	editor = {Rothenberg, Paula},
	year = {2005},
}

@article{Gianfortoni2011,
	title = {Modeling of stylistic variation in social media with stretchy patterns},
	url = {http://dl.acm.org/citation.cfm?id=2140533.2140539},
	abstract = {In this paper we describe a novel feature discovery technique that can be used to model stylistic variation in sociolects. While structural features offer much in terms of expressive power over simpler features used more frequently in machine learning approaches to modeling linguistic variation, they frequently come at an excessive cost in terms of feature space size expansion. We propose a novel form of structural features referred to as "stretchy patterns" that strike a balance between expressive power and compactness in order to enable modeling stylistic variation with reasonably small datasets. As an example we focus on the problem of modeling variation related to gender in personal blogs. Our evaluation demonstrates a significant improvement over standard baselines.},
	journal = {DIALECTS '11 Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties},
	author = {Gianfortoni, Philip and Adamson, David and Rose, Carolyn P.},
	year = {2011},
	note = {ISBN: 978-1-937284-17-6},
	pages = {49--59},
}

@article{Ardehaly2015,
	title = {Inferring latent attributes of {Twitter} users with label regularization},
	abstract = {Inferring latent attributes of online users has many applications in public health, politics, and marketing. Most existing approaches rely on supervised learning algorithms, which re-quire manual data annotation and therefore are costly to develop and adapt over time. In this paper, we propose a lightly supervised approach based on label regularization to in-fer the age, ethnicity, and political orientation of Twitter users. Our approach learns from a heterogeneous collection of soft constraints derived from Census demographics, trends in baby names, and Twitter accounts that are em-blematic of class labels. To counteract the im-precision of such constraints, we compare sev-eral constraint selection algorithms that opti-mize classification accuracy on a tuning set. We find that using no user-annotated data, our approach is within 2\% of a fully supervised baseline for three of four tasks. Using a small set of labeled data for tuning further improves accuracy on all tasks.},
	journal = {HLT-NAACL 2015 - Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings of the Main Conference},
	author = {Ardehaly, Ehsan Mohammady and Culotta, Aron},
	year = {2015},
	note = {ISBN: 9781941643495},
	pages = {185--195},
}

@article{Argamon2009,
	title = {Automatically profiling the author of an anonymous text},
	volume = {52},
	issn = {00010782},
	url = {http://portal.acm.org/citation.cfm?doid=1461928.1461959},
	doi = {10.1145/1461928.1461959},
	abstract = {Imagine that you have been given an important text of unknown authorship, and wish to know as much as possible about the unknown author (demographics, personality, cultural background, etc.), just by analyzing the given text. This authorship profiling problem is of growing importance in the current global information environment – applications abound in forensics, security, and commercial settings. For example, authorship profiling can help police identify characteristics of the perpetrator of a crime when there are too few (or too many) specific suspects to consider. On the other hand, large corporations may be interested in knowing what types of people like or dislike their products, based on analysis of blogs and online product reviews. The question we therefore ask is: How much can we discern about the author of a text simply by analyzing the text itself? It turns out that, with varying degrees of accuracy, we can say a great deal indeed.},
	number = {2},
	journal = {Communications of the ACM},
	author = {Argamon, Shlomo and Koppel, Moshe and Pennebaker, James W. and Schler, Jonathan},
	year = {2009},
	pmid = {36673346},
	note = {ISBN: 9782895291305},
	pages = {119},
}

@article{Fink2012,
	title = {Inferring {Gender} from the {Content} of {Tweets}: {A} {Region} {Specific} {Example}.},
	url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4644/5031},
	abstract = {There is growing interest in using social networking sites such as Twitter to gather real-time data on the reactions and opinions of a region's population, including locations in the developing world where social media has played an important role in recent events, such as the 2011 Arab Spring. However, many interesting and important opinions and reactions may differ significantly within a given region depending on the demographics of the subpopulation, including such categories as gender and ethnicity. This information may not be explicitly available in user content or metadata, however, and automated methods are required to infer such hidden attributes. In this paper we describe a method to infer the gender of Twitter users from only the content of their tweets. Looking at Twitter users from the West African nation of Nigeria, we applied supervised machine learning using features derived from the content of user tweets to train a classifier. Using unigram features alone, we obtained an accuracy of 80\% for predicting gender, suggesting that content alone can be a good predictor of gender. An analysis of the highest weighted features shows some interesting distinctions between men and women both topically and emotionally. We argue that approaches such as the one described here can give us a clearer picture of who is utilizing social media when certain user attributes are unreliable or not available.},
	journal = {International Conference on Weblogs and Social Media},
	author = {Fink, Clay and Kopecky, Jonathon and Morawski, M},
	year = {2012},
	note = {ISBN: 9781577355564},
	keywords = {Short Papers},
	pages = {459--462},
}

@article{Rao2010,
	title = {Classifying latent user attributes in twitter},
	issn = {00010782},
	url = {http://portal.acm.org/citation.cfm?doid=1871985.1871993},
	doi = {10.1145/1871985.1871993},
	abstract = {Social media outlets such as Twitter have become an impor- tant forum for peer interaction. Thus the ability to classify latent user attributes, including gender, age, regional origin, and political orientation solely fromTwitter user language or similar highly informal content has important applications in advertising, personalization, and recommendation. This paper includes a novel investigation of stacked-SVM-based classification algorithms over a rich set of original features, applied to classifying these four user attributes. It also in- cludes extensive analysis of features and approaches that are effective and not effective in classifying user attributes in Twitter-style informal written genres as distinct from the other primarily spoken genres previously studied in the user- property classification literature. Our models, singly and in ensemble, significantly outperform baseline models in all cases. A detailed analysis of model components and fea- tures provides an often entertaining insight into distinctive language-usage variation across gender, age, regional origin and political orientation inmodern informal communication.},
	journal = {Proceedings of the 2nd international workshop on Search and mining user-generated contents - SMUC '10},
	author = {Rao, Delip and Yarowsky, David and Shreevats, Abhishek and Gupta, Manaswi},
	year = {2010},
	pmid = {23479631},
	note = {arXiv: 1690219.1690245‎
ISBN: 9781450303866},
	pages = {37},
}

@book{Butler1990,
	title = {Gender {Trouble}: {Feminism} and the {Subversion} of {Identity}},
	publisher = {Routledge},
	author = {Butler, Judith},
	year = {1990},
}

@article{Chen2015,
	title = {A {Comparative} {Study} of {Demographic} {Attribute} {Inference} in {Twitter}},
	url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/view/10541},
	abstract = {Social media platforms have become a major gateway to receive and analyze public opinions. Understandingusers can provide invaluable context information of their social media posts and significantly improve traditional opinion analysis models. Demographic attributes,such as ethnicity, gender, age, among others,have been extensively applied to characterize social mediausers. While studies have shown that user groups formed by demographic attributes can have coherent opinions towards political issues, these attributes are often not explicitly coded by users through their profiles.Previous work has demonstrated the effectiveness of different user signals such as users’ posts and names in determining demographic attributes. Yet, these efforts mostly evaluate linguistic signals from users’ postsand train models from artificially balanced datasets. In this paper, we propose a comprehensive list of user signals:self-descriptions and posts aggregated from users’ friends and followers, users’ profile images, and users’ names.We provide a comparative study of these signalsside-by-side in the tasks on inferring three major demographic attributes, namely ethnicity, gender, and age.We utilize a realistic unbalanced datasets that share similar demographic makeups in Twitter for training modelsand evaluation experiments. Our experiments indicate that self-descriptions provide the strongest signal for ethnicity and age inference and clearly improve the overall performance when combined with tweets. Profile images for gender inference have the highest precision score with overall score close to the best result in our setting. This suggests that signals in self descriptions and profile images have potentials to facilitate demographic attribute inferences in Twitter, and are promising for future investigation.},
	journal = {Ninth International AAAI Conference on Web and Social Media},
	author = {Chen, Xin and Wang, Yu and Agichtein, Eugene and Wang, Fusheng},
	year = {2015},
	note = {ISBN: 9781577357339},
	keywords = {Poster Papers},
	pages = {590--593},
}

@article{Kim2017,
	title = {Demographic {Inference} on {Twitter} using {Recursive} {Neural} {Networks}},
	url = {http://aclweb.org/anthology/P17-2075},
	doi = {10.18653/v1/P17-2075},
	abstract = {In social media, demographic inference is a critical task in order to gain a better understanding of a cohort and to facilitate interacting with one's audience. Most previous work has made independence assumptions over topological, textual and label information on social networks. In this work, we employ recursive neural networks to break down these independence assumptions to obtain inference about demographic characteristics on Twitter. We show that our model performs better than existing models including the state-of-the-art.},
	journal = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Kim, Sunghwan Mac and Xu, Qiongkai and Qu, Lizhen and Wan, Stephen and Paris, Cecile},
	year = {2017},
	note = {ISBN: 9781945626760},
	pages = {471--477},
}

@article{Bergsma2013,
	title = {Broadly {Improving} {User} {Classification} via {Communication}-{Based} {Name} and {Location} {Clustering} on {Twitter}.},
	issn = {0364-2348},
	doi = {10.1007/s00256-005-0933-8},
	abstract = {Hidden properties of social media users, such as their ethnicity, gender, and location, are of-ten reflected in their observed attributes, such as their first and last names. Furthermore, users who communicate with each other of-ten have similar hidden properties. We pro-pose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users. Attributes such as user names are grouped together if users with those names communicate with other similar users. We separately cluster millions of unique first names, last names, and user-provided locations. The efficacy of these clus-ters is then evaluated on a diverse set of clas-sification tasks that predict hidden users prop-erties such as ethnicity, geographic location, gender, language, and race, using only pro-file names and locations when appropriate. Our readily-replicable approach and publicly-released clusters are shown to be remarkably effective and versatile, substantially outper-forming state-of-the-art approaches and hu-man accuracy on each of the tasks studied.},
	number = {June},
	journal = {Hlt-Naacl},
	author = {Bergsma, Shane and Dredze, Mark and Van Durme, Benjamin and Wilson, Theresa and Yarowsky, David},
	year = {2013},
	pmid = {15999282},
	note = {arXiv: 1011.1669v3
ISBN: 9781937284473},
	pages = {1010--1019},
}

@article{Jernigan2009,
	title = {Gaydar: {Facebook} friendships expose sexual orientation},
	volume = {14},
	doi = {fm.v14i10.2611.},
	number = {10},
	journal = {First Monday},
	author = {Jernigan, Carter and Mistree, Behram F.T.},
	year = {2009},
	note = {ISBN: 13960466},
}

@article{Gong2016,
	title = {You are {Who} {You} {Know} and {How} {You} {Behave}: {Attribute} {Inference} {Attacks} via {Users}' {Social} {Friends} and {Behaviors}},
	url = {http://arxiv.org/abs/1606.05893},
	abstract = {We propose new privacy attacks to infer attributes (e.g., locations, occupations, and interests) of online social network users. Our attacks leverage seemingly innocent user information that is publicly available in online social networks to infer missing attributes of targeted users. Given the increasing availability of (seemingly innocent) user information online, our results have serious implications for Internet privacy -- private attributes can be inferred from users' publicly available data unless we take steps to protect users from such inference attacks. To infer attributes of a targeted user, existing inference attacks leverage either the user's publicly available social friends or the user's behavioral records (e.g., the webpages that the user has liked on Facebook, the apps that the user has reviewed on Google Play), but not both. As we will show, such inference attacks achieve limited success rates. However, the problem becomes qualitatively different if we consider both social friends and behavioral records. To address this challenge, we develop a novel model to integrate social friends and behavioral records and design new attacks based on our model. We theoretically and experimentally demonstrate the effectiveness of our attacks. For instance, we observe that, in a real-world large-scale dataset with 1.1 million users, our attack can correctly infer the cities a user lived in for 57\% of the users, via confidence estimation, we are able to increase the attack success rate to over 90\% if the attacker selectively attacks a half of the users. Moreover, we show that our attack can correctly infer attributes for significantly more users than previous attacks.},
	journal = {25th USENIX Security Symposium},
	author = {Gong, Neil Zhenqiang and Liu, Bin},
	year = {2016},
	note = {arXiv: 1606.05893
ISBN: 978-1-931971-32-4},
}

@article{Jurgens2017,
	title = {Writer profiling without the writer’s text},
	issn = {16113349},
	doi = {10.1007/978-3-319-67256-4_43},
	abstract = {Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. However, they have no control over the language in incoming communications. We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user's gender, age, religion, diet, and even personality traits. Moreover, we show that directed communication is even more predictive of a user's profile than the user's own language. We then conduct a nuanced analysis of what types of social relation-ships are most predictive of users' attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming commu-nications to preserve their anonymity.},
	journal = {International Conference on Social Informatics},
	author = {Jurgens, David and Tsvetkov, Yulia and Jurafsky, Dan},
	year = {2017},
	note = {ISBN: 9783319672557},
	pages = {537--558},
}

@article{Ciot2013,
	title = {Gender {Inference} of {Twitter} {Users} in {Non}-{English} {Contexts}},
	abstract = {While much work has considered the problem of latent attribute inference for users of social media such as Twitter, little has been done on non-English-based content and users. Here, we conduct the first assessment of latent at- tribute inference in languages beyond English, focusing on gender inference. We find that the gender inference problem in quite diverse languages can be addressed using existing ma- chinery. Further, accuracy gains can be made by taking language-specific features into ac- count. We identify languages with complex orthography, such as Japanese, as difficult for existing methods, suggesting a valuable direc- tion for future research.},
	number = {October},
	journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	author = {Ciot, Morgane and Sonderegger, Morgan and Ruths, Derek},
	year = {2013},
	note = {ISBN: 9781937284978},
	pages = {1136--1145},
}

@article{Durme2012,
	title = {Streaming analysis of discourse participants},
	url = {http://dl.acm.org/citation.cfm?id=2390955},
	abstract = {Inferring attributes of discourse participants has been treated as a batch-processing task: data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest. Given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as a streaming challenge. We show that under certain common formulations, the batch-processing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classification. Once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efficient streaming classification. © 2012 Association for Computational Linguistics.},
	number = {July},
	journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	author = {Durme, Benjamin Van},
	year = {2012},
	note = {ISBN: 9781937284435},
	pages = {48--58},
}

@article{archer_private_2007,
	title = {The private life of the social agent. {What} difference does it make?},
	journal = {Critical Realism. The difference it makes.},
	author = {Archer, Margaret},
	year = {2007},
	keywords = {Archer, CONTEXT},
	pages = {18--29},
}

@article{Zhao2017,
	title = {Men {Also} {Like} {Shopping}: {Reducing} {Gender} {Bias} {Amplification} using {Corpus}-level {Constraints}},
	url = {http://arxiv.org/abs/1707.09457},
	abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2017},
	note = {arXiv: 1707.09457},
	pages = {2979--2989},
}

@article{doi:10.1300/J082v41n01\_06,
	title = {Heterosexual {Identity} and {Heterosexism}},
	volume = {41},
	url = {https://doi.org/10.1300/J082v41n01_06},
	doi = {10.1300/J082v41n01_06},
	number = {1},
	journal = {Journal of Homosexuality},
	author = {Simoni, Jane M and Walters, Karina L},
	year = {2001},
	note = {Publisher: Routledge},
	pages = {157--172},
}

@article{Kiesling2001,
	title = {Stances of {Whiteness} and {Hegemony} in {Fraternity} {Men}'s {Discourse}},
	volume = {11},
	issn = {1055-1360},
	url = {http://doi.wiley.com/10.1525/jlin.2001.11.1.101},
	doi = {10.1525/jlin.2001.11.1.101},
	abstract = {In this article I explore how a group of White men display and reaffirm their hegemonic social position through in-group discourse. The men do not baldly assert such positions; rather, they create local relationships by indexing elements of wider racial ideologies and cultural models, thus reinforcing these ideologies and models. This view of "doing Whiteness" suggests that identities of all types are performed less directly (both ideologically and indexically) than often thought.},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {Kiesling, Scott},
	year = {2001},
	note = {ISBN: 1548-1395},
	pages = {101--115},
}

@article{Hughey2010,
	title = {The (dis)similarities of white racial identities: {The} conceptual framework of 'hegemonic whiteness'},
	volume = {33},
	issn = {01419870},
	doi = {10.1080/01419870903125069},
	abstract = {This article examines the processes of white racial identity formation in the United States via an examination of a white nationalist organization and a white antiracist organization. Findings indicate that the construc- tion of white racial identity in both groups is based on the reproduction of various racist and essentialist ideologies. The realization that there is a shared ‘groupness’ to outwardly different white identities has the potential to destabilize the recent trend that over-emphasizes white heterogeneity at the expense of discussion of power, racism and discrimination. As a resolution to this analytic dilemma, this article advances a conceptual framework entitled ‘hegemonic whiteness’. White identity formation is thereby understood as a cultural process in which (1) racist, reactionary and essentialist ideologies are used to demarcate inter-racial boundaries, and (2) performances of white racial identity that fail to meet those ideals are marginalized and stigmatized, thereby creating intra-racial distinctions within the category ‘white’. Keywords:},
	number = {8},
	journal = {Ethnic and Racial Studies},
	author = {Hughey, Matthew W.},
	year = {2010},
	note = {ISBN: 0141-9870},
	keywords = {Antiracism, Hegemony, Ideology, Racism, White racial identity, White supremacy},
	pages = {1289--1309},
}

@article{walton_part_2011,
	title = {{PART} 4 {Stance} : {Ideological} {Position} {Taking} and {Social} {Categorization} {Chapter} 10 “ {Stuff} {White} {People} {Like} ”: {Stance} , {Class} , {Race} , and {Internet} {Commentary}},
	author = {Walton, Shana and Jaffe, Alexandra},
	year = {2011},
	pages = {287--301},
}

@article{walton2011stuff,
	title = {“{Stuff} {White} {People} {Like}”: {Stance}, {Class}, {Race}, and {Internet} {Commentary}},
	journal = {Digital discourse: Language in the new media},
	author = {Walton, Shana and Jaffe, Alexandra},
	year = {2011},
	note = {Publisher: Oxford University Press Oxford},
	pages = {199--219},
}

@article{mcelhinny2001see,
	title = {See {No} {Evil}, {Speak} {No} {Evil}: {White} {Police} {Officers}' {Talk} about {Race} and {Affirmative} {Action}},
	volume = {11},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {McElhinny, Bonnie},
	year = {2001},
	note = {Publisher: Wiley Online Library},
	pages = {65--78},
}

@article{schildkraut_defining_2007,
	title = {Defining {American} identity in the 21st {Century}: {How} much “there” is there},
	volume = {69},
	issn = {0022-3816},
	doi = {10.1111/j.1468-2508.2007.00562.x},
	abstract = {This study examines whether the increasing ethnic diversity of the United States is changing how the normative content of American identity is defined. It relies on a wide-ranging set of norms to test the claim that an increas- ingly multicultural America will engender a multicreedal America. In addressing this claim, the study provides an empirical assessment of the “multiple traditions” theory and develops more accurate measures of how Americans view the content of American identity than has typically been included in public opinion research. The results confirm the multiple traditions perspective, showing that a broad range of constitutive norms define being Ameri- can. A complex and contradictory set of norms exist, and it is difficult to reduce them into a single measure of “Americanism.” The results further show that most Americans, regardless of their ethnic or immigrant background, share this complex view of the norms that constitute American identity, though there are signs of divergence to monitor.},
	number = {3},
	journal = {Journal of Politics},
	author = {Schildkraut, Deborah J},
	year = {2007},
	pages = {597--615},
}

@article{leonardo_souls_2002,
	title = {The {Souls} of {White} {Folk}: critical pedagogy, whiteness studies, and globalization discourse},
	volume = {5},
	number = {1},
	journal = {Race, Ethnicity and Education},
	author = {Leonardo, Zeus},
	year = {2002},
	pages = {29--50},
}

@article{kuklick_myth_1972,
	title = {Myth and {Symbol} in {American} {Studies}},
	volume = {24},
	issn = {0003-0678},
	doi = {10.2307/2711683},
	number = {4},
	journal = {American Quarterly},
	author = {Kuklick, Bruce},
	year = {1972},
	note = {ISBN: 0003-0678},
	keywords = {myth-and-symbol school, theories of American studies},
	pages = {435--450},
}

@book{holland1987cultural,
	title = {Cultural models in language and thought},
	publisher = {Cambridge University Press},
	author = {Holland, Dorothy and Quinn, Naomi},
	year = {1987},
}

@incollection{Hewstone1997,
	address = {London},
	title = {Social {Groups} and {Social} {Stereotypes}},
	isbn = {978-1-349-25582-5},
	url = {http://dx.doi.org/10.1007/978-1-349-25582-5_22},
	booktitle = {Sociolinguistics: {A} {Reader}},
	publisher = {Macmillan Education UK},
	author = {Hewstone, Miles and Giles, Howard},
	editor = {Coupland, Nikolas and Jaworski, Adam},
	year = {1997},
	doi = {10.1007/978-1-349-25582-5_22},
	pages = {270--283},
}

@article{huynh_boundaries_2015,
	title = {Boundaries of {American} {Identity}: {Relations} {Between} {Ethnic} {Group} {Prototypicality} and {Policy} {Attitudes}},
	volume = {36},
	issn = {14679221},
	doi = {10.1111/pops.12189},
	abstract = {We sought to document that the extent to which different ethnic groups are perceived as embodying the American identity is more strongly linked to antiminority policy attitudes and acculturation ideologies among majority-group members (European Americans) than among minority-group members (Asian Americans or Latino/as). Participants rated 13 attributes of the American identity as they pertain to different ethnic groups and reported their endorsement of policy attitudes and acculturation ideologies. We found a relative consensus across ethnic groups regarding defining components of the American identity. However, European Americans were perceived as more prototypical of this American identity than ethnic minorities, especially by European American raters. Moreover, for European Americans but not for ethnic minorities, relative ingroup prototypicality was related to antiminority policy attitudes and acculturation ideologies. These findings suggest that for European Americans, perceptions of ethnic group prototypicality fulfill an instrumental function linked to preserving their group interests and limiting the rights afforded to ethnic minorities. Reprinted by permission of Blackwell Publishers},
	number = {4},
	journal = {Political Psychology},
	author = {Huynh, Que Lam and Devos, Thierry and Altman, Hannah R.},
	year = {2015},
	keywords = {Acculturation ideologies, American identity, Ethnic group prototypicality, Policy attitudes, Relative ingroup prototypicality},
	pages = {449--468},
}

@article{greenwald_measuring_1998,
	title = {Measuring individual differences in implicit cognition: the implicit association test.},
	volume = {74},
	issn = {0022-3514},
	url = {http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&DbFrom=pubmed&Cmd=Link&LinkName=pubmed_pubmed&LinkReadableName=Related Articles&IdsFromResult=9654756&ordinalpos=3&itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum},
	doi = {10.1037/0022-3514.74.6.1464},
	abstract = {An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect \&amp; pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects).},
	number = {6},
	journal = {Journal of personality and social psychology},
	author = {Greenwald, A G and McGhee, D E and Schwartz, J L},
	year = {1998},
	pmid = {9654756},
	note = {ISBN: 0022-3514{\textbackslash}r1939-1315},
	keywords = {Asian Americans, Attitude, Ethnic Groups, European Continental Ancestry Group, Female, Humans, Japan, Korea, Logistic Models, Male, Psychometrics, Reaction Time, Reproducibility of Results, Self Concept, Stereotyping, Washington, Word Association Tests},
	pages = {1464--1480},
}

@article{faflik_myth_2009,
	title = {Myth , {Symbol} , and {American} {Studies} {Methodology} : {The} {Post}-{National} {Persistence} of the {Humanities}},
	volume = {54},
	number = {2},
	journal = {Amerikastudien / American Studies},
	author = {Faflik, David},
	year = {2009},
	pages = {229--247},
}

@article{eckert_constructing_2002,
	title = {Constructing meaning in sociolinguistic variation},
	number = {November},
	journal = {Annual Meeting of the American Anthropological Association},
	author = {Eckert, Penelope},
	year = {2002},
}

@book{bucholtz2010white,
	title = {White {Kids}: {Language}, {Race}, and {Styles} of {Youth} {Identity}},
	isbn = {978-1-139-49509-7},
	url = {https://books.google.com/books?id=mtqrQIzIM4wC},
	publisher = {Cambridge University Press},
	author = {Bucholtz, M},
	year = {2010},
}

@article{citrin_american_1990,
	title = {American {Identity} and the {Politics} of {Ethnic} {Change}},
	volume = {52},
	issn = {0022-3816},
	url = {http://links.jstor.org/sici?sici=0022-3816(199011)52:4<1124:AIATPO>2.0.CO;2-5\nhttp://www.jstor.org/sici?sici=0022-3816(1990)52:4<1124:AIATPO>2.0.CO;2-K&origin=serialsolutions},
	doi = {10.2307/2131685},
	abstract = {This article explores the way subjective conceptions of national identity influence the mass public's reactions to the changing ethnic composition of American society. Using the symbolic politics model of opinion formation to analyze survey data collected in early 1988, the article demonstrates that normative beliefs about Americanism strongly influence general attitudes toward cultural minorities and, when the appropriate symbolic cues are present, policy preferences on ethnic issues.},
	number = {4},
	journal = {Journal of Politics},
	author = {Citrin, Jack and Reingold, Beth and Green, Donald P},
	year = {1990},
	note = {ISBN: 0022-3816},
	pages = {1124--1154},
}

@article{baker_why_2013,
	title = {‘{Why} do white people have thin lips?’ {Google} and the perpetuation of stereotypes via auto-complete search forms},
	volume = {10},
	issn = {1740-5904},
	url = {http://dx.doi.org/10.1080/17405904.2012.744320},
	doi = {10.1080/17405904.2012.744320},
	abstract = {This study highlights how the auto-complete search algorithm offered by the search tool Google can produce suggested terms which could be viewed as racist, sexist or homophobic. Google was interrogated by entering different combinations of question words and identity terms such as ?why are blacks?? in order to elicit auto-completed questions. Two thousand, six hundred and ninety questions were elicited and then categorised according to the qualities they referenced. Certain identity groups were found to attract particular stereotypes or qualities. For example, Muslims and Jewish people were linked to questions about aspects of their appearance or behaviour, while white people were linked to questions about their sexual attitudes. Gay and black identities appeared to attract higher numbers of questions that were negatively stereotyping. The article concludes by questioning the extent to which such algorithms inadvertently help to perpetuate negative stereotypes. This study highlights how the auto-complete search algorithm offered by the search tool Google can produce suggested terms which could be viewed as racist, sexist or homophobic. Google was interrogated by entering different combinations of question words and identity terms such as ?why are blacks?? in order to elicit auto-completed questions. Two thousand, six hundred and ninety questions were elicited and then categorised according to the qualities they referenced. Certain identity groups were found to attract particular stereotypes or qualities. For example, Muslims and Jewish people were linked to questions about aspects of their appearance or behaviour, while white people were linked to questions about their sexual attitudes. Gay and black identities appeared to attract higher numbers of questions that were negatively stereotyping. The article concludes by questioning the extent to which such algorithms inadvertently help to perpetuate negative stereotypes.},
	number = {May 2015},
	journal = {Critical Discourse Studies},
	author = {Baker, Paul and Potts, Amanda},
	year = {2013},
	pages = {187--204},
}

@article{blommaert_sociolinguistics_2007,
	title = {Sociolinguistics and {Discourse} {Analysis}: {Orders} of {Indexicality} and {Polycentricity}},
	volume = {2},
	issn = {1744-7143},
	url = {http://dx.doi.org/10.2167/md089.0},
	doi = {10.2167/md089.0},
	abstract = {Developments in the structure of societies, such as globalisation processes, compel us to devote more attention to issues of sociolinguistic variation in discourse, because features of such variation become ever more important to users. Yet a lot of discourse analysis starts from an old monolingual ideal, in which the sociolinguistic dimensions of the linguistic resources used by people remains underproblematised. This paper argues that our discourse analytic toolkit needs to be complemented with some seriously useful sociolinguistic tools and presents two such tools in this paper: orders of indexicality and polycentricity. Both concepts are designed to observe forms of linguistic and cultural variation that characterise Late Modern diasporic environments.},
	number = {2},
	journal = {Journal of Multicultural Discourses},
	author = {Blommaert, Jan},
	year = {2007},
	note = {ISBN: 1744-7143},
	keywords = {analysis, discourse, globalisation, late modernity, multilingualism, orders of indexicality, polycentricity, sociolinguistics, variation},
	pages = {115--130},
}

@book{connell2005masculinities,
	title = {Masculinities},
	isbn = {978-0-7456-3427-2},
	url = {https://books.google.com/books?id=YuR2uFxxvPoC},
	publisher = {Polity Press},
	author = {Connell, R W},
	year = {2005},
}

@article{Volkova2014,
	title = {Twitter {Data} {Collection} : {Crawling} {Users} , {Neighbors} and {Their} {Communication} for {Personal} {Attribute} {Prediction} in {Social} {Media}},
	author = {Volkova, Svitlana},
	year = {2014},
}

@article{Hart2015,
	title = {Youth {Intimacy} on {Tumblr}},
	volume = {23},
	issn = {1103-3088},
	url = {http://journals.sagepub.com/doi/10.1177/1103308815577878},
	doi = {10.1177/1103308815577878},
	abstract = {I examine young people’s intimate relationships formed on Tumblr. Research has extensively documented how and why adults utilize online dating services to articulate relationships online; however, scholars of this phenomenon have largely overlooked the lived experiences of young people. This article presents the initial findings of a qualitative pilot study in which 10 young male and female Tumblr users participated in hour-long, in-depth, semi-structured, synchronous online interviews via Skype. The initial findings generated by the pilot study suggest that young people are engaging online technologies to practice intimacy and sociality in diverse ways. Responses suggest that young people conceptualize Tumblr as being distinct from existing social network sites (SNS) as a result of its perceived affordances. Iconclude that the ways in which young people engage and date and socialize on Tumblr suggest a rethinking of established contemporary notions of intimacy and community in the digital era.},
	number = {3},
	journal = {Young},
	author = {Hart, Matt},
	year = {2015},
	note = {ISBN: 1103308815},
	keywords = {everyday life, internet, intimacy, online dating, relationships, tumblr, young people},
	pages = {193--208},
}

@inproceedings{Zamal2012,
	title = {Homophily and {Latent} {Attribute} {Inference} : {Inferring} {Latent} {Attributes} of {Twitter} {Users} from {Neighbors}},
	isbn = {978-1-57735-556-4},
	abstract = {In this paper, we extend existing work on latent attribute inference by leveraging the principle of homophily: we evaluate the inference accuracy gained by augmenting the user features with features derived from the Twitter profiles and postings of her friends. We consider three attributes which have varying degrees of assortativity: gender, age, and political affiliation. Our approach yields a significant and robust increase in accuracy for both age and political affiliation, indicating that our approach boosts performance for attributes with moderate to high assortativity. Furthermore, different neighborhood subsets yielded optimal performance for different attributes, suggesting that different subsamples of the user’s neighborhood characterize different aspects of the user herself. Finally, inferences using only the features of a user’s neighbors outperformed those based on the user’s features alone. This suggests that the neighborhood context alone carries substantial information about the user.},
	booktitle = {{ICWSM}},
	author = {Zamal, Faiyaz Al and Liu, Wendy and Ruths, Derek},
	year = {2012},
	keywords = {Short Papers},
	pages = {387--390},
}

@article{gong2014joint,
	title = {Joint link prediction and attribute inference using a social-attribute network},
	volume = {5},
	number = {2},
	journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
	author = {Gong, Neil Zhenqiang and Talwalkar, Ameet and Mackey, Lester and Huang, Ling and Shin, Eui Chul Richard and Stefanov, Emil and Shi, Elaine Runting and Song, Dawn},
	year = {2014},
	note = {Publisher: ACM},
	pages = {27},
}

@article{thomas_multi-party_2010,
	title = {Multi-party privacy risks in social networks},
	url = {https://www.ideals.illinois.edu/handle/2142/16498},
	journal = {Privacy Enhancing Technologies},
	author = {Thomas, KA},
	year = {2010},
	pages = {236--252},
}

@article{leskovec_learning_2012,
	title = {Learning to discover social circles in ego networks},
	issn = {15564681},
	url = {http://papers.nips.cc/paper/4532-learning-to-discover-social-circles-in-ego-networks},
	doi = {10.1145/0000000.0000000},
	abstract = {Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. ‘circles’ on Google+, and ‘lists’ on Facebook and Twitter), however they are laborious to construct and must be updated when- ever a user’s network grows. We define a novel machine learning task of identi- fying users’ social circles. We pose the problem as a node clustering problem on a user’s ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user pro- file information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth},
	journal = {Advances in neural information processing …},
	author = {Leskovec, Jure and Mcauley, Jj},
	year = {2012},
	pmid = {1000285845},
	note = {arXiv: 1210.8182
ISBN: 9781627480031},
	pages = {1--9},
}

@article{volkova_inferring_2014,
	title = {Inferring {User} {Political} {Preferences} from {Streaming} {Communications}},
	abstract = {Existing models for social media per- sonal analytics assume access to thou- sands of messages per user, even though most users author content only sporadi- cally over time. Given this sparsity, we: (i) leverage content from the local neigh- borhood of a user; (ii) evaluate batch mod- els as a function of size and the amount of messages in various types of neighbor- hoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences. We show that even when limited or no self- authored data is available, language from friend, retweet and user mention commu- nications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that polit- ical preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. 1},
	journal = {Acl},
	author = {Volkova, Svitlana and Coppersmith, Glen and Durme, Benjamin Van},
	year = {2014},
	note = {ISBN: 9781937284725},
	pages = {186--196},
}

@article{Layton2013,
	title = {Automated unsupervised authorship analysis using evidence accumulation clustering},
	volume = {19},
	issn = {13513249},
	doi = {10.1017/S1351324911000313},
	abstract = {Authorship Analysis aims to extract information about the authorship of documents from features within those documents. Typically, this is performed as a classification task with the aim of identifying the author of a document, given a set of documents of known authorship. Alternatively, unsupervised methods have been developed primarily as visualisation tools to assist the manual discovery of clusters of authorship within a corpus by analysts. However, there is a need in many fields for more sophisticated unsupervised methods to automate the discovery, profiling and organisation of related information through clustering of documents by authorship. An automated and unsupervised methodology for clustering documents by authorship is proposed in this paper. The methodology is named NUANCE, for n-gram Unsupervised Automated Natural Cluster Ensemble. Testing indicates that the derived clusters have a strong correlation to the true authorship of unseen documents.},
	number = {1},
	journal = {Natural Language Engineering},
	author = {Layton, R. and Watters, P. and Dazeley, R.},
	year = {2013},
	pages = {95--120},
}

@article{Daks2016,
	title = {Unsupervised {Authorial} {Clustering} {Based} on {Syntactic} {Structure}},
	abstract = {This paper proposes a new unsupervised technique for clustering a collection of documents written by distinct individu-als into authorial components. We high-light the importance of utilizing syntactic structure to cluster documents by author, and demonstrate experimental results that show the method we outline performs on par with state-of-the-art techniques. Addi-tionally, we argue that this feature set out-performs previous methods in cases where authors consciously emulate each other's style or are otherwise rhetorically similar.},
	number = {2013},
	journal = {Proceedings of the ACL 2016 Student Research Workshop},
	author = {Daks, Alon and Clark, Aidan},
	year = {2016},
	note = {ISBN: 9781510827608},
	pages = {114--118},
}

@article{Kocher2016,
	title = {{UniNE} at {CLEF} 2016: {Author} {Clustering}},
	volume = {1609},
	issn = {16130073},
	abstract = {This paper describes and evaluates an effective unsupervised author clustering authorship linking model called SPATIUM-L1. The suggested strategy can be adapted without any problem to different languages (such as Dutch, English, and Greek) in different genres (e.g., newspaper articles and reviews). As features, we suggest using the m most frequent terms of each text (isolated words and punctuation symbols with m at most 200). Applying a simple distance measure, we determine whether there is enough indication that two texts were written by the same author. The evaluations are based on six test collections (PAN AUTHOR CLUSTERING task at CLEF 2016).},
	journal = {CEUR Workshop Proceedings},
	author = {Kocher, Mirco},
	year = {2016},
	pages = {895--902},
}

@article{Stamatatos2008,
	title = {Author identification: {Using} text sampling to handle the class imbalance problem},
	volume = {44},
	issn = {03064573},
	doi = {10.1016/j.ipm.2007.05.012},
	abstract = {Authorship analysis of electronic texts assists digital forensics and anti-terror investigation. Author identification can be seen as a single-label multi-class text categorization problem. Very often, there are extremely few training texts at least for some of the candidate authors or there is a significant variation in the text-length among the available training texts of the candidate authors. Moreover, in this task usually there is no similarity between the distribution of training and test texts over the classes, that is, a basic assumption of inductive learning does not apply. In this paper, we present methods to handle imbalanced multi-class textual datasets. The main idea is to segment the training texts into text samples according to the size of the class, thus producing a fairer classification model. Hence, minority classes can be segmented into many short samples and majority classes into less and longer samples. We explore text sampling methods in order to construct a training set according to a desirable distribution over the classes. Essentially, by text sampling we provide new synthetic data that artificially increase the training size of a class. Based on two text corpora of two languages, namely, newswire stories in English and newspaper reportage in Arabic, we present a series of authorship identification experiments on various multi-class imbalanced cases that reveal the properties of the presented methods. © 2007 Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {Information Processing and Management},
	author = {Stamatatos, Efstathios},
	year = {2008},
	note = {ISBN: 0306-4573},
	keywords = {Author identification, Class imbalance, Text categorization},
	pages = {790--799},
}

@article{Aldebei2015,
	title = {Unsupervised {Decomposition} of a {Multi}-{Author} {Document} {Based} on {Naive}-{Bayesian} {Model}},
	abstract = {This paper proposes a new unsupervised method for decomposing a multi-author document into authorial components. We assume that we do not know anything about the document and the authors, ex-cept the number of the authors of that doc-ument. The key idea is to exploit the dif-ference in the posterior probability of the Naive-Bayesian model to increase the pre-cision of the clustering assignment and the accuracy of the classification process of our method. Experimental results show that the proposed method outperforms two state-of-the-art methods.},
	number = {2011},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Aldebei, Khaled and He, Xiangjian and Yang, Jie},
	year = {2015},
	note = {ISBN: 9781941643730},
	pages = {501--505},
}

@article{Clement2003,
	title = {Ngram and {Bayesian} {Classification} of {Documents} for {Topic} and {Authorship}},
	volume = {18},
	issn = {0268-1145},
	url = {http://llc.oupjournals.org/cgi/doi/10.1093/llc/18.4.423},
	doi = {10.1093/llc/18.4.423},
	abstract = {Large, real world, data sets have been investigated in the context of Authorship Attribution of real world documents. Ngram measures can be used to accur- ately assign authorship for long documents such as novels. A number of 5 (authors) 5 (movies) arrays of movie reviews were acquired from the Internet Movie Database. Both ngram and naive Bayes classifiers were used to classify along both the authorship and topic (movie) axes. Both approaches yielded similar results, and authorship was as accurately detected, or more accurately detected, than topic. Part of speech tagging and function-word lists were used to investigate the influence of structure on classification tasks on documents with meaning removed but grammatical structure intact.},
	number = {4},
	journal = {Literary and Linguistic Computing},
	author = {Clement, Ross and Sharp, David},
	year = {2003},
	pages = {423--447},
}

@article{Koppel2011,
	title = {Unsupervised {Decomposition} of a {Document} into {Authorial} {Components}},
	url = {http://portal.acm.org/citation.cfm?id=2002640},
	abstract = {We propose a novel unsupervised method for separating out distinct authorial components of a document. In particular, we show that, given a book artificially munged from two thematically similar biblical books, we can separate out the two constituent books almost perfectly. This allows us to automatically recapitulate many conclusions reached by Bible scholars over centuries of research. One of the key elements of our method is exploitation of differences in synonym choice by different authors.},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics},
	author = {Koppel, Moshe and Akiva, Navot and Dershowitz, Idan and Dershowitz, Nachum},
	year = {2011},
	note = {ISBN: 9781932432879},
	pages = {1356--1364},
}

@article{Mosteller1964,
	title = {Inference and {Disputed} {Authorship}: {The} {Federalist}},
	volume = {58},
	number = {302},
	journal = {Journal of the American Statistical Association},
	author = {Mosteller, Frederick and Wallace, David L.},
	year = {1964},
	pages = {275--309},
}

@article{Samdani2014,
	title = {A {Discriminative} {Latent} {Variable} {Model} for {Online} {Clustering}},
	volume = {32},
	abstract = {This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L M). We present an online clustering algorithm for L 3 M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L 3 Moutperforms several existing online as well as batch supervised clustering techniques.},
	journal = {International Conference on Machine Learning},
	author = {Samdani, Rajhans and Chang, Kai-Wei and Roth, Dan},
	year = {2014},
	note = {ISBN: 9781634393973},
	pages = {1--10},
}

@article{fifield_unsupervised_2018,
	title = {Unsupervised authorship attribution},
	url = {http://arxiv.org/abs/1503.07613},
	abstract = {We describe a technique for attributing parts of a written text to a set of unknown authors. Nothing is assumed to be known a priori about the writing styles of potential authors. We use multiple independent clusterings of an input text to identify parts that are similar and dissimilar to one another. We describe algorithms necessary to combine the multiple clusterings into a meaningful output. We show results of the application of the technique on texts having multiple writing styles.},
	author = {Fifield, David and Follan, Torbjørn and Lunde, Emil},
	year = {2018},
	note = {arXiv: 1503.07613},
}

@article{Gomez-Adorno2017,
	title = {Author clustering using hierarchical {Clustering} analysis: {Notebook} for {PAN} at {CLEF} 2017},
	volume = {1866},
	issn = {16130073},
	abstract = {This paper presents our approach to the Author Clustering task at PAN 2017. We performed a hierarchical clustering analysis of different document fea-tures: typed and untyped character n-grams, and word n-grams. We experimented with two feature representation methods, log-entropy model, and tf-idf; while tuning minimum frequency threshold values to reduce the dimensionality. Our system was ranked 1 st in both subtasks, author clustering and authorship-link ranking.},
	journal = {CEUR Workshop Proceedings},
	author = {Gómez-Adorno, Helena and Aleman, Yuridiana and Vilariño, Darnes and Sanchez-Perez, Miguel A. and Pinto, David and Sidorov, Grigori},
	year = {2017},
}

@article{Iqbal2010,
	title = {Mining writeprints from anonymous e-mails for forensic investigation},
	volume = {7},
	issn = {17422876},
	doi = {10.1016/j.diin.2010.03.003},
	abstract = {Many criminals exploit the convenience of anonymity in the cyber world to conduct illegal activities. E-mail is the most commonly used medium for such activities. Extracting knowledge and information from e-mail text has become an important step for cybercrime investigation and evidence collection. Yet, it is one of the most challenging and time-consuming tasks due to special characteristics of e-mail dataset. In this paper, we focus on the problem of mining the writing styles from a collection of e-mails written by multiple anonymous authors. The general idea is to first cluster the anonymous e-mail by the stylometric features and then extract the writeprint, i.e., the unique writing style, from each cluster. We emphasize that the presented problem together with our proposed solution is different from the traditional problem of authorship identification, which assumes training data is available for building a classifier. Our proposed method is particularly useful in the initial stage of investigation, in which the investigator usually have very little information of the case and the true authors of suspicious e-mail collection. Experiments on a real-life dataset suggest that clustering by writing style is a promising approach for grouping e-mails written by the same author. © 2010 Elsevier Ltd. All rights reserved.},
	number = {1-2},
	journal = {Digital Investigation},
	author = {Iqbal, Farkhund and Binsalleeh, Hamad and Fung, Benjamin C.M. and Debbabi, Mourad},
	year = {2010},
	note = {ISBN: 1742-2876},
	keywords = {Authorship analysis, Classification, Clustering, E-mail, Forensic investigation, Stylometric features, Writeprint, Writing styles},
	pages = {56--64},
}

@article{srinivasa_rao_performance_2017,
	title = {Performance {Evaluation} of {Unsupervised} {Algorithms} on {Morpheme} based {Authorship} {Clustering}},
	volume = {3297},
	issn = {2394-1588},
	doi = {10.17148/IARJSET.2017.4808},
	abstract = {The aim of authorship attribution is to identify the author of an anonymous document. Earlier, many types of research used authorship attribution as a multi-class single labeled text classifier problem. However, in several applications, it is neither easy nor possible to find such labeled data so it is necessary to build unsupervised attribution models that are able to estimate similarities or differences in personal style of authors. The present paper experiments authorship clustering using morpheme-based N-gram on unsupervised clustering algorithms like K-means, Mini Batch K-means, and Ward Hierarchial clusterings. The performance of the clustering algorithms is evaluated using silhouette coefficient and calculated B-cubed F-score and found that K-means algorithm achieves better clustering performance on C 50 news groups data set.},
	number = {8},
	journal = {International Advanced Research Journal in Science, Engineering and Technology ISO},
	author = {Srinivasa Rao, O and Ganapathi Raju, N V and Vijaya Latha, Y and Vivek Varma, P},
	year = {2017},
	keywords = {BCubed F-score, authorship clustering, morphemes, silhouette coefficient},
	pages = {59--63},
}

@article{Bagnall2016,
	title = {Authorship clustering using multi-headed recurrent neural networks},
	volume = {1609},
	issn = {16130073},
	abstract = {A recurrent neural network that has been trained to separately model the language of several documents by unknown authors is used to measure similarity between the documents. It is able to find clues of common authorship even when the documents are very short and about disparate topics. While it is easy to make statistically significant predictions regarding authorship, it is difficult to group documents into definite clusters with high accuracy.},
	journal = {CEUR Workshop Proceedings},
	author = {Bagnall, Douglas},
	year = {2016},
	note = {arXiv: 1608.04485},
	pages = {791--804},
}

@article{Hitschler2017,
	title = {Authorship {Attribution} with {Convolutional} {Neural} {Networks} and {POS}-{Eliding}},
	url = {http://www.aclweb.org/anthology/W17-4907},
	abstract = {We use a convolutional neural network to perform authorship identification on a very homogeneous dataset of scientific publica-tions. In order to investigate the effect of domain biases, we obscure words below a certain frequency threshold, retaining only their POS-tags. This procedure improves test performance due to better generaliza-tion on unseen data. Using our method, we are able to predict the authors of scien-tific publications in the same discipline at levels well above chance.},
	number = {2013},
	journal = {Proceeedings of the Workshop on Stylistic Variation},
	author = {Hitschler, Julian and Berg, Esther Van Den and Rehbein, Ines},
	year = {2017},
	pages = {53--58},
}

@article{layton_recentred_2012,
	title = {Recentred local profiles for authorship attribution},
	volume = {18},
	issn = {13513249},
	doi = {10.1017/S1351324911000180},
	number = {3},
	journal = {Natural Language Engineering},
	author = {Layton, Robert and Watters, Paul and Dazeley, Richard},
	year = {2012},
	note = {ISBN: 1351-3249},
	pages = {293--312},
}

@book{Griffin2017,
	title = {A {Dictionary} of {Gender} {Studies}},
	publisher = {Oxford University Press},
	author = {Griffin, Gabriele},
	year = {2017},
	doi = {10.1093/acref/9780191834837.001.0001},
}

@inproceedings{Burger2011,
	title = {Discriminating {Gender} on {Twitter}},
	volume = {146},
	isbn = {978-1-937284-11-4},
	url = {http://www.mitre.org/work/tech_papers/2011/11_0170/},
	doi = {10.1007/s00256-005-0933-8},
	abstract = {Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal in- vestigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classifier types on this dataset. We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. We also perform a large-scale human assessment us- ing Amazon Mechanical Turk. Our methods signifi- cantly out-perform both baseline models and almost all humans on the same task.},
	booktitle = {Association for {Computational} {Linguistics}},
	author = {Burger, John D and Henderson, John and Kim, George and Zarrella, Guido},
	year = {2011},
	pmid = {15999282},
	note = {arXiv: 1011.1669v3
ISSN: 0364-2348},
	pages = {1301--1309},
}

@article{altenburger_monophily_2018,
	title = {Monophily in social networks introduces similarity among friends-of-friends},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0321-8},
	doi = {10.1038/s41562-018-0321-8},
	abstract = {The observation that individuals tend to be friends with people who are similar to themselves, commonly known as homophily, is a prominent feature of social networks. While homophily describes a bias in attribute preferences for similar others, it gives limited attention to variability. Here, we observe that attribute preferences can exhibit variation beyond what can be explained by homophily. We call this excess variation monophily to describe the presence of individuals with extreme preferences for a particular attribute possibly unrelated to their own attribute. We observe that monophily can induce a similarity among friends-of-friends without requiring any similarity among friends. To simulate homophily and monophily in synthetic networks, we propose an overdispersed extension of the classical stochastic block model. We use this model to demonstrate how homophily-based methods for predicting attributes on social networks based on friends (that is, 'the company you keep') are fundamentally different from monophily-based methods based on friends-of-friends (that is, 'the company you’re kept in'). We place particular focus on predicting gender, where homophily can be weak or non-existent in practice. These findings offer an alternative perspective on network structure and prediction, complicating the already difficult task of protecting privacy on social networks.},
	journal = {Nature Human Behaviour 2018},
	author = {Altenburger, Kristen M. and Ugander, Johan},
	year = {2018},
	note = {Publisher: Springer US},
	pages = {1},
}

@article{Koppel2009,
	title = {Computational {Methods} in {Authorship} {Attribution}},
	volume = {60},
	issn = {19335954},
	doi = {10.1002/asi},
	number = {1},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	year = {2009},
	pmid = {502955140},
	note = {arXiv: 0803.1716
ISBN: 9783848215430},
	pages = {9--26},
}

@article{Koppel2013,
	title = {Authorship {Attribution}: {What}'s {Easy} and {What}'s {Hard}?},
	volume = {39},
	url = {https://litigation-essentials.lexisnexis.com/webcd/app?action=DocumentDisplay&crawlid=1&doctype=cite&docid=21+J.L.+&+Pol'y+317&srctype=smi&srcid=3B15&key=65613cadac2053fa3c914912f421651c},
	abstract = {This paper considers four versions of the authorship attribution problem that are typically encountered in the forensic context and offers algorithmic solutions for each. Part I describes the simple authorship attribution problem described above. Part II considers the long-text verification problem, in which we are asked if two long texts are by the same author. Part III discusses the many-candidates problem, in which we are asked which among thousands of candidate authors is the author of a given text. Finally, Part IV considers the fundamental problem of authorship attribution, in which we are asked if two short texts are by the same author. Although other researchers have considered these problems, here we offer our own solutions to each problem and indicate the degree of accuracy that can be expected in each case under specified conditions.},
	number = {2006},
	journal = {The Journal of Law, Economics \& Policy},
	author = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	year = {2013},
	pages = {317--331},
}

@article{Stamatatos2016,
	title = {Clustering by authorship within and across documents},
	volume = {1609},
	issn = {16130073},
	doi = {10.1007/s00256-005-0933-8},
	abstract = {The vast majority of previous studies in authorship attribution assume the existence of documents (or parts of documents) labeled by authorship to be used as training instances in either closed-set or open-set attribution. However, in several applications it is not easy or even possible to find such labeled data and it is necessary to build unsupervised attribution models that are able to estimate similarities/differences in personal style of authors. The shared tasks on author clustering and author diarization at PAN 2016 focus on such unsupervised authorship attribution problems. The former deals with single-author documents and aims at grouping documents by authorship and establishing authorship links between documents. The latter considers multi-author documents and attempts to segment a document into authorial components, a task strongly associated with intrinsic plagiarism detection. This paper presents an overview of the two tasks including evaluation datasets, measures, results, as well as a survey of a total of 10 submissions (8 for author clustering and 2 for author diarization).},
	journal = {CEUR Workshop Proceedings},
	author = {Stamatatos, Efstathios and Tschuggnall, Michael and Verhoeven, Ben and Daelemans, Walter and Specht, Gönther and Stein, Benno and Potthast, Martin},
	year = {2016},
	pmid = {20879418},
	note = {arXiv: 1690219.1690245‎
ISBN: 9781450308137},
	pages = {691--715},
}

@article{Shrestha2017,
	title = {Convolutional {Neural} {Networks} for {Authorship} {Attribution} of {Short} {Texts}},
	volume = {2},
	url = {http://www.aclweb.org/anthology/E17-2106},
	abstract = {We present a model to perform author-ship attribution of tweets using Convolu-tional Neural Networks (CNNs) over char-acter n-grams. We also present a strategy that improves model interpretability by es-timating the importance of input text frag-ments in the predicted classification. The experimental evaluation shows that text CNNs perform competitively and are able to outperform previous methods.},
	journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
	author = {Shrestha, Prasha and Sierra, Sebastian and González, Fabio A and Rosso, Paolo and Montes-Y-Gómez, Manuel and Solorio, Thamar},
	year = {2017},
	note = {ISBN: 9781510838604},
	keywords = {Authorship attribution, Classification (of information), Computational lin, Convolution, Convolutional neural netw},
	pages = {669--674},
}

@article{tovares_going_2016,
	title = {Going off-script and reframing the frame: {The} dialogic intertwining of the centripetal and centrifugal voices in the {Truth} and {Reconciliation} {Commission} hearings},
	volume = {27},
	issn = {14603624},
	doi = {10.1177/0957926516651365},
	abstract = {After the end of apartheid, the South African Truth and Reconciliation Commission (TRC) was established to uncover the truth and, most importantly, unite a deeply divided nation. This overarching goal of reconciliation not only framed the TRC's work, but also shaped each hearing so that it followed a somewhat predictable script. In this article, bringing together Goffman's framing and Bakhtin's dialogicality, I analyze two selected hearings to show how the testifiers and the TRC commissioners go off-script and thereby reframe participants' relationships in terms of power and solidarity, the hearing as a social situation, and understandings of the past and the truth. Suggesting that the TRC officials represent centripetal (official) voices and the testifiers represent centrifugal (marginal) voices, I show how going off-script works to not only reframe and laminate frames in a highly organized institutional encounter, but also is a way of intertwining the centripetal and centrifugal voices, thus creating dialogicality.},
	number = {5},
	journal = {Discourse and Society},
	author = {Tovares, Alla V.},
	year = {2016},
	note = {ISBN: 0957926516651},
	keywords = {Centripetal and centrifugal forces, dialogicality, framing, going off-script, heteroglossia, the TRC discourse, voice},
	pages = {554--573},
}

@article{sari_continuous_2017,
	title = {Continuous {N}-gram {Representations} for {Authorship} {Attribution}},
	volume = {2},
	abstract = {This paper presents work on using con-tinuous representations for authorship at-tribution. In contrast to previous work, which uses discrete feature representa-tions, our model learns continuous repre-sentations for n-gram features via a neu-ral network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while pro-ducing comparable results on the remain-ing two.},
	journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	author = {Sari, Yunita and Vlachos, Andreas and Stevenson, Mark},
	year = {2017},
	note = {ISBN: 9781510838604},
	keywords = {Authorship attribution, Classification (of information), Computational linguistics, Feature representation, Linguistics, N},
	pages = {267--273},
}

@article{gomez-adorno_document_2018,
	title = {Document embeddings learned on various types of n-grams for cross-topic authorship attribution},
	volume = {100},
	issn = {0010485X},
	url = {https://doi.org/10.1007/s00607-018-0587-8},
	doi = {10.1007/s00607-018-0587-8},
	abstract = {Recently, document embeddings methods have been proposed aiming at capturing hidden properties of the texts. These methods allow to represent documents in terms of fixed-length, continuous and dense feature vectors. In this paper, we propose to learn document vectors based on n-grams and not only on words. We use the recently proposed Paragraph Vector method. These n-grams include character n-grams, word n-grams and n-grams of POS tags (in all cases with n varying from 1 to 5). We considered the task of Cross-Topic Authorship Attribution and made experiments on The Guardian corpus. Experimental results show that our method outperforms word-based embeddings and character n-gram based linear models, which are among the most effective approaches for identifying the writing style of an author. © 2018 Springer-Verlag GmbH Austria, part of Springer Nature},
	number = {7},
	journal = {Computing},
	author = {Gómez-Adorno, Helena and Posadas-Durán, Juan Pablo and Sidorov, Grigori and Pinto, David},
	year = {2018},
	note = {Publisher: Springer Vienna
ISBN: 0060701805878},
	keywords = {Authorship attribution, Doc2vec, Document embeddings, Neural networks, n-Grams},
	pages = {1--16},
}

@article{stamatatos_authorship_2017,
	title = {Authorship {Attribution} {Using} {Text} {Distortion}},
	volume = {1},
	doi = {10.18653/v1/E17-1107},
	abstract = {Authorship attribution is associated with important applications in forensics and hu- manities research. A crucial point in this field is to quantify the personal style of writing, ideally in a way that is not af- fected by changes in topic or genre. In this paper, we present a novel method that en- hances authorship attribution effectiveness by introducing a text distortion step be- fore extracting stylometric measures. The proposed method attempts to mask topic- specific information that is not related to the personal style of authors. Based on ex- periments on two main tasks in authorship attribution, closed-set attribution and au- thorship verification, we demonstrate that the proposed approach can enhance exist- ing methods especially under cross-topic conditions, where the training and test cor- pora do not match in topic.},
	journal = {15th Conference of the European Chapter of the Association for Computational Linguistics},
	author = {Stamatatos, Efstathios},
	year = {2017},
	note = {ISBN: 9781510838604},
	pages = {1138--1149},
}

@inproceedings{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	isbn = {0147-006X (Print)},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.1146/annurev.neuro.26.041002.131047},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	booktitle = {{ICLR} 2015},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
	pmid = {14527267},
	note = {arXiv: 1409.0473
ISSN: 0147-006X},
	pages = {1--15},
}

@article{clark_neural_2018,
	title = {Neural {Text} {Generation} in {Stories} {Using} {Entity} {Representations} as {Context}},
	volume = {1},
	url = {http://aclweb.org/anthology/N18-1204},
	doi = {10.18653/v1/N18-1204},
	abstract = {We introduce an approach to neural text gen-eration that explicitly represents entities men-tioned in the text. Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evalu-ation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further re-search.},
	journal = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	author = {Clark, Elizabeth and Ji, Yangfeng and Smith, Noah A.},
	year = {2018},
	pages = {2250--2260},
}

@article{haimson_disclosure_2015,
	title = {Disclosure, {Stress}, and {Support} {During} {Gender} {Transition} on {Facebook}},
	url = {http://dl.acm.org/citation.cfm?doid=2675133.2675152},
	doi = {10.1145/2675133.2675152},
	abstract = {Social computing technologies, such as social networking sites (SNSs), often privilege people who fit within expected, static categories. Thus, users embarking on major identity changes, such as gender transition, often encounter stress when using SNSs to interact with their online social networks. To address this problem and reflect on the design of SNSs and other social computing systems, we present the results of a comprehensive online survey of transgender and gender non-conforming SNS users. Our findings indicate that although Facebook can be a stressful place for gender transition due to difficulties of transition disclosure, support from one’s Facebook network can help to mitigate some of this stress. We examine Facebook both as a site of stress and as a site of support. Better understanding the relationships between stress, disclosure, and support on SNSs for these particular users can inform technology design that will benefit people who struggle with navigating a wide range of major identity changes online. Author},
	journal = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing - CSCW '15},
	author = {Haimson, Oliver L. and Brubaker, Jed R. and Dombrowski, Lynn and Hayes, Gillian R.},
	year = {2015},
	note = {ISBN: 9781450329224},
	pages = {1176--1190},
}

@book{martin2007language,
	title = {The {Language} of {Evaluation}: {Appraisal} in {English}},
	isbn = {978-0-230-51191-0},
	publisher = {Palgrave Macmillan UK},
	author = {Martin, J and White, P R R},
	year = {2005},
}

@inproceedings{pryzant_deconfounded_2018,
	title = {Deconfounded {Lexicon} {Induction} for {Interpretable} {Social} {Science}},
	booktitle = {{NAACL}},
	author = {Pryzant, Reid and Shen, Kelly and Jurafsky, Dan and Wager, Stefan},
	year = {2018},
}

@article{howard_fine-tuned_2018,
	title = {Fine-tuned {Language} {Models} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Transfer learning has revolutionized computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24\% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.},
	author = {Howard, Jeremy and Ruder, Sebastian},
	year = {2018},
	note = {arXiv: 1801.06146},
}

@article{bamman_unsupervised_2014,
	title = {Unsupervised {Discovery} of {Biographical} {Structure} from {Text}},
	volume = {2},
	abstract = {We present a method for discovering abstract event classes in biographies, based on a prob-abilistic latent-variable model. Taking as in-put timestamped text, we exploit latent corre-lations among events to learn a set of event classes (such as BORN, GRADUATES HIGH SCHOOL, and BECOMES CITIZEN), along with the typical times in a person's life when those events occur. In a quantitative evalua-tion at the task of predicting a person's age for a given event, we find that our genera-tive model outperforms a strong linear regres-sion baseline, along with simpler variants of the model that ablate some features. The ab-stract event classes that we learn allow us to perform a large-scale analysis of 242,970 Wikipedia biographies. Though it is known that women are greatly underrepresented on Wikipedia—not only as editors (Wikipedia, 2011) but also as subjects of articles (Reagle and Rhue, 2011)—we find that there is a bias in their characterization as well, with biogra-phies of women containing significantly more emphasis on events of marriage and divorce than biographies of men.},
	journal = {Transactions of the ACL},
	author = {Bamman, David and Smith, Noah A},
	year = {2014},
	pages = {363--376},
}

@article{massey_annotating_2015,
	title = {Annotating {Character} {Relationships} in {Literary} {Texts}},
	url = {http://arxiv.org/abs/1512.00728},
	abstract = {We present a dataset of manually annotated relationships between characters in literary texts, in order to support the training and evaluation of automatic methods for relation type prediction in this domain (Makazhanov et al., 2014; Kokkinakis, 2013) and the broader computational analysis of literary character (Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and Gurevych, 2015). In this work, we solicit annotations from workers on Amazon Mechanical Turk for 109 texts ranging from Homer's \_Iliad\_ to Joyce's \_Ulysses\_ on four dimensions of interest: for a given pair of characters, we collect judgments as to the coarse-grained category (professional, social, familial), fine-grained category (friend, lover, parent, rival, employer), and affinity (positive, negative, neutral) that describes their primary relationship in a text. We do not assume that this relationship is static; we also collect judgments as to whether it changes at any point in the course of the text.},
	author = {Massey, Philip and Xia, Patrick and Bamman, David and Smith, Noah A.},
	year = {2015},
	note = {arXiv: 1512.00728},
	pages = {1--7},
}

@article{goldberg_primer_2015,
	title = {A {Primer} on {Neural} {Network} {Models} for {Natural} {Language} {Processing}},
	volume = {cs.CL},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1510.00726v1\npapers3://publication/uuid/72FCE784-2FD1-484E-8444-0B331B3BDD57},
	doi = {10.1613/jair.4992},
	abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
	journal = {arXiv.org},
	author = {Goldberg, Yoav},
	year = {2015},
	note = {arXiv: 1510.00726v1},
	pages = {726},
}

@incollection{bucholtz2006word,
	title = {Word up: {Social} meanings of slang in {California} youth culture},
	booktitle = {A cultural approach to interpersonal communication: {Essential} readings},
	author = {Bucholtz, Mary},
	year = {2006},
	pages = {243--267},
}

@article{gallon_making_2016,
	title = {Making a {Case} for the {Black} {Digital} {Humanities}},
	issn = {0736-0983},
	url = {http://minnesota.universitypressscholarship.com/view/10.5749/minnesota/9780816677948.001.0001/upso-9780816677948},
	doi = {10.5749/minnesota/9780816677948.001.0001},
	abstract = {Debates in the Digital Humanities by Matthew K. Gold, ed. University of Minnesota Press 2012 516 pages ISBN: 978-0-8166-7795-5 (paperback) In 2008, the National Endowment for the Humanities (NEH) established [...]},
	author = {Gallon, Kim},
	year = {2016},
	pmid = {17065154},
	note = {arXiv: 1011.1669v3
ISBN: 9780816677948},
}

@article{Hosseini2017,
	title = {Deceiving {Google}'s {Perspective} {API} {Built} for {Detecting} {Toxic} {Comments}},
	url = {http://arxiv.org/abs/1702.08138},
	abstract = {Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.},
	author = {Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
	year = {2017},
	note = {arXiv: 1702.08138
ISBN: 0001414100},
}

@inproceedings{Spertus1997,
	address = {Providence, RI, USA},
	title = {Smokey : {Automatic} {Recognition} of {Hostile} {Messages}},
	booktitle = {Proceedings of the {Four}- teenth {National} {Conference} on {Artificial} {Intelligence} and {Ninth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}, {AAAI}’97/{IAAI}’97},
	publisher = {AAAI Press},
	author = {Spertus, Ellen},
	year = {1997},
	pages = {1058--1065},
}

@article{Warner2012,
	title = {Detecting hate speech on the world wide web},
	url = {http://dl.acm.org/citation.cfm?id=2390374.2390377},
	abstract = {We present an approach to detecting hate speech in online text, where hate speech is defined as abusive speech targeting specific group characteristics, such as ethnic origin, re- ligion, gender, or sexual orientation. While hate speech against any group may exhibit some common characteristics, we have ob- served that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words; however, such words may be used in either a positive or a negative sense, making our task similar to that of words sense disambigua- tion. In this paper we describe our definition of hate speech, the collection and annotation of our hate speech corpus, and a mechanism for detecting some commonly used methods of evading common “dirty word” filters. We describe pilot classification experiments in which we classify anti-semitic speech reach- ing an accuracy 94\%, precision of 68\% and recall at 60\%, for an F1 measure of .6375.},
	journal = {Proceeding LSM '12 Proceedings of the Second Workshop on Language in Social Media},
	author = {Warner, William and Hirschberg, Julia},
	year = {2012},
	note = {ISBN: 9781937284206},
	pages = {19--26},
}

@misc{acm_future_of_computing_academy_its_2018,
	title = {It’s {Time} to {Do} {Something}: {Mitigating} the {Negative} {Impacts} of {Computing} {Through} a {Change} to the {Peer} {Review} {Process}},
	url = {https://acm-fca.org/2018/03/29/negativeimpacts/},
	author = {{ACM Future of Computing Academy}},
	year = {2018},
}

@article{krause_critique_2017,
	title = {Critique {Style} {Guide} : {Improving} {Crowdsourced} {Design} {Feedback} with a {Natural} {Language} {Model} {Critique} {Style} {Guide} : {Improving} {Crowdsourced} {Design} {Feedback} with a {Natural} {Language} {Model}},
	doi = {10.1145/3025453.3025883},
	number = {January},
	author = {Krause, Markus and Krause, Markus and Gerber, Elizabeth M and Dow, Steven P and Bailey, Brian P},
	year = {2017},
	note = {ISBN: 9781450346559},
}

@article{gonzalez-hernandez_capturing_2017,
	title = {Capturing the {Patient}'s {Perspective}: a {Review} of {Advances} in {Natural} {Language} {Processing} of {Health}-{Related} {Text}},
	doi = {10.15265/IY-2017-029},
	abstract = {Background: Natural Language Processing (NLP) methods are increasingly being utilized to mine knowledge from unstructured health-related texts. Recent advances in noisy text processing techniques are enabling researchers and medical domain experts to go beyond the information encapsulated in published texts (e.g., clinical trials and systematic reviews) and structured questionnaires, and obtain perspectives from other unstructured sources such as Electronic Health Records (EHRs) and social media posts. Objectives: To review the recently published literature discussing the application of NLP techniques for mining health-related information from EHRs and social media posts. Methods: Literature review included the research published over the last five years based on searches of PubMed, conference proceedings, and the ACM Digital Library, as well as on relevant publications referenced in papers. We particularly focused on the techniques employed on EHRs and social media data. Results: A set of 62 studies involving EHRs and 87 studies involving social media matched our criteria and were included in this paper. We present the purposes of these studies, outline the key NLP con-tributions, and discuss the general trends observed in the field, the current state of research, and important outstanding problems.},
	journal = {IMIA Yearbook of Medical Informatics},
	author = {Gonzalez-Hernandez, G and Sarker, A and O 'connor, K and Savova, G},
	year = {2017},
	keywords = {214-27, medical terms, mining electronic health records, natural language processing review, social media, yearb med inform 2017},
	pages = {214--227},
}

@article{garimella_demographic-aware_2017,
	title = {Demographic-aware word associations},
	url = {https://www.aclweb.org/anthology/D17-1241},
	abstract = {Variations of word associations across different groups of people can provide
insights into peopleâ€\{{\textbackslash}texttrademark\}s psychologies and their world views. To capture these
variations, we introduce the task of demographic-aware word associations. We
build a new gold standard dataset consisting of word association responses for
approximately 300 stimulus words, collected from more than 800 respondents of
different gender (male/female) and from different locations (India/United
States), and show that there are significant variations in the word
associations made by these groups. We also introduce a new demographic-aware
word association model based on a neural net skip-gram architecture, and show
how computational methods for measuring word associations that specifically
account for writer demographics can outperform generic methods that are
agnostic to such information.},
	journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	author = {Garimella, Aparna and Banea, Carmen and Mihalcea, Rada},
	year = {2017},
	pages = {2275--2285},
}

@incollection{ellison_little_2011,
	title = {With a little help from my friends: {How} social network sites affect social capital processes},
	booktitle = {The networked self: {Identity}, community, and culture on social network sites},
	author = {Ellison, Nicole B. and Lampe, Cliff and Steinfield, Charles and Vitak, Jessica},
	year = {2011},
	note = {Issue: January},
	pages = {124--145},
}

@article{law_curiosity_2016,
	title = {Curiosity {Killed} the {Cat}, but {Makes} {Crowdwork} {Better}},
	url = {http://dl.acm.org/citation.cfm?doid=2858036.2858144},
	doi = {10.1145/2858036.2858144},
	abstract = {Crowdsourcing systems are designed to elicit help from humans to accomplish tasks that are still difficult for computers. How to motivate workers to stay longer and/or perform better in crowdsourcing systems is a critical question for designers. Previous work have explored different motivational frameworks, both extrinsic and intrinsic. In this work, we examine the potential for curiosity as a new type of intrinsic motivational driver to incentivize crowd workers. We design crowdsourcing task interfaces that explicitly incorporate mechanisms to induce curiosity and conduct a set of experiments on Amazon’s Mechanical Turk. Our experiment results show that curiosity interventions improve worker retention without degrading performance, and the magnitude of the effects are influenced by both the personal characteristics of the worker and the nature of the task.},
	journal = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI '16},
	author = {Law, Edith and Yin, Ming and Goh, Joslin and Chen, Kevin and Terry, Michael A. and Gajos, Krzysztof Z},
	year = {2016},
	note = {ISBN: 9781450333627},
	pages = {4098--4110},
}

@article{pennington_glove:_2014,
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	issn = {10495258},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pmid = {1710995},
	note = {arXiv: 1504.06654
ISBN: 9781937284961},
	pages = {1532--1543},
}

@article{barbieri_how_2016,
	title = {How {Cosmopolitan} {Are} {Emojis}?: {Exploring} {Emojis} {Usage} and {Meaning} over {Different} {Languages} with {Distributional} {Semantics}},
	url = {https://doi.org/10.1145/2964284.2967278},
	doi = {10.1145/2964284.2967278},
	abstract = {Choosing the right emoji to visually complement or condense the meaning of a message has become part of our daily life. Emojis are pictures, which are naturally combined with plain text, thus creating a new form of language. These pictures are the same independently of where we live, but they can be interpreted and used in different ways. In this paper we compare the meaning and the usage of emojis across different languages. Our results suggest that the overall semantics of the subset of the emojis we studied is preserved across all the languages we analysed. However, some emojis are interpreted in a different way from language to language, and this could be related to socio-geographical differences.},
	journal = {Proceedings of the 2016 ACM on Multimedia Conference},
	author = {Barbieri, Francesco and Kruszewski, German and Ronzano, Francesco and Saggion, Horacio},
	year = {2016},
	note = {ISBN: 9781450336031},
	keywords = {distributional semantics, emojis, natural language processing, twitter},
	pages = {531--535},
}

@incollection{Ochs,
	title = {Indexing {Gender}},
	booktitle = {Rethinking context: {Language} as an interactive phenomenon},
	publisher = {Cambridge University Press},
	author = {Ochs, Elinor},
	editor = {Duranti, Alessandro and Goodwin, Charles},
	year = {1992},
	keywords = {★},
	pages = {335--358},
}

@article{freed_stalkers_2018,
	title = {“{A} {Stalker}'s {Paradise}”: {How} {Intimate} {Partner} {Abusers} {Exploit} {Technology}},
	doi = {10.1145/3173574.3174241},
	journal = {Chi},
	author = {Freed, Diana and Palmer, Jackeline and Minchala, Diana and Levy, Ψ Karen and Ristenpart, Φ Thomas and Dell, Nicola},
	year = {2018},
	note = {ISBN: 9781450356206},
}

@inproceedings{Gong2016,
	title = {Hashtag recommendation using attention-based convolutional neural network},
	abstract = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
	booktitle = {{IJCAI} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Gong, Yuyun and Zhang, Qi},
	year = {2016},
	note = {ISSN: 10450823},
	pages = {2782--2788},
}

@article{brown_identity_2018,
	title = {The {Identity} {Issue}},
	journal = {Journal of Cultural Analytics},
	author = {Brown, Susan and Mandell, Laura},
	year = {2018},
}

@article{long_turbulent_2016,
	title = {Turbulent {Flow}},
	volume = {77},
	issn = {0026-7929},
	url = {http://mlq.dukejournals.org/lookup/doi/10.1215/00267929-3570656},
	doi = {10.1215/00267929-3570656},
	number = {3},
	journal = {Modern Language Quarterly},
	author = {Long, Hoyt and So, Richard Jean},
	year = {2016},
	note = {ISBN: 978-0-8247-0938-9},
	keywords = {124, 1996, be, century, computational modeling, consciousness should be the, in view of what, it fully deserves to, it has done, japan, modernism, most famous technique of, stream of consciousness, t is no surprise, that the stream of, the twentieth, volumes of, world literature, writes franco moretti},
	pages = {345--367},
}

@article{gouws_unsupervised_2011,
	title = {Unsupervised {Mining} of {Lexical} {Variants} from {Noisy} {Text}},
	url = {http://dl.acm.org/citation.cfm?id=2140458.2140468},
	abstract = {The amount of data produced in user- generated content continues to grow at a stag- gering rate. However, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even seman- tics and present significant problems to down- stream applications which make use of this noisy data. In this paper we present a novel unsupervised method for extracting domain- specific lexical variants given a large volume of text. We demonstrate the utility of this method by applying it to normalize text mes- sages found in the online social media service, Twitter, into their most likely standard English versions. Our method yields a 20\% reduction inword error rate over an existing state-of-the- art approach.},
	journal = {Conference on Empirical Methods in Natural Language Processing},
	author = {Gouws, Stephan and Hovy, Dirk and Metzler, Donald and Rey, Marina},
	year = {2011},
	note = {ISBN: 978-1-937284-13-8},
	pages = {82--90},
}

@article{yang_hierarchical_2016,
	title = {Hierarchical {Attention} {Networks} for {Document} {Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	doi = {10.18653/v1/N16-1174},
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	year = {2016},
	note = {arXiv: 1606.02393
ISBN: 9781941643914},
	pages = {1480--1489},
}

@inproceedings{du_convolutional_2017,
	title = {A {Convolutional} {Attention} {Model} for {Text} {Classification}},
	doi = {10.1007/978-3-319-73618-1_16},
	abstract = {Neural network models with attention mechanism have shown their efficiencies on various tasks. However, there is little research work on attention mechanism for text classification and existing attention model for text classifi-cation lacks of cognitive intuition and mathematical explanation. In this paper, we propose a new architecture of neural network based on the attention model for text classification. In particular, we show that the convolutional neural net-work (CNN) is a reasonable model for extracting attentions from text sequences in mathematics. We then propose a novel attention model base on CNN and intro-duce a new network architecture which combines recurrent neural network with our CNN-based attention model. Experimental results on five datasets show that our proposed models can accurately capture the salient parts of sentences to im-prove the performance of text classification.},
	booktitle = {National {CCF} {Conference} on {Natural} {Language} {Processing} and {Chinese} {Computing}},
	publisher = {Springer Cham},
	author = {Du, Jiachen and Gui, Lin and Xu, Ruifeng and He, Yulan},
	year = {2017},
	pages = {183--195},
}

@article{yin_abcnn:_2016,
	title = {{ABCNN}: {Attention}-{Based} {Convolutional} {Neural} {Network} for {Modeling} {Sentence} {Pairs}},
	volume = {4},
	issn = {2307-387X},
	url = {http://arxiv.org/abs/1512.05193},
	abstract = {How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence's representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNN; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNN achieves state-of-the-art performance on AS, PI and TE tasks.},
	journal = {Transactions of the Association for Computational Linguistic},
	author = {Yin, Wenpeng and Schütze, Hinrich and Xiang, Bing and Zhou, Bowen},
	year = {2016},
	note = {arXiv: 1512.05193},
	pages = {259--272},
}

@article{barzilay_modeling_2005,
	title = {Modeling {Local} {Coherence}: {An} {Entity}-{Based} {Approach}},
	volume = {34},
	issn = {0891-2017},
	doi = {10.1162/coli.2008.34.1.1},
	abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
	number = {1},
	journal = {Proceedings of the 43rd Annual Meeting of the ACL},
	author = {Barzilay, Regina and Lapata, Mirella},
	year = {2005},
	note = {ISBN: 1932432515},
	pages = {141--148},
}

@inproceedings{Ross2017,
	title = {Measuring the {Reliability} of {Hate} {Speech} {Annotations}: {The} {Case} of the {European} {Refugee} {Crisis}},
	url = {http://arxiv.org/abs/1701.08118},
	doi = {10.17185/duepublico/42132},
	abstract = {Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.},
	booktitle = {{NLP4CMC} {III}: 3rd {Workshop} on {Natural} {Language} {Processing} for {Computer}-{Mediated} {Communication}, {Sept}. 2016},
	author = {Ross, Björn and Rist, Michael and Carbonell, Guillermo and Cabrera, Benjamin and Kurowsky, Nils and Wojatzki, Michael},
	year = {2016},
	note = {arXiv: 1701.08118},
}

@article{Dinakar2015,
	title = {Common {Sense} {Reasoning} for {Detection}, {Prevention}, and {Mitigation} of {Cyberbullying}},
	volume = {18},
	issn = {21606455},
	doi = {10.1145/2362394.2362400},
	number = {September},
	journal = {Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015) Common},
	author = {Dinakar, Karthik and Picard, Rosalind and Lieberman, Henry},
	year = {2015},
	keywords = {Journal Track},
	pages = {4168--4172},
}

@article{Schmidt2017,
	title = {A {Survey} on {Hate} {Speech} {Detection} using {Natural} {Language} {Processing}},
	url = {http://aclweb.org/anthology/W17-1101},
	doi = {10.18653/v1/W17-1101},
	abstract = {This paper presents a survey on hate speech detection. Given the steadily grow-ing body of social media content, the amount of online hate speech is also in-creasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey de-scribes key areas that have been explored to automatically recognize these types of utterances using natural language process-ing. We also discuss limits of those ap-proaches.},
	number = {2012},
	journal = {Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media},
	author = {Schmidt, Anna and Wiegand, Michael},
	year = {2017},
	pages = {1--10},
}

@article{bullingham_presentation_2013,
	title = {'{The} presentation of self in the online world': {Goffman} and the study of online identities},
	volume = {39},
	issn = {01655515},
	doi = {10.1177/0165551512470051},
	abstract = {This paper presents an exemplification and discussion of the contemporaneity of Erving Goffman's work and of its applicability to the analysis of identity and presentation of self in the blogging and Second Life (SL) contexts. An analysis of online identity and interaction practices in 10 different cases of bloggers and SL inhabitants and of their online spaces is presented in terms of: expressions given; embellishment as a minor form of persona adoption; dividing the self; conforming and fitting in'; and masking, anonymity and pseudonimity. The key finding of the research is that, contrary to engaging with the process of whole persona adoption, participants were keen to re-create their offline self online, but engaged in editing facets of self. This emphasizes the key premise in Goffman's work that, when in front stage', people deliberately chose to project a given identity. It is concluded that Goffman's original framework is of great usefulness as an explanatory framework for understanding identity through interaction and the presentation of self in the online world. Equally, the online environment, with its enhanced potential for editing the self, can offer opportunities to contribute to the further development of the Goffman framework.},
	number = {1},
	journal = {Journal of Information Science},
	author = {Bullingham, Liam and Vasconcelos, Ana C.},
	year = {2013},
	pmid = {74193159},
	note = {ISBN: 0165551500000},
	keywords = {Erving Goffman, constructivist case studies, online identity, online interaction},
	pages = {101--112},
}

@inproceedings{nemanja_djuric_jing_zhou_hate_2015,
	title = {Hate {Speech} {Detection} with {Comment} {Embeddings}},
	booktitle = {Proceedings of {WWW} 2015},
	author = {Nemanja Djuric, Jing Zhou, Robin Morris and Mihajlo Grbovic, Vladan Radosavljevic, Narayan Bhamidipati},
	year = {2015},
}

@article{Nobata2016,
	title = {Abusive {Language} {Detection} in {Online} {User} {Content}},
	url = {http://dl.acm.org/citation.cfm?doid=2872427.2883062},
	doi = {10.1145/2872427.2883062},
	abstract = {Detection of abusive language in user generated online con-tent has become an issue of increasing importance in recent years. Most current commercial methods make use of black-lists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted ex-amples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abu-sive language over time and in different settings to further enhance our knowledge of this behavior.},
	journal = {Proceedings of the 25th International Conference on World Wide Web - WWW '16},
	author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
	year = {2016},
	note = {ISBN: 9781450341431},
	keywords = {abusive language, discourse classification, hate speech, nlp, stylistic classifica-, tion},
	pages = {145--153},
}

@inproceedings{Davidson2017,
	title = {Automated {Hate} {Speech} {Detection} and the {Problem} of {Offensive} {Language}},
	isbn = {978-1-57735-788-9},
	url = {http://arxiv.org/abs/1703.04009},
	abstract = {A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.},
	booktitle = {{ICWSM} 2017},
	author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
	year = {2017},
	note = {arXiv: 1703.04009},
}

@article{Gamback2017,
	title = {Using {Convolutional} {Neural} {Networks} to {Classify} {Hate}-{Speech}},
	abstract = {The paper introduces a deep learning-based Twitter hate-speech text classifica-tion system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sex-ism) and non-hate-speech. Four Con-volutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3\% F-score.},
	number = {7491},
	journal = {Proceedings of the First Workshop on Abusive Language Online},
	author = {Gambäck, Björn and Sikdar, Utpal Kumar},
	year = {2017},
	pages = {85--90},
}

@article{KennedyIII2017,
	title = {Hack {Harassment}: {Technology} {Solutions} to {Combat} {Online} {Harassment}},
	url = {https://drive.google.com/file/d/0B4xDAGbwZJjQNzJtMGVrck4tVlE/view},
	abstract = {This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes under-reported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addi-tion, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media compa-nies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like.},
	journal = {Proceedings of the First Workshop on Abusive Language Online},
	author = {Kennedy III, George W. and McCollough, Andrew W. and Dixon, Edward and Bastidas, Alexie and Ryan, John and Loo, Chris and Sahay, Saurav},
	year = {2017},
	pages = {73--77},
}

@inproceedings{Pryzant2017,
	title = {Predicting {Sales} from the {Language} of {Product} {Descriptions}},
	abstract = {What can a business say to attract customers? E-commerce vendors frequently sell the same items but use different marketing strate-gies to present their goods. Understanding consumer responses to this heterogeneous landscape of information is important both as business intelligence and, more broadly, a window into consumer attitudes. When studying consumer behavior, the existing litera-ture is primarily concerned with product reviews. In this paper we posit that textual product descriptions are also important determi-nants of consumer choice. We mine 90,000+ product descriptions on the Japanese e-commerce marketplace Rakuten and identify ac-tionable writing styles and word usages that are highly predictive of consumer purchasing behavior. In the process, we observe the inadequacies of traditional feature extraction algorithms, namely their inability to control for the implicit effects of confounds like brand loyalty and pricing strategies. To circumvent this problem, we propose a novel neural network architecture that leverages an adversarial objective to control for confounding factors, and atten-tional scores over its input to automatically elicit textual features as a domain-specific lexicon. We show that these textual features can predict the sales of each product, and investigate the narratives highlighted by these words. Our results suggest that appeals to au-thority, polite language, and mentions of informative and seasonal language win over the most customers.},
	booktitle = {Proceedings of {SIGIR}, {Tokyo}, {Japan}, {August} 2017 ({SIGIR} 2017 {eCom})},
	author = {Pryzant, Reid and Chung, Young-Joo and Jurafsky, Dan},
	year = {2017},
	keywords = {KEYWORDS e-commerce, Neural networks, adversarial learn-ing, feature selection, natural language processing, neural networks},
}

@article{safi_samghabadi_detecting_2017,
	title = {Detecting {Nastiness} in {Social} {Media}},
	url = {http://aclweb.org/anthology/W17-3010},
	doi = {10.18653/v1/W17-3010},
	abstract = {Although social media has made it easy for people to connect on a virtually un-limited basis, it has also opened doors to people who misuse it to undermine, ha-rass, humiliate, threaten and bully others. There is a lack of adequate resources to de-tect and hinder its occurrence. In this pa-per, we present our initial NLP approach to detect invective posts as a first step to eventually detect and deter cyberbullying. We crawl data containing profanities and then determine whether or not it contains invective. Annotations on this data are improved iteratively by in-lab annotations and crowdsourcing. We pursue different NLP approaches containing various typi-cal and some newer techniques to distin-guish the use of swear words in a neutral way from those instances in which they are used in an insulting way. We also show that this model not only works for our data set, but also can be successfully applied to different data sets.},
	journal = {Proceedings of the First Workshop on Abusive Language Online},
	author = {Safi Samghabadi, Niloofar and Maharjan, Suraj and Sprague, Alan and Diaz-Sprague, Raquel and Solorio, Thamar},
	year = {2017},
	pages = {63--72},
}

@article{jones_conceptual_2000,
	title = {A conceptual model of multiple dimensions of identity},
	volume = {41},
	issn = {00219789},
	doi = {10.1353/csd.2007.0000},
	abstract = {A conceptual model of multiple dimensions of identity depicts a core sense of self or one’s personal identity. Intersecting circles surrounding the core identity represent significant identity dimensions (e.g., race, sexual orienta- tion, and religion) and contextual influences (e.g., family background and life experiences). The model evolved from a grounded theory study of a group of 10 women college students ranging in age from 20-24 and of diverse racial-ethnic backgrounds.},
	number = {4},
	journal = {Journal of College Student Development},
	author = {Jones, Susan R and Mcewen, Marylu K},
	year = {2000},
	pmid = {12289432},
	note = {ISBN: 0787958956},
	pages = {405--414},
}

@article{kumar_iit-tuda_2016,
	title = {{IIT}-{TUDA} at {SemEval}-2016 {Task} 5: {Beyond} {Sentiment} {Lexicon}: {Combining} {Domain} {Dependency} and {Distributional} {Semantics} {Features} for {Aspect} {Based} {Sentiment} {Analysis}},
	abstract = {This paper reports the IIT-TUDA participation in the SemEval 2016 shared Task 5 of Aspect Based Sentiment Analysis (ABSA) for sub-task 1. We describe our system incorporat-ing domain dependency graph features, dis-tributional thesaurus and unsupervised lexical induction using an unlabeled external corpus for aspect based sentiment analysis. Overall, we submitted 29 runs, covering 7 languages and 4 different domains. Our system is placed first in sentiment polarity classification for the English laptop domain, Spanish and Turkish restaurant reviews, and opinion target expres-sion for Dutch and French in restaurant do-main, and scores in medium ranks for aspect category identification and opinion target ex-traction.},
	number = {SemEval},
	journal = {Proceedings of the 10th International Workshop on Semantic Evaluation},
	author = {Kumar, Ayush and Kohail, Sarah and Kumar, Amit and Ekbal, Asif and Biemann, Chris},
	year = {2016},
	note = {ISBN: 9781941643952},
	pages = {1129--1135},
}

@article{alvarez-lopez_gti_2016,
	title = {{GTI} at {SemEval}-2016 {Task} 5: {SVM} and {CRF} for {Aspect} {Detection} and {Unsupervised} {Aspect}-{Based} {Sentiment} {Analysis}},
	url = {http://aclweb.org/anthology/S16-1049},
	abstract = {This paper describes in detail the approach carried out by the GTI research group for Se-mEval 2016 Task 5: Aspect-Based Sentiment Analysis, for the different subtasks proposed, as well as languages and dataset contexts. In particular, we developed a system for category detection based on SVM. Then for the opinion target detection task we developed a system based on CRFs. Both are built for restaurants domain in English and Spanish languages. Fi-nally for aspect-based sentiment analysis we carried out an unsupervised approach based on lexicons and syntactic dependencies, in En-glish language for laptops and restaurants do-mains.},
	journal = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
	author = {Àlvarez-López, Tamara and Juncal-Martínez, Jonathan and Fernández-Gavilanes, Milagros and Costa-Montenegro, Enrique and González-Castaño, Javier Francisco},
	year = {2016},
	note = {ISBN: 9781941643952},
	pages = {306--311},
}

@article{rosen-zvi_author-topic_2004,
	title = {The author-topic model for authors and documents},
	issn = {01689002},
	url = {http://portal.acm.org/citation.cfm?id=1036902},
	doi = {10.1016/j.nima.2010.11.062},
	abstract = {We introduce the author-topic model, a gen- erative model for documents that extends La- tent Dirichlet Allocation (LDA; Blei, Ng, \& Jordan, 2003) to include authorship informa- tion. Each author is associated with a multi- nomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple au- thors is modeled as a distribution over topics that is a mixture of the distributions associ- ated with the authors. We apply the model to a collection of 1,700 NIPS conference pa- pers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative mod- els for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each au- thor is associated with a distribution over words rather than a distribution over top- ics. We show topics recovered by the author- topic model, and demonstrate applications to computing similarity between authors and entropy of author output.},
	journal = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
	author = {Rosen-Zvi, M. and Griffiths, T. and Steyvers, M. and Smyth, P.},
	year = {2004},
	note = {arXiv: 1207.4169
ISBN: 0-9749039-0-6},
	pages = {487--494},
}

@article{pontiki_semeval-2016_2016,
	title = {{SemEval}-2016 {Task} 5: {Aspect} {Based} {Sentiment} {Analysis}},
	url = {http://aclweb.org/anthology/S16-1055},
	doi = {10.18653/v1/S16-1055},
	abstract = {This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams.},
	journal = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
	author = {Pontiki, Maria and Galanis, Dimitrios and Papageorgiou, Haris and Androutsopoulos, Ion and Manandhar, Suresh and Al-Smadi, Mohammad and Al-Ayyoub, Mahmoud and Zhao, Yanyan and Qin, Bing and De Clercq, Orphée and Hoste, Véronique and Apidianaki, Marianna and Tannier, Xavier and Louk, Natalia and Eryiğit, Gülşen},
	year = {2016},
	note = {ISBN: 978-1-941643-95-2},
	pages = {342--349},
}

@article{bopp_disempowered_2017,
	title = {Disempowered by {Data} : {Nonprofits} , {Social} {Enterprises} , and the {Consequences} of {Data}-{Driven} {Work}},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025694},
	doi = {10.1145/3025453.3025694},
	journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	author = {Bopp, Chris and Voida, Amy},
	year = {2017},
	note = {ISBN: 9781450346559},
	keywords = {accountability, data, metrics, mission-driven, monitoring and evaluation, nonprofit organization, social enterprise},
	pages = {3608--3619},
}

@book{sfard2008thinking,
	title = {Thinking as {Communicating}: {Human} {Development}, the {Growth} of {Discourses}, and {Mathematizing}},
	isbn = {978-1-139-46799-5},
	url = {https://books.google.com/books?id=cqwx8aDfUuQC},
	publisher = {Cambridge University Press},
	author = {Sfard, A},
	year = {2008},
	note = {Series Title: Learning in Doing: Social, Cognitive and Computational Perspectives},
}

@article{dame_making_2016,
	title = {Making a name for yourself: tagging as transgender ontological practice on {Tumblr}},
	volume = {33},
	issn = {14795809},
	doi = {10.1080/15295036.2015.1130846},
	abstract = {This article considers, first, the tag-based architecture as a communicative medium and, second, how transgender-identified Tumblr users navigate and exploit the particular limitations of tag-based architectures. In particular, I analyze how trans-identified Tumblr users navigate and exploit these architectures' affordances to manage feelings of ontological security in Tumblr's sharing-centered and tag-managed environment. Trans users express specific self and group identifications, as well as audience and social commentary, through their tagging practices. These tags form the basis of a trans-specific folksonomy, or emergent user-defined tag collections, on Tumblr. However, this folksonomy relies heavily on an existing subcultural vocabulary, limiting users' self-expression to recognizable terminology with unstable definitions. Terms and their usage become an ideological battleground when users attempt to establish normative tag definitions through public policing and shaming of others' usage. These conflicts indicate how semantic categorization limits expression of the scope of human self-presentation and gender performance.},
	number = {1},
	journal = {Critical Studies in Media Communication},
	author = {Dame, Avery},
	year = {2016},
	keywords = {Tumblr, folksonomy, networked publics, tags, transgender},
	pages = {23--37},
}

@article{Rendle2009,
	title = {Learning optimal ranking with tensor factorization for tag recommendation},
	url = {http://dl.acm.org/citation.cfm?id=1557100},
	doi = {10.1145/1557019.1557100},
	abstract = {Tag recommendation is the task of predicting a personalized list of tags for a user given an item. This is important for many websites with tagging capabilities like last.fm or delicious. In this paper, we propose a method for tag recommendation based on tensor factorization (TF). In contrast to other TF methods like higher order singular value decomposition (HOSVD), our method RTF ('ranking with tensor factorization') directly optimizes the factorization model for the best personalized ranking. RTF handles missing values and learns from pairwise ranking constraints. Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems. In all, RTF directly optimizes for the actual problem using a correct interpretation of the data. We provide a gradient descent algorithm to solve our optimization problem. We also provide an improved learning and prediction method with runtime complexity analysis for RTF. The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions. Besides the theoretical analysis, we empirically show that our method outperforms other state-of-the-art tag recommendation methods like FolkRank, PageRank and HOSVD both in quality and prediction runtime.},
	journal = {Kdd},
	author = {Rendle, Steffen and Marinho, Leandro Balby},
	year = {2009},
	note = {ISBN: 9781605584959},
	keywords = {allows users to describe, an item, e, friend, g, in general, ranking, song, tag recommendation, tagging, tags, tensor factorization, website, with a list of, words},
	pages = {727--736},
}

@article{Ohkura2006,
	title = {Browsing {System} for {Weblog} {Articles} based on {Automated} {Folksonomy}},
	abstract = {Folksonomy is a new manual classification scheme based on tagging efforts of users with freely chosen keywords. In folksonomy, a user puts an item (i.e. a photo, a book mark) on a server and shares it with other users. The owner and even the other users can attach tags to this item for their own classification, and they reflect many ones viewpoints. Since tags are chosen from users vocabulary and contain many ones viewpoints, classification results are easy to understand for ordinary users. As a result, folksonomy serves as an efficient browsing method, because users can grasp the essence of items by looking at the tags. Even though the scalability of folksonomy is much higher than the other manual classification schemes, the method cannot deal with tremendous number of items such as whole weblog articles on the Internet.},
	journal = {Workshop on the Weblogging Ecosystem Aggregation Analysis and Dynamics at WWW},
	author = {Ohkura, Tsutomu and Kiyota, Yoji and Nakagawa, Hiroshi},
	year = {2006},
}

@article{Liu2011,
	title = {A simple word trigger method for social tag suggestion},
	url = {http://dl.acm.org/citation.cfm?id=2145601},
	abstract = {It is popular for users in Web 2.0 era to freely annotate online resources with tags. To ease the annotation process, it has been great interest in automatic tag suggestion. We propose a method to suggest tags according to the text description of a resource. By considering both the description and tags of a given resource as summaries to the resource written in two languages, we adopt word alignment models in statistical machine translation to bridge their vocabulary gap. Based on the translation probabilities between the words in descriptions and the tags estimated on a large set of description-tags pairs, we build a word trigger method (WTM) to suggest tags according to the words in a resource description. Experiments on real world datasets show that WTM is effective and robust compared with other methods. Moreover, WTM is relatively simple and efficient, which is practical for Web applications.},
	journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Liu, Zhiyuan and Chen, Xinxiong and Sun, Maosong},
	year = {2011},
	note = {ISBN: 1937284115},
	pages = {1577--1588},
}

@article{Ferragina2015,
	title = {On {Analyzing} {Hashtags} in {Twitter}},
	abstract = {Hashtags, originally introduced in Twitter, are now becom-ing the most used way to tag short messages in social net-works since this facilitates subsequent search, classification and clustering over those messages. However, extracting in-formation from hashtags is difficult because their composi-tion is not constrained by any (linguistic) rule and they usu-ally appear in short and poorly written messages which are difficult to analyze with classic IR techniques. In this paper we address two challenging problems regard-ing the " meaning of hashtags " — namely, hashtag relatedness and hashtag classification — and we provide two main con-tributions. First we build a novel graph upon hashtags and (Wikipedia) entities drawn from the tweets by means of topic annotators (such as TagME); this graph will allow us to model in an efficacious way not only classic co-occurrences but also semantic relatedness among hashtags and entities, or between entities themselves. Based on this graph, we design algo-rithms that significantly improve state-of-the-art results upon known publicly available datasets. The second contribution is the construction and the public release to the research community of two new datasets: the former is a new dataset for hashtag relatedness, the latter is a dataset for hashtag classification that is up to two orders of magnitude larger than the existing ones. These datasets will be used to show the robustness and efficacy of our ap-proaches, showing improvements in F1 up to two-digits in percentage (absolute).},
	journal = {Aaai},
	author = {Ferragina, Paolo and Piccinno, Francesco and Santoro, Roberto},
	year = {2015},
	note = {ISBN: 9781577357339},
	keywords = {Full Papers},
	pages = {110--119},
}

@article{Ding2012,
	title = {Automatic {Hashtag} {Recommendation} for {Microblogs} using {Topic}-specific {Translation} {Model}},
	number = {December},
	journal = {Coling-2012},
	author = {Ding, Zhuoye and Zhang, Qi and Huang, Xuanjing},
	year = {2012},
	pages = {265--274},
}

@article{Chen2017,
	title = {{DocTag2Vec}: {An} {Embedding} {Based} {Multi}-label {Learning} {Approach} for {Document} {Tagging}},
	url = {http://arxiv.org/abs/1707.04596},
	abstract = {Tagging news articles or blog posts with relevant tags from a collection of predefined ones is coined as document tagging in this work. Accurate tagging of articles can benefit several downstream applications such as recommendation and search. In this work, we propose a novel yet simple approach called DocTag2Vec to accomplish this task. We substantially extend Word2Vec and Doc2Vec---two popular models for learning distributed representation of words and documents. In DocTag2Vec, we simultaneously learn the representation of words, documents, and tags in a joint vector space during training, and employ the simple \$k\$-nearest neighbor search to predict tags for unseen documents. In contrast to previous multi-label learning methods, DocTag2Vec directly deals with raw text instead of provided feature vector, and in addition, enjoys advantages like the learning of tag representation, and the ability of handling newly created tags. To demonstrate the effectiveness of our approach, we conduct experiments on several datasets and show promising results against state-of-the-art methods.},
	number = {January 2014},
	author = {Chen, Sheng and Soni, Akshay and Pappu, Aasish and Mehdad, Yashar},
	year = {2017},
	note = {arXiv: 1707.04596},
}

@article{heafield_scalable_2013,
	title = {Scalable {Modified} {Kneser}-{Ney} {Language} {Model} {Estimation}},
	journal = {ACL},
	author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H},
	year = {2013},
}

@article{burke_once_2016,
	title = {Once {More} with {Feeling}: {Supportive} {Responses} to {Social} {Sharing} on {Facebook}},
	url = {http://dl.acm.org/citation.cfm?doid=2818048.2835199},
	doi = {10.1145/2818048.2835199},
	abstract = {Life is more than cat pictures. There are tough days, heartbreak, and hugs. Under what contexts do people share these feelings online, and how do their friends respond? Using millions of de-identified Facebook status updates with poster-annotated feelings (e.g., feeling thankful or feeling worried), we examine the magnitude and circumstances in which people share positive or negative feelings and characterize the nature of the responses they receive. We find that people share greater proportions of both positive and negative emotions when their friend networks are smaller and denser. Consistent with social sharing theory, hearing about a friend’s troubles on Facebook causes friends to reply with more emotional and supportive comments. Friends’ comments are also more numerous and longer. Posts with positive feelings, on the other hand, receive more likes, and their comments have more positive language. Feelings that relate to the poster’s self worth, such as feeling defeated, feeling unloved, or feeling accomplished amplify these effects.},
	journal = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \& Social Computing - CSCW '16},
	author = {Burke, Moira and Develin, Mike},
	year = {2016},
	note = {ISBN: 9781450335928},
	pages = {1460--1472},
}

@article{cassell_embodied_2000,
	title = {Embodied conversational interface agents},
	volume = {43},
	issn = {00010782},
	url = {http://portal.acm.org/citation.cfm?doid=332051.332075},
	doi = {10.1145/332051.332075},
	abstract = {More than another friendly face, Rea knows how to have a conversation with living, breathing human users with a wink, a nod, and a sidelong glance.},
	number = {4},
	journal = {Communications of the ACM},
	author = {Cassell, Justine},
	year = {2000},
	note = {ISBN: 0001-0782},
	pages = {70--78},
}

@article{fu_style_2017,
	title = {Style {Transfer} in {Text}: {Exploration} and {Evaluation}},
	url = {http://arxiv.org/abs/1711.06861},
	abstract = {Style transfer is an important problem in natural language processing (NLP). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and principle evaluation metrics. In this paper, we propose to learn style transfer with non-parallel data. We explore two models to achieve this goal, and the key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. We also propose novel evaluation metrics which measure two aspects of style transfer: transfer strength and content preservation. We access our models and the evaluation metrics on two tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with higher style transfer strength and similar content preservation score comparing to auto-encoder.},
	author = {Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
	year = {2017},
	note = {arXiv: 1711.06861},
}

@article{vosoughi_tweet2vec:_2016,
	title = {{Tweet2Vec}: {Learning} {Tweet} {Embeddings} {Using} {Character}-level {CNN}-{LSTM} {Encoder}-{Decoder}},
	url = {http://arxiv.org/abs/1607.07514},
	doi = {10.1145/2911451.2914762},
	abstract = {We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.},
	author = {Vosoughi, Soroush and Vijayaraghavan, Prashanth and Roy, Deb},
	year = {2016},
	note = {arXiv: 1607.07514
ISBN: 9781450340694},
	keywords = {cnn, convolutional neural networks, embedding, encoder-decoder, lstm, tweet, tweet2vec, twitter},
	pages = {16--19},
}

@article{habernal_which_2016,
	title = {Which argument is more convincing? {Analyzing} and predicting convincingness of {Web} arguments using bidirectional {LSTM}},
	doi = {10.18653/v1/P16-1150},
	abstract = {We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classifica-tion, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investi-gate whether the relation " A is more con-vincing than B " exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all argu-ments to the topic based on their convinc-ingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spear-man's correlation in a cross-topic evalua-tion. We release the newly created corpus UKPConvArg1 and the experimental soft-ware under open licenses.},
	journal = {Acl},
	author = {Habernal, Ivan and Gurevych, Iryna},
	year = {2016},
	note = {ISBN: 9781510827585},
	pages = {1589--1599},
}

@inproceedings{Gweon2005,
	title = {Supporting {Efficient} and {Reliable} {Content} {Analysis} {Using} {Automatic} {Text} {Processing} {Technology} *},
	booktitle = {Human-{Computer} {Interaction}-{INTERACT} 2005},
	author = {Gweon, Gahgene and Rosé, Carolyn Penstein and Wittwer, Joerg and Nueckles, Matthias},
	year = {2005},
	pages = {1112--1115},
}

@article{pappu_lightweight_2017,
	title = {Lightweight {Multilingual} {Entity} {Extraction} and {Linking}},
	issn = {9781450321389},
	url = {http://dl.acm.org/citation.cfm?doid=3018661.3018724},
	doi = {10.1145/3018661.3018724},
	abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
	journal = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining - WSDM '17},
	author = {Pappu, Aasish and Blanco, Roi and Mehdad, Yashar and Stent, Amanda and Thadani, Kapil},
	year = {2017},
	note = {arXiv: 1508.06655v1
ISBN: 9781450346757},
	pages = {365--374},
}

@article{li_role_2016,
	title = {The {Role} of {Discourse} {Units} in {Near}-{Extractive} {Summarization}},
	url = {http://www.aclweb.org/anthology/W16-3617},
	abstract = {Although human-written summaries of documents tend to involve significant edits to the source text, most automated summa-rizers are extractive and select sentences verbatim. In this work we examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers to pro-duce a wider range of human-like sum-maries. Our analysis demonstrates that EDU segmentation is effective in preserv-ing human-labeled summarization con-cepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that us-ing EDUs as units of content selection in-stead of sentences leads to stronger sum-marization performance in near-extractive scenarios, especially under tight budgets.},
	number = {September},
	journal = {Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	author = {Li, Junyi Jessy and Thadani, Kapil and Stent, Amanda},
	year = {2016},
	pages = {137--147},
}

@incollection{pfaffenberger2003standing,
	title = {A {Standing} {Wave} in the {Web} of {Our} {Communications}: {Usenet} and the {Socio}-{Technical} {Construction} of {Cyberspace} {Values}},
	booktitle = {From {Usenet} to {Cowebs}},
	publisher = {Springer},
	author = {Pfaffenberger, Bryan},
	year = {2003},
	pages = {20--43},
}

@article{faruqui_problems_2016,
	title = {Problems {With} {Evaluation} of {Word} {Embeddings} {Using} {Word} {Similarity} {Tasks}},
	url = {http://arxiv.org/abs/1605.02276},
	doi = {10.18653/v1/W16-2506},
	abstract = {Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
	year = {2016},
	note = {arXiv: 1605.02276},
}

@article{Khatib2012,
	title = {Public {Diplomacy} 2 .0 : {A} {Case} {Study} of the {US} {Digital} {Outreach} {Team}},
	volume = {66},
	issn = {00263141},
	doi = {10.1353/mej.2012.0103},
	abstract = {The internet is enabling new approaches to public diplomacy. The US Digital Outreach Team (DOT) is one such initiative, aiming to engage directly with citizens in the Middle East by posting messages about US foreign policy on internet forums. This case study assesses the DOT’s work. Does this method provide a promising move towards a more interactive and individualized approach to connecting with the Middle East? What are the strategic challenges faced by “public diplomacy 2.0?”},
	number = {3},
	journal = {Middle East Journal},
	author = {Khatib, Lina and Dutton, William and Thelwall, Michael},
	year = {2012},
	note = {ISBN: 6507247197},
	pages = {453--472},
}

@article{detert_implicit_2011,
	title = {Implicit voice theories: {Taken}-for-granted rules of self-censorship at work},
	volume = {54},
	doi = {10.1007/sll423-006-9009-2},
	number = {3},
	journal = {The Academy of Management Journal},
	author = {Detert, James R. and Edmonson, Amy C.},
	year = {2011},
	pages = {461--488},
}

@article{Wright2007,
	title = {Democracy, deliberation and design: {The} case of online discussion forums},
	volume = {9},
	issn = {14614448},
	doi = {10.1177/1461444807081230},
	abstract = {Within democratic theory, the deliberative variant has assumed pre-eminence. It represents for many the ideal of democracy, and in pursuit of this ideal, online discussion forums have been proposed as solutions to the practical limits to mass deliberation. Critics have pointed to evidence which suggests that online discussion has tended to undermine deliberation. This article argues that this claim, which generates a stand-off between the two camps, misses a key issue: the role played by design in facilitating or thwarting deliberation. It argues that political choices are made both about the format and operation of the online discussion, and that this affects the possibility of deliberation. Evidence for the impact of design (and the choices behind it) is drawn from analysis of European Union and UK discussion forums. This evidence suggests that we should view deliberation as dependent on design and choice, rather than a predetermined product of the technology.},
	number = {5},
	journal = {New Media and Society},
	author = {Wright, Scott and Street, John},
	year = {2007},
	note = {ISBN: 1461-4448},
	keywords = {Deliberative democracy, E-democracy, Online discussion, Theories of technology, Website design},
	pages = {849--869},
}

@article{Lampe2014,
	title = {Crowdsourcing civility : {A} natural experiment examining the effects of distributed moderation in online forums},
	volume = {31},
	issn = {0740-624X},
	url = {http://dx.doi.org/10.1016/j.giq.2013.11.005},
	doi = {10.1016/j.giq.2013.11.005},
	number = {2},
	journal = {Government Information Quarterly},
	author = {Lampe, Cliff and Zube, Paul and Lee, Jusil and Hyun, Chul and Johnston, Erik},
	year = {2014},
	note = {Publisher: Elsevier Inc.},
	keywords = {Deliberation, Human–computer interaction, Moderation systems, Politics, Public administration, Social media},
	pages = {317--326},
}

@article{kaatz_threats_2014,
	title = {Threats to objectivity in peer review: {The} case of gender},
	volume = {35},
	issn = {18733735},
	url = {http://dx.doi.org/10.1016/j.tips.2014.06.005},
	doi = {10.1016/j.tips.2014.06.005},
	abstract = {Scientists strive to be objective in their peer review of grant applications and manuscript submissions. Nevertheless, all humans are susceptible to biases in decision-making. To illustrate how cognitive bias unrelated to the merit of the science could influence scientific peer review we describe the potential impact of applicant gender on the judgment of reviewers. Table 1 describes some different types of cognitive biases and Table 2 describes conditions that might facilitate the operation of cognitive biases in peer review.},
	number = {8},
	journal = {Trends in Pharmacological Sciences},
	author = {Kaatz, Anna and Gutierrez, Belinda and Carnes, Molly},
	year = {2014},
	pmid = {25086743},
	note = {arXiv: 15334406
Publisher: Elsevier Ltd
ISBN: 01656147},
	pages = {371--373},
}

@book{davis1999web,
	title = {The web of politics: {The} {Internet}'s impact on the {American} political system},
	publisher = {Oxford University Press},
	author = {Davis, Richard},
	year = {1999},
}

@book{chadwick2006internet,
	title = {Internet {Politics}: {States}, {Citizens}, and {New} {Communication} {Technologies}},
	isbn = {978-0-19-517773-2},
	url = {https://books.google.com/books?id=EV6GAAAAMAAJ},
	publisher = {Oxford University Press},
	author = {Chadwick, A},
	year = {2006},
}

@article{garimella_ebb_2017,
	title = {The {Ebb} and {Flow} of {Controversial} {Debates} on {Social} {Media}},
	url = {http://arxiv.org/abs/1703.05994},
	abstract = {We explore how the polarization around controversial topics evolves on Twitter - over a long period of time (2011 to 2016), and also as a response to major external events that lead to increased related activity. We find that increased activity is typically associated with increased polarization; however, we find no consistent long-term trend in polarization over time among the topics we study.},
	author = {Garimella, Kiran and Morales, Gianmarco De Francisci and Gionis, Aristides and Mathioudakis, Michael},
	year = {2017},
	note = {arXiv: 1703.05994
ISBN: 9781577357889},
}

@article{wright_government-run_2006,
	title = {Government-run online discussion fora: {Moderation}, censorship and the shadow of control},
	volume = {8},
	issn = {13691481},
	doi = {10.1111/j.1467-856X.2006.00247.x},
	abstract = {Moderators are widely thought to be crucial to the facilitation of high-quality democratic debate, particularly in government-sponsored participatory exercises. There are, however, persistent fears that moderators censor rather than promote free speech, leading to a ‘shadow of control’. This article analyses the relationship between moderation and censorship on two British central government online discussion fora: Downing Street's Speaker's Corner and Policy Forum, and Citizen Space's E-Democracy Forum. Two models of moderation are developed to help structure the analysis. The main conclusions are that moderation strategies must be clearly linked to the policy goals behind the forum, and that the moderator's roles should be separated to limit the so-called ‘shadow’. The censorial role being conducted by an independent body, with facilitation activities conducted by civil servants linked to the policy being discussed.},
	number = {4},
	journal = {British Journal of Politics and International Relations},
	author = {Wright, Scott},
	year = {2006},
	pages = {550--568},
}

@inproceedings{Jo2017,
	title = {Modeling {Dialogue} {Acts} with {Content} {Word} {Filtering} and {Speaker} {Preferences}},
	copyright = {All rights reserved},
	url = {http://www.aclweb.org/anthology/D17-1232},
	abstract = {We present an unsupervised model of dia-logue act sequences in conversation. By modeling topical themes as transitioning more slowly than dialogue acts in conver-sation, our model de-emphasizes content-related words in order to focus on con-versational function words that signal di-alogue acts. We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dia-logue act prevalence beyond temporal de-pendencies. According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the ef-fectiveness of each modeling assumption is found to vary depending on characteris-tics of the data. De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker ten-dencies is advantageous on the NPS cor-pus. The components of our model com-plement one another to achieve robust per-formance on both corpora and outperform state-of-the-art baseline models.},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Jo, Yohan and Yoder, Michael Miller and Jang, Hyeju and Rosé, Carolyn P},
	year = {2017},
	pages = {2179--2189},
}

@article{shen_style_2017,
	title = {Style {Transfer} from {Non}-{Parallel} {Text} by {Cross}-{Alignment}},
	url = {http://arxiv.org/abs/1705.09655},
	abstract = {This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.},
	number = {Nips},
	author = {Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	year = {2017},
	note = {arXiv: 1705.09655},
	pages = {1--12},
}

@article{kanai_sociality_2016,
	title = {Sociality and {Classification}: {Reading} {Gender}, {Race}, and {Class} in a {Humorous} {Meme}},
	volume = {2},
	issn = {2056-3051},
	url = {http://journals.sagepub.com/doi/10.1177/2056305116672884},
	doi = {10.1177/2056305116672884},
	abstract = {This article is concerned with how the gendered, raced, and classed practices of readership of a humorous meme on Tumblr organize forms of sociality and belonging along these lines. Based on the anonymous Tumblr blog, WhatShouldWeCallMe, the meme narrates feelings and reactions related to youthful, feminine, Western “everyday” experience through the use of captions and Graphics Interchange Format (GIF) images. Drawing on the feminist Cultural Studies tradition of text-reader analysis as well as New Literacy Studies approaches to literacy, I suggest the practices of readerly participation in the meme require a social rather than individual set of competencies and knowledges. I propose “spectatorial girlfriendship” as a term encompassing how the texts of the meme require the reader to operationalize gendered, classed, and raced classificatory knowledges and construct social forms of commonality on this basis. In the meme, the reader “gets” the joke by aligning an ostensibly incongruous GIF and caption, remixing and matching existing classifications of people, bodies, and objects. I demonstrate how spectatorial girlfriendship as a readerly lens arranges, transacts, and interacts gender, class, and race in multiple ways, indexing social inequalities without recognizing them as such. Bodies in the GIFs become “stock” images, used for selective resignification. Consequently, while offering pleasures of an understood readerly feminine commonality, participation in the meme is structured unequally, going beyond the reader’s ability to decipher the GIF and caption in the posts. The meme privileges an ideal reader constructed through postrace, postfeminist “theories” of the useability of gender, race, and class. Keywords},
	number = {4},
	journal = {Social Media + Society},
	author = {Kanai, Akane},
	year = {2016},
	keywords = {class, gender, humor, literacy, meme, race, reader},
	pages = {205630511667288},
}

@article{he_unsupervised_2017,
	title = {An {Unsupervised} {Neural} {Attention} {Model} for {Aspect} {Extraction}},
	url = {http://aclweb.org/anthology/P17-1036},
	doi = {10.18653/v1/P17-1036},
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	journal = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	year = {2017},
	pages = {388--397},
}

@article{giannakopoulos_unsupervised_2017,
	title = {Unsupervised {Aspect} {Term} {Extraction} with {B}-{LSTM} \& {CRF} using {Automatically} {Labelled} {Datasets}},
	url = {http://arxiv.org/abs/1709.05094},
	abstract = {Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and is one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA) contest. The small amount of available datasets for supervised ATE and the costly human annotation for aspect term labelling give rise to the need for unsupervised ATE. In this paper, we introduce an architecture that achieves top-ranking performance for supervised ATE. Moreover, it can be used efficiently as feature extractor and classifier for unsupervised ATE. Our second contribution is a method to automatically construct datasets for ATE. We train a classifier on our automatically labelled datasets and evaluate it on the human annotated SemEval ABSA test sets. Compared to a strong rule-based baseline, we obtain a dramatically higher F-score and attain precision values above 80\%. Our unsupervised method beats the supervised ABSA baseline from SemEval, while preserving high precision scores.},
	author = {Giannakopoulos, Athanasios and Musat, Claudiu and Hossmann, Andreea and Baeriswyl, Michael},
	year = {2017},
	note = {arXiv: 1709.05094},
	pages = {180--188},
}

@article{pavlopoulos_aspect_2014,
	title = {Aspect {Term} {Extraction} for {Sentiment} {Analysis}: {New} {Datasets}, {New} {Evaluation} {Measures} and an {Improved} {Unsupervised} {Method}},
	abstract = {Given a set of texts discussing a particular entity (e.g., customer reviews of a smart-phone), aspect based sentiment analysis (ABSA) identifies prominent aspects of the entity (e.g., battery, screen) and an aver-age sentiment score per aspect. We fo-cus on aspect term extraction (ATE), one of the core processing stages of ABSA that extracts terms naming aspects. We make publicly available three new ATE datasets, arguing that they are better than previously available ones. We also introduce new evaluation measures for ATE, again argu-ing that they are better than previously used ones. Finally, we show how a pop-ular unsupervised ATE method can be im-proved by using continuous space vector representations of words and phrases.},
	journal = {Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL},
	author = {Pavlopoulos, John and Androutsopoulos, Ion},
	year = {2014},
	note = {ISBN: 9781937284923},
	pages = {44--52},
}

@article{Suzuki2017,
	title = {Joint {Multimodal} {Learning} with {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1611.01891},
	abstract = {We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.},
	journal = {ICLR Workshop Track},
	author = {Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
	year = {2017},
	note = {arXiv: 1611.01891},
}

@article{persing_vote_2014,
	title = {Vote {Prediction} on {Comments} in {Social} {Polls}},
	url = {http://www.aclweb.org/anthology/D14-1119},
	journal = {Aclweb.Org},
	author = {Persing, Isaac and Ng, Vincent},
	year = {2014},
	note = {ISBN: 9781937284961},
	pages = {1127--1138},
}

@article{Tan2016,
	title = {Winning {Arguments}: {Interaction} {Dynamics} and {Persuasion} {Strategies} in {Good}-faith {Online} {Discussions}},
	url = {http://arxiv.org/abs/1602.01103\nhttp://dx.doi.org/10.1145/2872427.2883081},
	doi = {10.1145/2872427.2883081},
	abstract = {Changing someone's opinion is arguably one of the most important challenges of social interaction. The underlying process proves difficult to study: it is hard to know how someone's opinions are formed and whether and how someone's views shift. Fortunately, ChangeMyView, an active community on Reddit, provides a platform where users present their own opinions and reasoning, invite others to contest them, and acknowledge when the ensuing discussions change their original views. In this work, we study these interactions to understand the mechanisms behind persuasion. We find that persuasive arguments are characterized by interesting patterns of interaction dynamics, such as participant entry-order and degree of back-and-forth exchange. Furthermore, by comparing similar counterarguments to the same opinion, we show that language factors play an essential role. In particular, the interplay between the language of the opinion holder and that of the counterargument provides highly predictive cues of persuasiveness. Finally, since even in this favorable setting people may not be persuaded, we investigate the problem of determining whether someone's opinion is susceptible to being changed at all. For this more difficult task, we show that stylistic choices in how the opinion is expressed carry predictive power.},
	journal = {Proceedings of WWW 2016},
	author = {Tan, Chenhao and Niculae, Vlad and Danescu-Niculescu-Mizil, Cristian and Lee, Lillian},
	year = {2016},
	note = {arXiv: 1602.01103
ISBN: 9781450341431},
	pages = {613--624},
}

@inproceedings{Darwish2013,
	title = {Arabizi {Detection} and {Conversion} to {Arabic}},
	url = {http://arxiv.org/abs/1306.6755},
	abstract = {Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5\%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7\% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.},
	booktitle = {{ANLP} 2014},
	author = {Darwish, Kareem},
	year = {2014},
	note = {arXiv: 1306.6755},
	pages = {217},
}

@inproceedings{hamilton_diachronic_2016,
	title = {Diachronic {Word} {Embeddings} {Reveal} {Statistical} {Laws} of {Semantic} {Change}},
	isbn = {978-1-945626-00-5},
	url = {http://arxiv.org/abs/1605.09096},
	abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
	booktitle = {Proceedings of {ACL}},
	author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	year = {2016},
	note = {arXiv: 1605.09096},
	pages = {1489--1501},
}

@article{paul_mixed_2012,
	title = {Mixed {Membership} {Markov} {Models} for {Unsupervised} {Conversation} {Modeling}},
	url = {http://www.aclweb.org/anthology/D12-1009},
	number = {July},
	journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	author = {Paul, Michael J.},
	year = {2012},
	note = {ISBN: 9781937284435},
	pages = {94--104},
}

@article{rashkin_connotation_2016,
	title = {Connotation {Frames}: {A} {Data}-{Driven} {Investigation}},
	abstract = {Through a particular choice of a predicate (e.g., “x violated y”), a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y: (1) writer’s perspective: projecting x as an “antagonist” and y as a “victim”, (2) entities’ perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be in- duced from various data sources that reflect how language is used in context. We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media.},
	number = {5},
	journal = {Acl},
	author = {Rashkin, Hannah and Singh, Sameer and Choi, Yejin},
	year = {2016},
	note = {arXiv: 1506.02739
ISBN: 9781510827585},
	pages = {311--321},
}

@article{Gweon2013,
	title = {Measuring {Prevalence} of {Other}-{Oriented} {Transactive} {Contributions} {Using} an {Automated} {Measure} of {Speech} {Style} {Accommodation}},
	volume = {8},
	issn = {1556-1607},
	url = {http://dx.doi.org/10.1007/s11412-013-9172-5},
	doi = {10.1007/s11412-013-9172-5},
	abstract = {This paper contributes to a theory-grounded methodological foundation for automatic collaborative learning process analysis. It does this by illustrating how insights from the social psychology and sociolinguistics of speech style provide a theoretical framework to inform the design of a computational model. The purpose of that model is to detect prevalence of an important group knowledge integration process in raw speech data. Specifically, this paper focuses on assessment of transactivity in dyadic discussions, where a transactive contribution is operationalized as one where reasoning is made explicit, and where that reasoning builds on a prior reasoning statement within the discussion. Transactive contributions can be either self-oriented, where the contribution builds on the speaker's own prior contribution, or other-oriented, where the contribution builds on a prior contribution of a conversational partner. Other-oriented transacts are particularly central to group knowledge integration processes. An unsupervised Dynamic Bayesian Network model motivated by concepts from Speech Accommodation Theory is presented and then evaluated on the task of estimating prevalence of other-oriented transacts in dyadic discussions. The evaluation demonstrates a significant positive correlation between an automatic measure of speech style accommodation and prevalence of other-oriented transacts (R = 0.36, p less than 0.05).},
	number = {2},
	journal = {International Journal of Computer-Supported Collaborative Learning},
	author = {Gweon, Gahgene and Jain, Mahaveer and McDonough, John and Raj, Bhiksha and Rose, Carolyn P and Ros??, Carolyn P.},
	year = {2013},
	keywords = {Cooperative Learning, Incidence, Machine learning, Measurement, Spee, Speech style accommodation, Speech-based assessment, Transactivity},
	pages = {245--265},
}

@inproceedings{alvarez1990rethinking,
	title = {Rethinking conversational code-switching: {Codes}, speech varieties, and contextualization},
	volume = {16},
	booktitle = {Annual {Meeting} of the {Berkeley} {Linguistics} {Society}},
	author = {Alvarez-Cáccamo, Celso},
	year = {1990},
	note = {Issue: 1},
	pages = {3--16},
}

@inproceedings{Kiesling2012,
	title = {Operationalizing {Stance} as an {Independent} {Variable}},
	booktitle = {Nwav 41},
	author = {Kiesling, Scott and Onuffer, Spencer and Hardware, Alexander},
	year = {2012},
	pages = {2007},
}

@article{Kiesling2004,
	title = {Dude},
	volume = {79},
	number = {3},
	journal = {American Speech},
	author = {Kiesling, Scott F.},
	year = {2004},
	pages = {281--305},
}

@article{rijhwani_estimating_2017,
	title = {Estimating {Code}-{Switching} on {Twitter} with a {Novel} {Generalized} {Word}-{Level} {Language} {Detection} {Technique}},
	url = {https://doi.org/10.18653/v1/P17-1180},
	doi = {10.18653/v1/P17-1180},
	abstract = {Word-level language detection is neces-sary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large num-ber of languages, which does not require any manually annotated training data. Our experiments with tweets in seven lan-guages show a 74\% relative error reduc-tion in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quanti-tative analysis of code-switching patterns on Twitter, both global as well as region-specific, with 58M tweets.},
	author = {Rijhwani, Shruti and Sequiera, Royal and Choudhury, Monojit and Bali, Kalika and Maddila, Chandra Sekhar},
	year = {2017},
	pages = {1971--1982},
}

@article{Srivastava2012,
	title = {Multimodal {Learning} with {Deep} {Boltzmann} {Machines}},
	issn = {10495258},
	doi = {10.1109/CVPR.2013.49},
	abstract = {A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a unified representation that fuses modalities together. We find that this representation is useful for classification and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and filling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model significantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains.},
	journal = {Advances in neural information processing systems (NIPS)},
	author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
	year = {2012},
	pmid = {2479307},
	note = {ISBN: 978-0-7695-4989-7},
	pages = {2222--2230},
}

@article{pasha_madamira_2014,
	title = {{MADAMIRA} : {A} {Fast} , {Comprehensive} {Tool} for {Morphological} {Analysis} and {Disambiguation} of {Arabic}},
	abstract = {In this paper, we present MADAMIRA, a system for morphological analysis and disambiguation of Arabic that combines some of the best aspects of two previously commonly used systems for Arabic processing, MADA (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2013) and AMIRA (Diab et al., 2007). MADAMIRA improves upon the two systems with a more streamlined Java implementation that is more robust, portable, extensible, and is faster than its ancestors by more than an order of magnitude. We also discuss an online demo (see http://nlp.ldeo.columbia.edu/madamira/) that highlights these aspects.},
	journal = {Proceedings of the 9th Language Resources and Evaluation Conference (LREC'14)},
	author = {Pasha, Arfath and Al-badrashiny, Mohamed and Diab, Mona and Kholy, Ahmed El and Eskander, Ramy and Habash, Nizar and Pooleery, Manoj and Rambow, Owen and Roth, Ryan M},
	year = {2014},
	note = {ISBN: 978-2-9517408-8-4},
	keywords = {a fast, ahmed el kholy, amira, and ryan m, arfath pasha, comprehensive tool, disambiguation of arabic, for morphological analysis and, manoj pooleery, mohamed al-badrashiny, mona diab, nizar habash, owen rambow, ramy eskander, roth},
	pages = {1094--1101},
}

@book{kaufer2004power,
	title = {The power of words: unveiling the speaker and writer's hidden craft},
	publisher = {Routledge},
	author = {Kaufer, David S and Ishizaki, Suguru and Butler, Brian S and Collins, Jeff},
	year = {2004},
}

@article{beauregard1992collaborative,
	title = {Collaborative strategies for reindustrialization: {Sheffield} and {Pittsburgh}},
	volume = {6},
	number = {4},
	journal = {Economic Development Quarterly},
	author = {Beauregard, Robert A and Lawless, Paul and Deitrick, Sabina},
	year = {1992},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {418--430},
}

@article{crook_sequence_2017,
	title = {Sequence to {Sequence} {Modeling} for {User} {Simulation} in {Dialog} {Systems}},
	doi = {10.21437/Interspeech.2017-161},
	abstract = {User simulators are a principal offline method for training and evaluating human-computer dialog systems. In this paper, we examine simple sequence-to-sequence neural network architec-tures for training end-to-end, natural language to natural lan-guage, user simulators, using only raw logs of previous inter-actions without any additional human labelling. We compare the neural network-based simulators with a language model (LM)-based approach for creating natural language user simu-lators. Using both an automatic evaluation using LM perplexity and a human evaluation, we demonstrate that the sequence-to-sequence approaches outperform the LM-based method. We show correlation between LM perplexity and the human eval-uation on this task, and discuss the benefits of different neural network architecture variations.},
	journal = {Interspeech2017},
	author = {Crook, Paul and Marin, Alex},
	year = {2017},
	keywords = {[Electronic Manuscript]},
	pages = {1706--1710},
}

@article{kiseleva_predicting_2016,
	title = {Predicting {User} {Satisfaction} with {Intelligent} {Assistants}},
	url = {http://dl.acm.org/citation.cfm?doid=2911451.2911521},
	doi = {10.1145/2911451.2911521},
	abstract = {Voice-controlled intelligent personal assistants, such as Cortana, Google Now, Siri and Alexa, are increasingly becoming a part of users' daily lives, especially on mobile devices. They introduce a significant change in information access, not only by introduc-ing voice control and touch gestures but also by enabling dialogues where the context is preserved. This raises the need for evaluation of their effectiveness in assisting users with their tasks. However, in order to understand which type of user interactions reflect differ-ent degrees of user satisfaction we need explicit judgements. In this paper, we describe a user study that was designed to measure user satisfaction over a range of typical scenarios of use: controlling a device, web search, and structured search dialogue. Using this data, we study how user satisfaction varied with different usage scenar-ios and what signals can be used for modeling satisfaction in the different scenarios. We find that the notion of satisfaction varies across different scenarios, and show that, in some scenarios (e.g. making a phone call), task completion is very important while for others (e.g. planning a night out), the amount of effort spent is key. We also study how the nature and complexity of the task at hand affects user satisfaction, and find that preserving the conversation context is essential and that overall task-level satisfaction cannot be reduced to query-level satisfaction alone. Finally, we shed light on the relative effectiveness and usefulness of voice-controlled in-telligent agents, explaining their increasing popularity and uptake relative to the traditional query-response interaction.},
	journal = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
	author = {Kiseleva, Julia and Williams, Kyle and Hassan Awadallah, Ahmed and Crook, Aidan C. and Zitouni, Imed and Anastasakos, Tasos},
	year = {2016},
	note = {ISBN: 9781450340694},
	keywords = {H.3 [Information Storage and Retrieval], a hotel close to, can you find me, ence, in mountain view, intelligent assistant, mobile search, mountain view, q2, q3, s the weather like, spoken dialogue system, user experi-, user satisfaction, user study, what},
	pages = {45--54},
}

@article{mehrotra_user_2017,
	title = {User {Interaction} {Sequences} for {Search} {Satisfaction} {Prediction}},
	url = {http://dl.acm.org/citation.cfm?doid=3077136.3080833},
	doi = {10.1145/3077136.3080833},
	journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval  - SIGIR '17},
	author = {Mehrotra, Rishabh and Zitouni, Imed and Hassan Awadallah, Ahmed and Kholy, Ahmed El and Khabsa, Madian},
	year = {2017},
	note = {ISBN: 9781450350228},
	pages = {165--174},
}

@article{wu_citeseerx_2014,
	title = {{CiteSeerX} : {AI} in a {Digital} {Library} {Search} {Engine}},
	issn = {07384602},
	abstract = {CiteSeerX is a digital library search engine that pro- vides access to more than 4 million academic docu- ments with nearly a million users and millions of hits per day. Artificial intelligence (AI) technologies are used in many components of CiteSeerX e.g. to accurately extract metadata, intelligently crawl the web, and ingest documents. We present key AI technologies used in the following components: document classification and deduplication, document and citation clustering, automatic metadata extraction and indexing, and author disambiguation. These AI technologies have been developed by CiteSeerX group members over the past 5–6 years. We also show the usage status, payoff, development challenges, main design concepts, and deployment and maintenance requirements. While it is challenging to rebuild a system like CiteSeerX from scratch, many of these AI technologies are transferable to other digital libraries and/or search engines.},
	journal = {Proceedings of the Twenty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence},
	author = {Wu, Jian and Williams, Kyle and Chen, Hung-hsuan and Khabsa, Madian and Caragea, Cornelia and Ororbia, Alexander and Jordan, Douglas and Giles, C Lee},
	year = {2014},
	note = {ISBN: 9781577356806},
	keywords = {Deployed Application Papers},
	pages = {2930--2937},
}

@article{williams_scholarly_2014,
	title = {Scholarly big data information extraction and integration in the {CiteSeer}?? digital library},
	issn = {10844627},
	doi = {10.1109/ICDEW.2014.6818305},
	abstract = {CiteSeerχ is a digital library that contains approximately 3.5 million scholarly documents and receives between 2 and 4 million requests per day. In addition to making documents available via a public Website, the data is also used to facilitate research in areas like citation analysis, co-author network analysis, scalability evaluation and information extraction. The papers in CiteSeerχ are gathered from the Web by means of continuous automatic focused crawling and go through a series of automatic processing steps as part of the ingestion process. Given the size of the collection, the fact that it is constantly expanding, and the multiple ways in which it is used both by the public to access scholarly documents and for research, there are several big data challenges. In this paper, we provide a case study description of how we address these challenges when it comes to information extraction, data integration and entity linking in CiteSeer χ. We describe how we: aggregate data from multiple sources on the Web; store and manage data; process data as part of an automatic ingestion pipeline that includes automatic metadata and information extraction; perform document and citation clustering; perform entity linking and name disambiguation; and make our data and source code available to enable research and collaboration. © 2014 IEEE.},
	journal = {Proceedings - International Conference on Data Engineering},
	author = {Williams, Kyle and Wu, Jian and Choudhury, Sagnik Ray and Khabsa, Madian and Giles, C. Lee},
	year = {2014},
	note = {ISBN: 9781479934805},
	keywords = {[Electronic Manuscript]},
	pages = {68--73},
}

@article{williams_information_2016,
	title = {Information {Extraction} for {Scholarly} {Digital} {Libraries}},
	author = {Williams, Kyle and Wu, Jian and Wu, Zhaohui and Giles, C Lee},
	year = {2016},
	note = {ISBN: 9781450342292},
	keywords = {digital libraries, information extraction, scholarly big data},
	pages = {287--288},
}

@article{williams_does_2017,
	title = {Does {That} {Mean} {You}'re {Happy}?},
	url = {http://dl.acm.org/citation.cfm?doid=3132847.3133035},
	doi = {10.1145/3132847.3133035},
	journal = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management  - CIKM '17},
	author = {Williams, Kyle and Zitouni, Imed},
	year = {2017},
	note = {ISBN: 9781450349185},
	keywords = {good abandonment, lstm, satisfaction, user interaction modeling},
	pages = {727--736},
}

@article{bloom_unsupervised_2010,
	title = {Unsupervised extraction of appraisal expressions},
	volume = {6085 LNAI},
	issn = {03029743},
	doi = {10.1007/978-3-642-13059-5_31},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bloom, Kenneth and Argamon, Shlomo},
	year = {2010},
	note = {ISBN: 3642130585},
	pages = {290--294},
}

@article{mohammad_semeval-2016_2016,
	title = {{SemEval}-2016 {Task} 6: {Detecting} {Stance} in {Tweets}},
	volume = {16},
	doi = {10.18653/v1/S16-1003},
	abstract = {Here for the first time we present a shared task on detecting stance from tweets: given a tweet and a target entity (person, organiza-tion, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely. The target of interest may or may not be referred to in the tweet, and it may or may not be the tar-get of opinion. Two tasks are proposed. Task A is a traditional supervised classification task where 70\% of the annotated data for a target is used as training and the rest for testing. For Task B, we use as test data all of the instances for a new target (not used in task A) and no training data is provided. Our shared task re-ceived submissions from 19 teams for Task A and from 9 teams for Task B. The highest clas-sification F-score obtained was 67.82 for Task A and 56.28 for Task B. However, systems found it markedly more difficult to infer stance towards the target of interest from tweets that express opinion towards another entity.},
	number = {July},
	journal = {Proceedings of SemEval},
	author = {Mohammad, Saif M and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
	year = {2016},
	pages = {31--41},
}

@article{benamara_evaluative_2016,
	title = {Evaluative {Language} {Beyond} {Bags} {ofWords}: {Linguistic} {Insights} and {Computational} {Applications}},
	volume = {43},
	issn = {04194217},
	url = {http://arxiv.org/abs/1604.07370},
	doi = {10.1162/COLI},
	abstract = {In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using integer linear programming. We show that our model considerably improves the performance of base classifiers and significantly outperforms challenging heuristic baselines. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement. This corpus and the annotation guidelines are freely available for ensuring reproducibility and to encourage future research in computational argumentation.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Benamara, Farah and Taboada, Maite and Mathieu, Yannick},
	year = {2016},
	pmid = {22251136},
	note = {arXiv: 1604.07370
ISBN: 9781608459858},
}

@article{read_annotating_2012,
	title = {Annotating expressions of {Appraisal} in {English}},
	volume = {46},
	issn = {1574020X},
	doi = {10.1007/s10579-010-9135-7},
	abstract = {In the context of Systemic Functional Linguistics, Appraisal is a theory describing the types of language utilised in communicating emotion and opinion. Robust automatic analyses of Appraisal could contribute in a number of ways to computational sentiment analysis by: distinguishing various types of evaluation, for example affect, ethics or aesthetics; discriminating between an author’s opinions and the opinions of authors referenced by the author and determining the strength of evaluations. This paper reviews the typology described by Appraisal, presents a methodology for annotating Appraisal, and the use of this to annotate a corpus of book reviews. It discusses an inter-annotator agreement study, and considers instances of systematic disagreement that indicate areas in which Appraisal may be refined or clarified. Although the annotation task is difficult, there are many instances where the annotators agree; these are used to create a gold-standard corpus for future experimentation with Appraisal. Keywords},
	number = {3},
	journal = {Language Resources and Evaluation},
	author = {Read, Jonathon and Carroll, John},
	year = {2012},
	note = {arXiv: astro-ph/0507464v2
ISBN: 1057901091},
	keywords = {Appraisal, Corpus annotation, Inter-annotator agreement, Opinion, Subjectivity, Systemic Functional Linguistics},
	pages = {421--447},
}

@article{read_weakly-supervised_2012,
	title = {Weakly-supervised {Appraisal} {Analysis}},
	volume = {8},
	number = {2},
	journal = {Linguistic Issues in Language Technology},
	author = {Read, Jonathon and Carroll, John},
	year = {2012},
	pages = {1--21},
}

@article{ward_inferring_2017,
	title = {Inferring {Stance} in {News} {Broadcasts} from {Prosodic}-{Feature} {Configurations}},
	author = {Ward, Nigel G and Carlson, Jason C and Fuentes, Olac},
	year = {2017},
	keywords = {american english, attitude, broadcast news, information retrieval, prosody},
	pages = {1--26},
}

@article{deng_joint_2015,
	title = {Joint {Prediction} for {Entity}/{Event}-{Level} {Sentiment} {Analysis} using {Probabilistic} {Soft} {Logic} {Models}},
	url = {http://aclweb.org/anthology/D15-1018},
	abstract = {In thiswork, we build an entity/event-level sentiment analysis system, which is able to recognize and infer both explicit and im- plicit sentiments toward entities and events in the text. We design Probabilistic Soft Logic models that integrate explicit senti- ments, inference rules, and +/-effect event information (events that positively or neg- atively affect entities). The experiments show that the method is able to greatly im- prove over baseline accuracies in recog- nizing entity/event-level sentiments},
	number = {September},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	author = {Deng, Lingjia and Wiebe, Janyce},
	year = {2015},
	note = {ISBN: 9781941643327},
	pages = {179--189},
}

@article{deng_recognizing_2016,
	title = {Recognizing opinion sources based on a new categorization of opinion types},
	volume = {2016-Janua},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Deng, Lingjia and Wiebe, Janyce},
	year = {2016},
	keywords = {Natural Language Processing},
	pages = {2775--2781},
}

@article{deng_mpqa_2015,
	title = {{MPQA} 3.0: {An} {Entity}/{Event}-{Level} {Sentiment} {Corpus}},
	abstract = {This paper presents an annotation scheme for adding entity and event target annotations to theMPQAcorpus, a rich span-annotated opin- ion corpus. The new corpus promises to be a valuable new resource for developing sys- tems for entity/event-level sentiment analysis. Such systems, in turn, would be valuable in NLP applications such as Automatic Question Answering. We introduce the idea of entity and event targets (eTargets), describe the an- notation scheme, and present the results of an agreement study. 1},
	journal = {NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Deng, Lingjia},
	year = {2015},
	note = {ISBN: 9781941643495},
	pages = {1323--1328},
}

@misc{xie_developing_nodate,
	title = {Developing and {Deconstructing} {Gender}: the {Nonbinary} {Search} for {Identity}},
	author = {Xie, Jasmine},
}

@incollection{jurafsky_information_2017,
	title = {Information extraction},
	isbn = {978-1-60198-188-2},
	url = {http://dl.acm.org/citation.cfm?id=234209},
	booktitle = {Speech and {Language} {Processing}},
	author = {Jurafsky, Daniel and Martin, James},
	year = {2017},
	note = {Issue: November},
}

@article{choi_+/-effectwordnet:_2014,
	title = {+/-{EffectWordNet}: {Sense}-level {Lexicon} {Acquisition} for {Opinion} {Inference}},
	url = {http://www.aclweb.org/anthology/D14-1125},
	abstract = {Recently, work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (+/-effect events). This paper addresses methods for creating a lexicon of such events, to support such work on opinion inference. Due to significant sense ambiguity, our goal is to develop a sense-level rather than word-level lexicon. To maximize the effectiveness of different types of information, we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set.},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	author = {Choi, Yoonjung and Wiebe, Janyce},
	year = {2014},
	note = {ISBN: 9781937284961},
	pages = {1181--1191},
}

@article{sukhbaatar_end--end_2015,
	title = {End-{To}-{End} {Memory} {Networks}},
	issn = {10495258},
	url = {http://arxiv.org/abs/1503.08895},
	doi = {v5},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	year = {2015},
	pmid = {9377276},
	note = {arXiv: 1503.08895
ISBN: 1551-6709},
	pages = {1--11},
}

@article{Gong2013,
	title = {Hashtag {Recommendation} {Using} {Dirichlet} {Process} {Mixture} {Models} {Incorporating} {Types} of {Hashtags}},
	journal = {[EMNLP2015]Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	author = {Gong, Yeyun and Zhang, Qi and Huang, Xuanjing},
	year = {2015},
}

@article{agre_toward_1997,
	title = {Toward a {Critical} {Technical} {Practice}: {Lessons} {Learned} in {Trying} to {Reform} {AI}},
	url = {http://polaris.gseis.ucla.edu/pagre/},
	abstract = {Every technology fits, in its own unique way, into a far-flung network of different sites of social practice. Some technologies are employed in a specific site, and in those cases we often feel that we can warrant clear cause-and-effect stories about the transformations that have accompanied them, either in that site or others. Other technologies are so ubiquitous - found contributing to the evolution of the activities and relationships of so many distinct sites of practice - that we have no idea how to begin reckoning their effects upon society, assuming that such a global notion of "effects" even makes sense. Computers fall in this latter category of ubiquitous technologies. In fact, from an analytical standpoint, computers are worse than that. Computers are representational artifacts, and the people who design them often start by constructing representations of the activities that are found in the sites where they will be used. This is the purpose of systems analysis, for example, and of the systematic mapping of conceptual entities and relationships in the early stages of database design. A computer, then, does not simply have an instrumental use in a given site of practice; the computer is frequently about that site in its very design. In this sense computing has been constituted as a kind of imperialism; it aims to reinvent virtually every other site of practice in its own image.},
	journal = {Social Science Technical Systems and Cooperative Work Beyond the Great Divide},
	author = {Agre, Philip E},
	year = {1997},
	note = {ISBN: 0805824030},
	pages = {1--17},
}

@article{chandrasekharan_you_2017,
	title = {You {Can}’t {Stay} {Here}: {The} {Efficacy} of {Reddit}’s 2015 {Ban} {Examined} {Through} {Hate} {Speech}},
	volume = {1},
	doi = {10.1145/3134666},
	abstract = {In 2015, Reddit closed several subreddits—foremost among them r/fatpeoplehate and r/CoonTown—due to violations of Reddit’s anti-harassment policy. However, the effectiveness of banning as a moderation approach remains unclear: banning might diminish hateful behavior, or it may relocate such behavior to different parts of the site. We study the ban of r/fatpeoplehate and r/CoonTown in terms of its effect on both participating users and affected subreddits. Working from over 100M Reddit posts and comments, we generate hate speech lexicons to examine variations in hate speech usage via causal inference methods. We find that the ban worked for Reddit. More accounts than expected discontinued using the site; those that stayed drastically decreased their hate speech usage—by at least 80\%. Though many subreddits saw an influx of r/fatpeoplehate and r/CoonTown “migrants,” those subreddits saw no significant changes in hate speech usage. In other words, other subreddits did not inherit the problem. We conclude by reflecting on the apparent success of the ban, discussing implications for online moderation, Reddit and internet communities more broadly.},
	number = {2},
	journal = {Proc. ACM Hum.-Comput. Interact},
	author = {Chandrasekharan, Eshwar and Pavalanathan, Umashanthi and Srinivasan, Anirudh and Glynn, Adam and Eisenstein, Jacob and Gilbert, Eric},
	year = {2017},
	pages = {1--22},
}

@article{lloret_towards_2012,
	title = {Towards a unified framework for opinion retrieval, mining and summarization},
	volume = {39},
	issn = {09259902},
	doi = {10.1007/s10844-012-0209-4},
	abstract = {The exponential increase of subjective, user-generated content since the birth of the Social Web, has led to the necessity of developing automatic text processing systems able to extract, process and present relevant knowledge. In this paper, we tackle the Opinion Retrieval, Mining and Summarization task, by proposing a unified framework, composed of three crucial components (information retrieval, opinion mining and text summarization) that allow the retrieval, classification and summarization of subjective information. An extensive analysis is conducted, where different configurations of the framework are suggested and analyzed, in order to determine which is the best one, and under which conditions. The evaluation carried out and the results obtained show the appropriateness of the individual components, as well as the framework as a whole. By achieving an improvement over 10\% compared to the state-of-the-art approaches in the context of blogs, we can conclude that subjective text can be efficiently dealt with by means of our proposed framework.},
	number = {3},
	journal = {Journal of Intelligent Information Systems},
	author = {Lloret, Elena and Balahur, Alexandra and Gómez, José M. and Montoyo, Andrés and Palomar, Manuel},
	year = {2012},
	keywords = {Information retrieval, Intelligent system, Mining and summarization framework, Opinion mining, Opinion retrieval, Text summarization},
	pages = {711--747},
}

@article{ranade_online_2013,
	title = {Online debate summarization using topic directed sentiment analysis},
	url = {http://dl.acm.org/citation.cfm?doid=2502069.2502076},
	doi = {10.1145/2502069.2502076},
	abstract = {Social networking sites provide users a virtual community interaction platform to share their thoughts, life experiences and opinions. Online debate forum is one such platform where people can take a stance and argue in support or opposition of debate topics. An important feature of such forums is that, they are dynamic and increase rapidly. In such situations, effective opinion summarization approaches are needed so that readers need not go through the entire debate. This paper aims to summarize online debates by extracting highly topic relevant and sentiment rich sentences. The proposed approach takes into account topic relevant, document relevant and sentiment based features to capture topic opinionated sentences. ROUGE scores are used to evaluate our system. Our system significantly outperforms several baseline systems and show 5:2\% (ROUGE-1), 7:3\% (ROUGE-2) and 5:5\% (ROUGE-L) improvement over the state-of-the-art opinion summarization system. The results verify that topic directed sentiment features are most important to generate effective debate summaries.},
	journal = {Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining - WISDOM '13},
	author = {Ranade, Sarvesh and Gupta, Jayant and Varma, Vasudeva and Mamidi, Radhika},
	year = {2013},
	note = {ISBN: 9781450323321},
	pages = {1--6},
}

@article{ng_supervised_2010,
	title = {Supervised {Noun} {Phrase} {Coreference} {Research}: {The} {First} {Fifteen} {Years}},
	issn = {10772626},
	doi = {10.1109/TVCG.2007.24},
	abstract = {The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.},
	number = {July},
	journal = {Acl},
	author = {Ng, Vincent},
	year = {2010},
	pmid = {17093346},
	note = {ISBN: 9781617388088},
	pages = {1396--1411},
}

@article{Jang2016,
	title = {Metaphor {Detection} with {Topic} {Transition}, {Emotion} and {Cognition} in {Context}},
	volume = {1},
	url = {http://www.aclweb.org/anthology/P16-1021},
	journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Jang, Hyeju and Jo, Yohan and Shen, Qinlan and Miller, Michael and Moon, Seungwhan and Rosé, Carolyn P.},
	year = {2016},
	pages = {216--225},
}

@incollection{kaufer_cataloging_nodate,
	title = {Cataloging {English} {Strings} for {Their} {Priming} {Potencies} : {A} {Report} of a {Research} {Study}},
	booktitle = {The {Power} of {Words}: {Unveiling} the {Speaker} and the {Writer}'s {Hidden} {Craft}},
	author = {Kaufer, David},
	pages = {19--36},
}

@incollection{kaufer_priming_2003,
	title = {Priming {Audience} and {Practices} of {Literacy}},
	booktitle = {The {Power} of {Words}: {Unveiling} the {Speaker} and the {Writer}'s {Hidden} {Craft}},
	author = {Kaufer, David},
	year = {2003},
	pages = {3--18},
}

@incollection{kaufer_introduction_2003,
	title = {{INTRODUCTION} : {WORDS} {AND} {THEIR} {POTENCY}},
	booktitle = {The {Power} of {Words}: {Unveiling} the {Speaker} and the {Writer}'s {Hidden} {Craft}},
	author = {Kaufer, David},
	year = {2003},
}

@misc{zalmout_unsupervised_nodate,
	title = {Unsupervised {Neologism} {Normalization} {Using} {Embedding} {Space} {Mapping}},
	author = {Zalmout, Nasser and Pappu, Aasish and Thadani, Kapil},
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	issn = {0253-0465},
	url = {http://arxiv.org/abs/1701.00160},
	doi = {10.1001/jamainternmed.2016.8245},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	author = {Goodfellow, Ian},
	year = {2016},
	pmid = {15040217},
	note = {arXiv: 1701.00160
ISBN: 1581138285},
}

@article{hyland_disciplinary_2004,
	title = {Disciplinary {Discourses}: {Social} {Interactions} in {Academic} {Writing}},
	volume = {58},
	issn = {10182101},
	abstract = {No abstract is available for this item},
	number = {3},
	journal = {Journal of Documentation},
	author = {Hyland, Ken},
	year = {2004},
	pmid = {2550794},
	note = {Publisher: University of Michigan Press
ISBN: 0582419042},
}

@book{borsboom2005measuring,
	title = {Measuring the mind: {Conceptual} issues in contemporary psychometrics},
	publisher = {Cambridge University Press},
	author = {Borsboom, Denny},
	year = {2005},
}

@article{rose2017artificial,
	title = {A social spin on language analysis},
	volume = {545},
	number = {7653},
	journal = {Nature},
	author = {Rosé, Carolyn Penstein},
	year = {2017},
	note = {Publisher: Nature Research},
	pages = {166--167},
}

@article{ravi_learning_2009,
	title = {Learning phoneme mappings for transliteration without parallel data},
	url = {http://portal.acm.org/citation.cfm?doid=1620754.1620761},
	doi = {10.3115/1620754.1620761},
	number = {June},
	journal = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL '09},
	author = {Ravi, Sujith and Knight, Kevin},
	year = {2009},
	note = {ISBN: 9781932432411},
	pages = {37},
}

@article{eskander_foreign_2014,
	title = {Foreign {Words} and the {Automatic} {Processing} of {Arabic} {Social} {Media} {Text} {Written} in {Roman} {Script}},
	abstract = {Arabic on social media has all the prop-erties of any language on social media that make it tough for natural language processing, plus some specific problems. These include diglossia, the use of an alternative alphabet (Roman), and code switching with foreign languages. In this paper, we present a system which can process Arabic written in Roman alpha-bet (" Arabizi "). It identifies whether each word is a foreign word or one of an-other four categories (Arabic, name, punc-tuation, sound), and transliterates Arabic words and names into the Arabic alphabet. We obtain an overall system performance of 83.8\% on an unseen test set.},
	author = {Eskander, Ramy and Al-Badrashiny, Mohamed and Habash, Nizar and Rambow, Owen},
	year = {2014},
	pages = {1--12},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	author = {Doersch, Carl},
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {neural networks, prediction, structured, unsupervised learning, variational autoencoders},
	pages = {1--23},
}

@article{Zhang2014,
	title = {Time-aware personalized hashtag recommendation on social media},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959863839&partnerID=40&md5=abc82215d8a8c905394733f7b79145fc},
	abstract = {The task of recommending hashtags for microblogs has been received considerable attention in recent years, and many applications can reap enormous benefits from it. Various approaches have been proposed to study the problem from different aspects. However, the impacts of temporal and personal factors have rarely been considered in the existing methods. In this paper, we propose a novel method that extends the translation based model and incorporates the temporal and personal factors. To overcome the limitation of only being able to recommend hashtags that exist in the training data of the existing methods, the proposed method also incorporates extraction strategies into it. The results of experiments on the data collected from real world microblogging services by crawling demonstrate that the proposed method outperforms state-of-the-art methods that do not consider these aspects. The relative improvement of the proposed method over the method without considering these aspects is around 47.8\% in F1-score.},
	journal = {25th International Conference on Computational Linguistics, COLING 2014},
	author = {Zhang, Qi and Gong, Yeyun and Sun, Xuyang and Huang, Xuanjing},
	year = {2014},
	note = {ISBN: 9781941643266 (ISBN)},
	keywords = {Computational linguistics, F1 scores, Hashtags, Linguistics, Micro-blogging services, Microblogs, Real-world, Social media, Social networking (online), State-of-the-art methods, Training data, World Wide Web},
	pages = {203--212},
}

@article{Xu2014,
	title = {Rolling {Through} {Tumblr}: {Characterizing} {Behavioral} {Patterns} of the {Microblogging} {Platform}},
	doi = {10.1145/2615569.2615694},
	abstract = {Tumblr, a microblogging platform and social media website, has been gaining popularity over the past few years. Despite its success, little has been studied on the human behavior and interaction on this platform. This is important as it sheds light on the driving force behind Tumblr's growth. In this work, we present a quantitative study of Tumblr based on the complete data coverage for four consecutive months consisting of 23.2 million users and 10.2 billion posts. We first explore various attributes of users, posts, and tags in detail and extract behavioral patterns based on the user generated content. We then construct a massive reblog network based on the primary user interactions on Tumblr and present findings on analyzing its topological structure and properties. Finally, we show substantial results on providing location-specific usage patterns from Tumblr, despite no built-in support for geo-tagging or user location functionality. Essentially this is done by conducting a large-scale user alignment with a different social media platform (e.g., Twitter) and subsequently propagating geo-information across platforms. To the best of our knowledge, this work is the first attempt to carry out large-scale measurement-driven analysis on Tumblr.},
	number = {April 2015},
	journal = {WebSci},
	author = {Xu, Jiejun and Compton, Ryan and Lu, Tsai-Ching and Allen, David},
	year = {2014},
	note = {ISBN: 978-1-4503-2622-3},
	keywords = {all or part of, location-based patterns, online social network, or, or hard copies of, permission to make digital, quantitative methods, this work for personal, tumblr},
	pages = {13--22},
}

@inproceedings{ounis2011overview,
	title = {Overview of the trec-2011 microblog track},
	volume = {32},
	booktitle = {Proceeddings of the 20th {Text} {REtrieval} {Conference} ({TREC} 2011)},
	author = {Ounis, Iadh and Macdonald, Craig and Lin, Jimmy and Soboroff, Ian},
	year = {2011},
}

@article{ounis_overview_2014,
	title = {Overview of the {TREC}-2012 microblog track},
	author = {Ounis, Iadh and Soboroff, Craig and Lin, Jimmy and Soboroff, Ian},
	year = {2014},
	note = {ISBN: 3857291841983},
}

@inproceedings{Xu2014,
	title = {Civil unrest prediction: {A} tumblr-based exploration},
	booktitle = {Social {Computing}, {Behavioral}-{Cultural} {Modeling} and {Prediction}},
	author = {Xu, Jiejun and Lu, Tsai-Ching and Compton, Ryan and Allen, David},
	year = {2014},
	keywords = {big data, early event detection, social media},
	pages = {403--411},
}

@inproceedings{Shin2015,
	title = {Tumblr {Blog} {Recommendation} with {Boosted} {Inductive} {Matrix} {Completion}},
	isbn = {978-1-4503-3794-6},
	doi = {10.1145/2806416.2806578},
	abstract = {Popular microblogging sites such as Tumblr have attracted hundreds of millions of users as a content sharing platform, where users can create rich content in the form of posts that are shared with other users who follow them. Due to the sheer amount of posts created on such services, an im-portant task is to make quality recommendations of blogs for users to follow. Apart from traditional recommender system settings where the follower graph is the main data source, additional side-information of users and blogs such as user activity (e.g., like and reblog) and rich content (e.g., text and images) are also available to be exploited for en-hanced recommendation performance. In this paper, we pro-pose a novel boosted inductive matrix completion method (BIMC) for blog recommendation. BIMC is an additive low-rank model for user-blog preferences consisting of two components; one component captures the low-rank struc-ture of follow relationships and the other captures the la-tent structure using side-information. Our model formula-tion combines the power of the recently proposed inductive matrix completion (IMC) model (for side-information) to-gether with a standard matrix completion (MC) model (for low-rank structure). Furthermore, we utilize recently devel-oped deep learning techniques to obtain semantically rich feature representations of text and images that are incor-porated in BIMC. Experiments on a large-scale real-world dataset from Tumblr illustrate the effectiveness of the pro-posed BIMC method. (a) Blog recommendation module in Tumblr (b) Example post with high note count. Figure 1: The blog recommendation module (a) and an example post (b) with high note count (i.e., like and reblog count) in Tumblr.},
	booktitle = {Proceedings of the 24th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management} - {CIKM} '15},
	author = {Shin, Donghyuk and Cetintas, Suleyman and Lee, Kuang-Chih and Dhillon, Inderjit S.},
	year = {2015},
}

@article{Hillman2014,
	title = {alksjdf lksfd: {Tumblr} and the {Fandom} {User} {Experience}},
	doi = {10.1145/2598510.2600887},
	abstract = {A growing trend is the participation in online fandom communities through the support of the blogging platform Tumblr. While past research has investigated backchannels-chatter related to live entertainment on micro-blogging sites such as Twitter-there is a lack of research on the behaviours and motivations of Tumblr users. In our study, we investigate why fandom users chose Tumblr over other social networking sites, their motivations behind participating in fandoms, and how they interact within the Tumblr community. Our findings show that users face many user interface challenges when participating in Tumblr fandoms, especially initially; yet, despite this, Tumblr fandom communities thrive with a common sense of social purpose and exclusivity where users feel they can present a more authentic reflection of themselves to those sharing similar experiences and interests. We describe how this suggests design directions for social networking and blogging sites in order to promote communities of users.},
	journal = {ACM Conference on Designing Interactive Systems},
	author = {Hillman, Serena and Procyk, Jason and Neustaedter, Carman},
	year = {2014},
	note = {ISBN: 9781450329026},
	pages = {1--10},
}

@article{Bourlai2014,
	title = {Multimodal {Communication} on {Tumblr}: “ {I} {Have} {So} {Many} {Feels} !”},
	doi = {10.1145/2615569.2615697},
	abstract = {We manually analyzed a corpus of Tumblr posts for sentiment, looking at images, text, and their combination. A dataset was constructed of posts with both text and images, as well as a dataset of posts containing only text, along with a codebook for classifying and counting the content in each. This paper reports on the construction of the overall corpus and the codebook, and presents the results of a preliminary analysis that focuses on emotion. Posts containing images expressed more emotion, more intense emotion, and were more positive in valence than posts containing only text. The study contributes a micro-level analysis of multimodal communication in a social media platform, as well as a gold standard corpus that can be used to train learning algorithms to identify sentiment in multimodal Tumblr data. Categories},
	journal = {Proceedings of the 2014 ACM Conference on Web Science},
	author = {Bourlai, Elli and Herring, Susan C.},
	year = {2014},
	note = {ISBN: 9781450326223},
	keywords = {communication, gif, image analysis, meme, multimodality, sarcasm, sentiment, social media},
	pages = {171--175},
}

@article{Booten2016,
	title = {Patterns of {Wisdom} : {Discourse}-{Level} {Style} in {Multi}-{Sentence} {Quotations}},
	abstract = {Quotations are kernels not just of wisdom but also of beautiful and striking language. While recent studies have characterized the stylistic features of quotations, we characterize the or-der of stylistic information within quotations. Analyzing a corpus of two-sentence quota-tions collected from the social network Tum-blr, we explore the ways that both low-level features and high-level features tend to occur in either the first or second sentence. Through analysis of examples, we interpret these ten-dencies as manifestations of rhetorical pat-terns. Results from a prediction task suggest that stylistic patterns are more prominent in quotations than in a comparison corpus.},
	journal = {Naacl},
	author = {Booten, Kyle and Hearst, Marti A},
	year = {2016},
	note = {ISBN: 9781941643914},
	pages = {1139--1144},
}

@article{Kozareva2016,
	title = {Which {Tumblr} {Post} {Should} {I} {Read} {Next}?},
	url = {http://anthology.aclweb.org/P16-2054},
	abstract = {Microblogging sites have emerged as major platforms for bloggers to create and consume posts as well as to follow other bloggers and get informed of their updates. Due to the large number of users, and the huge amount of posts they create, it becomes extremely difficult to identify relevant and interesting blog posts. In this paper, we propose a novel convex collective matrix completion (CCMC) method that effectively utilizes user-item matrix and incorporates additional user activity and topic-based signals to recommend relevant content. The key advantage of CCMC over existing methods is that it can obtain a globally optimal solution and can easily scale to large-scale matrices using Hazan's algorithm. To the best of our knowledge, this is the first work which applies and studies CCMC as a recommendation method in social media. We conduct a large scale study and show significant improvement over existing state-ofthe-art approaches.},
	journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Kozareva, Zornitsa and Yamada, Makoto},
	year = {2016},
	note = {ISBN: 9781510827592},
	pages = {332--336},
}

@inproceedings{Kiesling2011,
	title = {Stance in context : {Affect}, alignment and investment in the analysis of stancetaking},
	doi = {10.13140/RG.2.1.2372.8807},
	booktitle = {{iMean} conference},
	author = {Kiesling, Scott F},
	year = {2011},
	note = {Issue: April},
	pages = {1--11},
}

@article{yang_towards_2014,
	title = {Towards {Identifying} the {Resolvability} of {Threads} in {MOOCs}},
	journal = {EMNLP 2014},
	author = {Yang, Diyi and Wen, Miaomiao and Rose, Carolyn},
	year = {2014},
	pages = {21--31},
}

@article{hox_introduction_2007,
	title = {Introduction to {Structural} {Equation} {Modeling}},
	volume = {11},
	issn = {10705511},
	doi = {10.1080/10705510903008345},
	abstract = {This article presents a short and non-technical introduction to Structural Equation Modeling or SEM. SEM is a powerful technique that can combine complex path models with latent variables (factors). Using SEM, researchers can specify confirmatory factor analysis models, regression models, and complex path models. We present the basic elements of a structural equation model, introduce the estimation technique, which is most often maximum Likelihood (ML), and discuss some problems concerning the assessment and improvement of the model fit, and model extensions to multigroup problems including factor means. Finally, we discuss some of the software, and list useful handbooks and Internet sites.},
	journal = {Family Science Review},
	author = {Hox, J.J. and Bechger, T.M},
	year = {2007},
	pmid = {22417004},
	note = {arXiv: 1011.1669v3
ISBN: 1070551090},
	keywords = {SEM, structural equation modeling},
	pages = {354--373},
}

@article{Ding2013,
	title = {Learning topical translation model for microblog hashtag suggestion},
	issn = {10450823},
	abstract = {Hashtags can be viewed as an indication to the context of the tweet or as the core idea expressed in the tweet. They provide valuable information for many applications, such as information retrieval, opinion mining, text classification, and so on. However, only a small number of microblogs are manually tagged. To address this problem, in this work, we propose a topical translation model for microblog hashtag suggestion. We assume that the content and hashtags of the tweet are talking about the same themes but written in different languages. Under the assumption, hashtag suggestion is modeled as a translation process from content to hashtags. Moreover, in order to cover the topic of tweets, the proposed model regards the translation probability to be topic-specific. It uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags, and discovers the topics of tweets by a topic model designed for microblogs. Experimental results on the dataset crawled from real world microblogging service demonstrate that the proposed method outperforms state-of-the-art methods.},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Ding, Zhuoye and Qiu, Xipeng and Zhang, Qi and Huang, Xuanjing},
	year = {2013},
	note = {ISBN: 9781577356332},
	keywords = {Natural Language Processing},
	pages = {2078--2084},
}

@article{Zangerle2011,
	title = {Recommending \#-tags in {Twitter}},
	volume = {730},
	issn = {16130073},
	abstract = {tweet与query中overlap的词的tfidf相加作为similarity，得到与query相似的tweet后从这些tweet中提取hashtag用于推荐},
	journal = {CEUR Workshop Proceedings},
	author = {Zangerle, Eva and Gassler, Wolfgang and Specht, Günther},
	year = {2011},
}

@article{Simeon2015,
	title = {Using {Combined} {Lexical} {Resources} to {Identify} {Hashtag} {Types}},
	doi = {10.18653/v1/W15-2924},
	abstract = {This paper seeks to identify sentiment and non-sentiment bearing hashtags by combining existing lexical resources. By using a lexicon-based approach, we achieve 86.3\% and 94.5\% precision in identifying sentiment and non-sentiment hashtags, respectively. Moreover, results obtained from both of our classification models demonstrate that using combined lexical, emotion and word resources is more effective than using a single resource in identifying the two types of hashtags.},
	number = {Wassa},
	journal = {Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
	author = {Simeon, Credell and Hilderman, Robert},
	year = {2015},
	pages = {169--174},
}

@article{Kywe2012,
	title = {On {Recommending} {Hashtags} in {Twitter} {Networks}},
	issn = {16113349},
	doi = {10.1007/978-3-642-35386-4},
	abstract = {Twitter network is currently overwhelmed by massive amount of tweets generated by its users. To effectively organize and search tweets, users have to depend on appropriate hashtags inserted into tweets. We begin our research on hashtags by first analyzing a Twitter dataset generated by more than 150,000 Singapore users over a three-month period. Among several interesting findings about hashtag usage by this user community, we have found a consistent and signif-icant use of new hashtags on a daily basis. This suggests that most hashtags have very short life span. We further propose a novel hashtag recommendation method based on collaborative filtering and the method recommends hashtags found in the previous month's data. Our method considers both user preferences and tweet content in selecting hashtags to be recommended. Our experiments show that our method yields bet-ter performance than recommendation based only on tweet content, even by considering the hashtags adopted by a small number (1 to 3)of users who share similar user preferences.},
	journal = {International Conference on Social Informatics},
	author = {Kywe, Su Mon and Hoang, Tuan-Anh and Lim, Ee-Peng and Zhu, Feida},
	year = {2012},
	note = {ISBN: 978-3-642-35385-7},
	keywords = {hashtag, recommendation systems, twitter},
	pages = {337--350},
}

@article{Tariq2013,
	title = {Exploiting {Topical} {Perceptions} {Over} {Multi}-{Lingual} {Text} for {Hashtag} {Suggestion} on {Twitter}},
	url = {http://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS13/paper/viewFile/5848/6120},
	abstract = {Microblogging websites, such as Twitter, provide seemingly endless amount of textual information on a wide variety of topics generated by a large number of users. Microblog posts, or tweets in Twitter, are often written in an informal manner using multi-lingual styles. Ignoring informal styles or multiple languages can hamper the usefulness of microblogging mining applications. In this paper, we present a statistical method for processing tweets according to users perceptions of topics and hashtags. Based on the non-classical notion of relatedness of vocabulary terms to topics in a corpus, which is quantified by discriminative term weights, our method builds a ranked list of terms related to hashtags. Subsequently, given a new tweet, our method can suggest a ranked list of hashtags. Our method allows enhanced understanding and normalization of users perceptions for improved information retrieval applications. We evaluate our method on a dataset of 14 million tweets collected over a period of 52 days. Results demonstrate that the method actually learns useful relationships between vocabulary terms and topics, and that the performance is better than a Naive Bayes suggestion system. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	journal = {Proc. Int. Florida Artificial Intelligence Research Society Conference},
	author = {Tariq, Amara and Karim, Asim and Gomez, Fernando and Foroosh, H},
	year = {2013},
	note = {ISBN: 9781577356059},
	keywords = {Data Mining Special Track},
	pages = {474--479},
}

@article{Dhingra2016,
	title = {{Tweet2Vec}: {Character}-{Based} {Distributed} {Representations} for {Social} {Media}},
	url = {http://arxiv.org/abs/1605.03481},
	doi = {10.18653/v1/P16-2044},
	abstract = {Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available.},
	author = {Dhingra, Bhuwan and Zhou, Zhong and Fitzpatrick, Dylan and Muehl, Michael and Cohen, William W.},
	year = {2016},
	note = {arXiv: 1605.03481
ISBN: 9781510827592},
}

@incollection{kubler_dependency_2017,
	title = {Dependency {Parsing}},
	isbn = {978-1-59829-596-2},
	url = {https://books.google.ca/books?id=k3iiup7HB9UC},
	abstract = {Dependency-based methods for syntactic parsing have become increasingly popular in natural language processing in recent years. This book gives a thorough introduction to the methods that are most widely used today. After an introduction to dependency grammar and dependency parsing, followed by a formal characterization of the dependency parsing problem, the book surveys the three major classes of parsing models that are in current use: transition-based, graph-based, and grammar-based models. It continues with a chapter on evaluation and one on the comparison of different methods, and it closes with a few words on current trends and future prospects of dependency parsing. The book presupposes a knowledge of basic concepts in linguistics and computer science, as well as some knowledge of parsing methods for constituency-based representations. Table of Contents: Introduction / Dependency Parsing / Transition-Based Parsing / Graph-Based Parsing / Grammar-Based Parsing / Evaluation / Comparison / Final Thoughts},
	booktitle = {Speech and {Language} {Processing}},
	author = {Kübler, Sandra and McDonald, Ryan and Nivre, Joakim},
	editor = {Jurafsky, Daniel and Martin, James H.},
	year = {2017},
	doi = {10.2200/S00169ED1V01Y200901HLT002},
	pages = {128},
}

@article{huang_bidirectional_2015,
	title = {Bidirectional {LSTM}-{CRF} {Models} for {Sequence} {Tagging}},
	url = {http://arxiv.org/abs/1508.01991},
	abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
	author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
	year = {2015},
	note = {arXiv: 1508.01991},
}

@article{Silverstein2003,
	title = {Indexical order and the dialectics of sociolinguistic life},
	volume = {23},
	issn = {02715309},
	doi = {10.1016/S0271-5309(03)00013-2},
	abstract = {The concept of indexical order is introduced, necessary to any empirical investigation of the inherently dialectical facts of indexicality. Indexical order is central to analyzing how semiotic agents access macro-sociological plane categories and concepts as values in the indexable realm of the micro-contextual. Through such access their relational identities are presupposed and creatively (trans)formed in interaction. We work through several classic examples of indexicality well-known in the literature of sociolinguistics, the clarification of which can be enhanced by using the concept of indexical order, viz., 'T/V' deference-indexicality, speech levels, indexically significant variation in phonetics informed by a standard phonological register. We conclude with an analysis of identity-commoditizing indexical overlays such as the American English register here dubbed " oinoglossia ," 'wine talk'. © 2003 Elsevier Ltd. All rights reserved.},
	number = {3},
	journal = {Language and Communication},
	author = {Silverstein, Michael},
	year = {2003},
	pmid = {20844133},
	note = {ISBN: 0271-5309},
	keywords = {Honorifics, Indexicality, Language ideology, Political economy of language, Registers, Sociolinguistic variation, ★},
	pages = {193--229},
}

@incollection{mitchell_mitchell2010latinocommunity.pdf_nodate,
	title = {{Mitchell2010LatinoCommunity}.pdf},
	author = {Mitchell, Thomas D.},
}

@article{sumanth_how_2015,
	title = {How much does word sense disambiguation help in sentiment analysis of micropost data?},
	abstract = {This short paper describes a sentiment analysis system for micro-post data that includes analysis of tweets from Twitter and Short Messaging Service (SMS) text messages. We discuss our system that makes use of Word Sense Disambigua-tion techniques in sentiment analysis at the message level, where the entire tweet or SMS text was analysed to determine its dominant sentiment. Previous work done in the area of Word Sense Disambigua-tion does not throw light on its influence on the analysis of social-media text and micropost data, which is what our work aims to achieve. Our experiments show that the use of Word Sense Disambigua-tion alone has resulted in an improved sen-timent analysis system that outperforms systems built without incorporating Word Sense Disambiguation.},
	number = {Wassa},
	journal = {6Th Workshop on Computational Approaches To Subjectivity, Sentiment and Social Media Analysis Wassa 2015},
	author = {Sumanth, Chiraag and Inkpen, Diana},
	year = {2015},
	pages = {115},
}

@article{rentoumi_sentiment_2009,
	title = {Sentiment analysis of figurative language using a word sense disambiguation approach},
	abstract = {In this paper we propose a methodology for sen- timent analysis of figurative language which ap- plies Word Sense Disambiguation and, through an n-gram graph based method, assigns polar- ity to word senses. Polarity assigned to senses, combined with contextual valence shifters, is exploited for further assigning polarity to sen- tences, using Hidden Markov Models. Evalua- tion results using the corpus of the Affective Text task of SemEval’07, are presented together with a comparison with other state-of-the-art meth- ods, showing that the proposed method provides promising results, and positive evidence support- ing our conjecture: figurative language conveys sentiment.},
	number = {c},
	journal = {Proc. of the …},
	author = {Rentoumi, Vassiliki and Giannakopoulos, George},
	year = {2009},
	pages = {370--375},
}

@article{lam_hackney:_2017,
	title = {Hackney: a cycling borough for whom?},
	issn = {2380-0127},
	url = {http://dx.doi.org/10.1080/23800127.2017.1305151},
	doi = {10.1080/23800127.2017.1305151},
	abstract = {London's internationally acclaimed " cycling revolution " was characterised by an unprecedented investment in cycling infrastructure, particularly cycle lanes manifesting as Cycle Superhighways or Quietways. Despite the hegemony of cycle lanes in London's overarching cycling paradigm, the London Borough of Hackney has historically achieved the city's highest rates of cycling and a long-standing reputation as a cycling borough in the absence of cycle lanes. Instead, Hackney has always opted for spatial interventions (such as filtered permeability, a borough-wide 20 mph speed restriction, and speed humps). This paper challenges Hackney's reputation as a cycling borough and the alleged success of its spatial interventions. I argue that Hackney's privileging of spatial fixes treats spatial interventions as apolitical and value-neutral, which ignores inequities entrenched in cycling. I also argue that Hackney has taken for granted its high rates of cycling, therefore effectively adopting a cycle-blind (akin to race-blind) and cycle mainstreaming (akin to gender mainstreaming) approach to cycling policy and interventions. Consequently, Hackney's spatial interventions for cycling raise the profile of already-visible privileged cyclists (white, middle-class men – the middle-aged men in Lycra, or MAMILs, and the hipsters) for whom cycling is a lifestyle choice while further erasing " invisible cyclists " for whom cycling is an economic necessity. In order to be a relevant and sustainable mode of transportation for Hackney residents, equity and social justice must foreground the borough's approach to cycling.},
	number = {June},
	journal = {Applied Mobilities},
	author = {Lam, Tiffany F},
	year = {2017},
}

@incollection{irvine_language_2000,
	address = {Santa Fe, NM},
	title = {Language {Ideology} and {Linguistic} {Differentiation}},
	isbn = {0-19-510561-3 0-19-510562-1},
	abstract = {Irvine, J.T. and Gal. S. 2000. Language ideology and linguistic differentiation. In P.V.Kroskrity (Ed.) Regimes of Language: Ideologies, Politics, and Identities. Santa Fe, NM: School of American Research Press, 35-84.},
	booktitle = {Regimes of {Language}: {Ideologies}, {Polities}, and {Identities}},
	publisher = {School of American Research Press},
	author = {Irvine, Judith T and Gal, Susan},
	editor = {Kroskrity, P. V.},
	year = {2000},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
ISSN: 1405126337},
	pages = {35--84},
}

@article{gangal_charmanteau:_2017,
	title = {{CharManteau}: {Character} {Embedding} {Models} {For} {Portmanteau} {Creation}},
	url = {http://arxiv.org/abs/1707.01176},
	abstract = {Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.},
	number = {1},
	author = {Gangal, Varun and Jhamtani, Harsh and Neubig, Graham and Hovy, Eduard and Nyberg, Eric},
	year = {2017},
	note = {arXiv: 1707.01176},
}

@article{thelwall_language_2006,
	title = {Language evolution and the spread of ideas on the web: {A} procedure for identifying emergent hybrid word family members},
	volume = {57},
	issn = {15322882},
	doi = {10.1002/asi.20437},
	abstract = {Word usage is of interest to linguists for its own sake as well as to social scientists and others who seek to track the spread of ideas, for example, in public debates over political decisions. The historical evolution of language can be analyzed with the tools of corpus linguistics through evolving corpora and the Web. But word usage statistics can only be gathered for known words. In this article, techniques are described and tested for identify- ing new words from the Web, focusing on the case when the words are related to a topic and have a hybrid form with a common sequence of letters. The results highlight the need to employ a combination of search techniques and show the wide potential of hybrid word family inves- tigations in linguistics and social science.},
	number = {10},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Thelwall, Mike and Price, Liz},
	year = {2006},
	note = {ISBN: 1532-2890},
	pages = {1326--1337},
}

@article{neubig_neural_2017,
	title = {Neural {Machine} {Translation} and {Sequence}-to-sequence {Models}: {A} {Tutorial}},
	url = {http://arxiv.org/abs/1703.01619},
	abstract = {This tutorial introduces a new and powerful set of techniques variously called "neural machine translation" or "neural sequence-to-sequence models". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.},
	author = {Neubig, Graham},
	year = {2017},
	note = {arXiv: 1703.01619},
	pages = {1--65},
}

@article{kiros_skip-thought_2015,
	title = {Skip-{Thought} {Vectors}},
	issn = {09953914},
	doi = {10.1017/CBO9781107415324.004},
	abstract = {We describe an approach for unsupervised learning of a generic, distributed sen-tence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expan-sion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we ex-tract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.},
	number = {786},
	journal = {NIPS},
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	year = {2015},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
	pages = {1--9},
}

@article{modan_public_2002,
	title = {'{Public} toilets for a diverse neighborhood': {Spatial} purification practices in community development discourse},
	issn = {1360-6441},
	doi = {10.1111/1467-9481.00198},
	journal = {Journal of Sociolinguistics},
	author = {Modan, Gabriella},
	year = {2002},
	keywords = {place},
	pages = {487--513},
}

@book{goldberg_neural_2017,
	title = {Neural {Network} {Methods} for {Natural} {Language} {Processing}},
	author = {Goldberg, Yoav},
	year = {2017},
}

@article{baker_representations_2010,
	title = {Representations of {Islam} in {British} broadsheet and tabloid newspapers 1999-2005},
	volume = {9},
	issn = {15692159},
	url = {http://dx.doi.org/10.1075/jlp.9.2.07bak},
	doi = {10.1075/jlp.9.2.07bak},
	abstract = {This paper describes the analysis of an 87 million word corpus of British newspaper articles which refer to the subject of Islam. In order to examine representations of Islam and Muslims, the corpus was subjected to a comparative analysis, by analysing the lexis that was used most significantly in the tabloid articles, when compared to the broadsheets, and vice versa. Concordances were then analysed in order to investigate the data in a more qualitative way. It was found that the tabloids tended to focus more on British interests, writing about Muslims in a highly emotional style, in connection with terrorist attacks and religious extremism, focussing on a small number of high-profile Muslim "villains". On the other hand, the broadsheets had a more restrained reporting stance, writing about Muslims in a wider range of contexts, although their focus on world news resulted in them covering more stories about Muslims engaged in wars. The paper raises issues regarding the meaning of bias, and the process by which readers internalise lexical associations and the extent to which such associations impact on attitudes.},
	number = {2},
	journal = {Journal of Language and Politics},
	author = {Baker, Paul},
	year = {2010},
	note = {ISBN: 15692159},
	keywords = {P Philology. Linguistics},
	pages = {310--338},
}

@article{jurgens_analysis_nodate,
	title = {An {Analysis} of {Individuals}' {Behavior} {Change} in {Online} {Groups}},
	abstract = {Many online platforms support social functions that enable their mem-bers to communicate, befriend, and join groups with one another. These social en-gagements are known to shape individuals' future behavior. However, most work has focused solely on how peers influence behavior and little is known what ad-ditional role online groups play in changing behavior. We investigate the capacity for group membership to lead users to change their behavior in three settings: (1) selecting physical activities, (2) responding to help requests, and (3) remaining active on the platform. To do this, we analyze nearly half a million users over five years from a popular fitness-focused social media platform whose unique affordances allow us to precisely control for the effects of social ties, user demo-graphics, and communication. We find that after joining a group, users readily adopt the exercising behavior seen in the group, regardless of whether the group was exercise and non-exercise themed, and this change is not explained by the in-fluence of pre-existing social ties. Further, we find that the group setting equalizes the social status of individuals such that lower status users still receive responses to requests. Finally, we find, surprisingly, that the number of groups one joins is negatively associated with user retention, when controlling for other behavioral and social factors.},
	author = {Jurgens, David and Mccorriston, James and Ruths, Derek},
}

@article{jirschitzka_productive_2017,
	title = {A productive clash of perspectives? the interplay between articles' and authors' perspectives and their impact on {Wikipedia} edits in a controversial domain},
	volume = {12},
	issn = {19326203},
	doi = {10.1371/journal.pone.0178985},
	abstract = {This study examined predictors of the development of Wikipedia articles that deal with controversial issues. We chose a corpus of articles in the German-language version of Wikipedia about alternative medicine as a representative controversial issue. We extracted edits made until March 2013 and categorized them using a supervised machine learning setup as either being pro conventional medicine, pro alternative medicine, or neutral. Based on these categories, we established relevant variables, such as the perspectives of articles and of authors at certain points in time, the (im)balance of an article's perspective, the number of non-neutral edits per article, the number of authors per article, authors' heterogeneity per article, and incongruity between authors' and articles' perspectives. The underlying objective was to predict the development of articles' perspectives with regard to the controversial topic. The empirical part of the study is embedded in theoretical considerations about editorial biases and the effectiveness of norms and rules in Wikipedia, such as the neutral point of view policy. Our findings revealed a selection bias where authors edited mainly articles with perspectives similar to their own viewpoint. Regression analyses showed that an author's perspective as well as the article's previous perspectives predicted the perspective of the resulting edits, albeit both predictors interact with each other. Further analyses indicated that articles with more non-neutral edits were altogether more balanced. We also found a positive effect of the number of authors and of the authors' heterogeneity on articles' balance. However, while the effect of the number of authors was reserved to pro-conventional medicine articles, the authors' heterogenity effect was restricted to pro-alternative medicine articles. Finally, we found a negative effect of incongruity between authors' and articles' perspectives that was pronounced for the pro-alternative medicine articles.},
	number = {6},
	journal = {PLoS ONE},
	author = {Jirschitzka, Jens and Kimmerle, Joachim and Halatchliyski, Iassen and Hancke, Julia and Meurers, Detmar and Cress, Ulrike},
	year = {2017},
	pmid = {28575077},
	note = {ISBN: 1111111111},
	pages = {1--24},
}

@article{wang_learning_2016,
	title = {Learning {Language} {Games} through {Interaction}},
	url = {http://arxiv.org/abs/1606.02447},
	doi = {10.18653/v1/P16-1224},
	abstract = {We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.},
	journal = {The 54th Annual Meeting of the Association for Computational Linguistics},
	author = {Wang, Sida I. and Liang, Percy and Manning, Christopher D.},
	year = {2016},
	note = {arXiv: 1606.02447
ISBN: 9781510827585},
	pages = {2368--2378},
}

@article{ackerman_illegal_2014,
	title = {The “{Illegal} alien” as a category of analysis: {A} methodological intervention},
	volume = {13},
	issn = {1569-2159},
	url = {http://www.jbe-platform.com/content/journals/10.1075/jlp.13.3.09ack},
	doi = {10.1075/jlp.13.3.09ack},
	abstract = {{\textless}p{\textgreater}“Illegal alien,” as a category of analysis, should be understood primarily as a discursive formation, yet the emergence and spread of the category in public debate cannot be explained by its discursive qualities. Failing to see this has analytical consequences: it results in a constant sidetracking of the question of why illegality itself came to be a central issue, and in a reification of the category. This paper is intended as a methodological intervention aimed at solving these issues. The first part illustrates the absence of a strict legalistic basis for the category, and reviews key works that fail to incorporate this into their conceptual design. The second part, contrasts two periods of time in U.S. political debate – the mid-2000s when the category was dominant, and the 1930s when the category, albeit pushed by elites, failed to become central- suggesting the need for a discursive analysis that goes beyond discourse. Keywords: Illegal immigration; illegal alien; discourse analysis; methodology; discursive formations{\textless}/p{\textgreater}},
	number = {3},
	journal = {Journal of Language and Politics},
	author = {Ackerman, Edwin F.},
	year = {2014},
	keywords = {discourse analysis, discursive formations, illegal alien, illegal immigration, methodology},
	pages = {563--579},
}

@article{baker_corpus-based_2005,
	title = {A corpus-based approach to discourses of refugees and asylum seekers in {UN} and newspaper texts},
	volume = {42},
	issn = {15692159},
	doi = {10.1075/jlp.4.2.04bak},
	abstract = {A corpus-based analysis of discourses of refugees and asylum seekers was carried out on data taken from a range of British newspapers and texts from the Office of the United Nations High Commissioner for Refugees website, both published in 2003. Concordances of the terms refugee(s) and asylum seeker(s) were examined and grouped along patterns which revealed lin-guistic traces of discourses. Discourses which framed refugees as packages, invaders, pests or water were found in newspaper texts, although there were also cases of negative discourses found in the UNHCR texts, revealing how difficult it is to disregard dominant discourses. Lexical choice was found to be an essential aspect of maintaining discourses of asylum seekers — col-locational analyses of terms like failed vs. rejected revealed the underlying attitudes of the writers towards the subject.},
	number = {2005},
	journal = {Journal of Language and Politics},
	author = {Baker, Paul and Mcenery, Tony},
	year = {2005},
	note = {ISBN: 1569-2159},
	keywords = {collocation, concordance, corpus, discourse, refugee},
	pages = {97--226},
}

@article{machin_multimodality_2016,
	title = {Multimodality, politics and ideology},
	volume = {15},
	issn = {1569-2159},
	url = {http://www.jbe-platform.com/content/journals/10.1075/jlp.15.3.01mac},
	doi = {10.1075/jlp.15.3.01mac},
	abstract = {{\textless}p{\textgreater}This journal’s editorial statement is clear that political discourse should be studied not only as regards parliamentary type politics. In this introduction we argue precisely for the need to pay increasing attention to the way that political ideologies are infused into culture more widely, in entertainments media, software, administrative processes, children’s apps, healthcare and even office furniture design. We point to the way that there have been massive shifts away from traditional state forms of politics to the rule of neoliberalism and the power of the corporation which, like the former regime of power, requires meanings and identities which can hold them in place. We explain the processes by which critical multimodal discourse analysis can best draw out this ideology as it is realized through different semiotics resources.{\textless}/p{\textgreater}},
	number = {3},
	journal = {Journal of Language and Politics},
	author = {Machin, David and van Leeuwen, Theo},
	year = {2016},
	keywords = {critical discourse analysis, ideology, multimodal, politics},
	pages = {243--258},
}

@article{malmi_automatic_2017,
	title = {Automatic {Prediction} of {Discourse} {Connectives}},
	url = {http://arxiv.org/abs/1702.00992},
	abstract = {Accurate prediction of suitable discourse connectives (however, furthermore, etc.) is a key component of any system aimed at building coherent and fluent discourses from shorter sentences and passages. As an example, a dialog system might assemble a long and informative answer by sampling passages extracted from different documents retrieved from the web. We formulate the task of discourse connective prediction and release a dataset of 2.9M sentence pairs separated by discourse connectives for this task. Then, we evaluate the hardness of the task for human raters, apply a recently proposed decomposable attention (DA) model to this task and observe that the automatic predictor has a higher F1 than human raters (32 vs. 30). Nevertheless, under specific conditions the raters still outperform the DA model, suggesting that there is headroom for future improvements. Finally, we further demonstrate the usefulness of the connectives dataset by showing that it improves implicit discourse relation prediction when used for model pre-training.},
	author = {Malmi, Eric and Pighin, Daniele and Krause, Sebastian and Kozhevnikov, Mikhail},
	year = {2017},
	note = {arXiv: 1702.00992},
}

@inproceedings{Hassan2012,
	title = {Detecting subgroups in online discussions by modeling positive and negative relations among participants},
	isbn = {978-1-937284-43-5},
	url = {http://dl.acm.org/citation.cfm?id=2390956},
	abstract = {A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.},
	booktitle = {Proceedings of the 2012 {Joint} {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and {Computational} {Natural} {Language} {Learning}},
	author = {Hassan, Ahmed and Abu-Jbara, A and Radev, D},
	year = {2012},
	note = {Issue: July},
	pages = {59--70},
}

@incollection{walton_appeals_2008,
	title = {Appeals to authority},
	booktitle = {Informal logic: a pragmatic approach},
	publisher = {Cambridge University Press},
	author = {Walton, Douglas},
	year = {2008},
	pages = {209--245},
}

@article{cloud_rhetoric_1998,
	title = {The rhetoric of {\textless}family values{\textgreater}: {Scapegoating}, utopia, and the privatization of social responsibility},
	volume = {62},
	issn = {1057-0314},
	url = {http://dx.doi.org/10.1080/10570319809374617},
	doi = {10.1080/10570319809374617},
	abstract = {This article performs an ideographic analysis of the bipartisan political deployment of the slogan {\textless}family values{\textgreater} during the 1992 Presidential election campaign. The analysis shows that {\textless}family values{\textgreater} talk functioned during that campaign to scapegoat Black men and poor Americans for social problems. However, the {\textless}family values{\textgreater} ideograph also is invested with a gendered utopian narrative that makes its scapegoating less apparent and more persuasive. Ultimately, in constructing the family as the site of all responsibility and change, the rhetoric of {\textless}family values{\textgreater} privatizes social responsibility for ending poverty and racism.},
	number = {4},
	journal = {Western Journal of Communication},
	author = {Cloud, Dana L},
	year = {1998},
	note = {ISBN: 1057-0314},
	pages = {387--419},
}

@article{cloud_veil_2004,
	title = {“{To} veil the threat of terror”: {Afghan} women and the ⟨clash of civilizations⟩ in the imagery of the {U}.{S}. war on terrorism},
	volume = {90},
	issn = {0033-5630},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0033563042000270726},
	doi = {10.1080/0033563042000270726},
	abstract = {This article explores the role of widely circulated images of Afghan people in building public support for the 2001–2002 U.S. war with Afghanistan. Emphasizing images of women, I argue that these representations participate in the more general category of “the clash of civilizations,” which constitutes a verbal and a visual ideograph linked to the idea of the “white man's burden.” Through the construction of binary oppositions of self and Other, the evocation of a paternalistic stance toward the women of Afghanistan, and the figuration of modernity as liberation, these images participate in a set of justifications for war that contradicts the actual motives for the war. These contradictions have a number of implications for democratic deliberation and public life during wartime.},
	number = {3},
	journal = {Quarterly Journal of Speech},
	author = {Cloud, Dana L.},
	year = {2004},
	note = {ISBN: Quarterly Journal of Speech, Vol. 90, No. 3, August 2004, pp. 285–306},
	pages = {285--306},
}

@article{bennett_news_1995,
	title = {News {Icons} and the {Mainstreaming} of {Social} {Change}},
	volume = {45},
	issn = {14602466},
	doi = {10.1111/j.1460-2466.1995.tb00742.x},
	abstract = {Two theoretical paradigms have traditionally guided understandings of the news. The liberal-pluralist paradigm posits the mass media as a vital conduit of accountability and bottom-up change, whereas the critical paradigm views the news as a crucial site and mechanism of ideological domination. This analysis furthers an evolving ecological approach, which bridges the two dominant paradigms, contending that the same market imperatives and journalistic routines which so often produce news that reinforces the status quo can at times produce news which challenges it. Analysis of news about the environment and waste recycling from 1980–1990 reveals a dramatic shift in coverage patterns following the ill-fated voyage of the Mobro, the garbage barge that sailed the high seas for three months in an unsuccessful search for a port that would accept its cargo. The barge became what we call a news icon: an image that lived on beyond its originating event by being introduced into a variety of subsequent news contexts. The icon provided an occasion for both journalists and their sources to refigure cultural scripts about garbage and recycling. In this process news routines and source communication strategies interacted to produce news as cultural forum, creating opportunities for cultural transformation.},
	number = {3},
	journal = {Journal of Communication},
	author = {Bennett, W. Lance and Lawrence, Regina G.},
	year = {1995},
	note = {ISBN: 1460-2466},
	pages = {20--39},
}

@article{lee_symbolic_2011,
	title = {Symbolic {Use} of {Decisive} {Events}: {Tiananmen} as a {News} {Icon} in the editorials of the elite {U}.{S}. press},
	volume = {16},
	issn = {1940-1612},
	url = {http://hij.sagepub.com/content/16/3/335.abstract},
	doi = {10.1177/1940161211403310},
	abstract = {The Tiananmen crackdown on the pro-democracy movement in 1989 was a decisive event that has provided an enduring prism for the world media to interpret China. This article examines how two of the most preeminent U.S. newspapers—New York Times and Washington Post—editorially invoked Tiananmen as a “news icon” in the past twenty years. We contend that the meanings of Tiananmen were reconstructed over twenty years partly but not completely in line with the changes in the United States’ policy toward China. Specifically, Tiananmen symbolized Communist dictatorship in the initial years after 1989 and then became an example of China’s human rights abuse in the late 1990s. Into the 2000s, the significance of Tiananmen faded away. But it remained as part of United States’ ritualistic memory of China’s repression that invokes the moral bottom line of U.S. foreign policy. In theoretical terms, this study shows how a news icon, in the course of an extended life cycle, may exhibit both continuities and changes in its meanings, and there can also be subtle variations in the relationships between a news icon and the dominant power structure over time.},
	number = {3},
	journal = {The International Journal of Press/Politics},
	author = {Lee, Chin-Chuan and {Hongtao Li} and Lee, Francis L F},
	year = {2011},
	note = {ISBN: 1940-1612{\textbackslash}r1940-1620},
	keywords = {china relationship, media discourses, news icon, policy, s, the press and foreign, tiananmen, u},
	pages = {335--356},
}

@article{flock_wikiwho_2014,
	title = {{WikiWho} : {Precise} and {Efficient} {Attribution} of {Authorship} of {Revisioned} {Content} {Categories} and {Subject} {Descriptors}},
	doi = {10.1145/2566486.2568026},
	abstract = {Revisioned text content is present in numerous collaboration platforms on the Web, most notably Wikis. To track authorship of text tokens in such systems has many potential applications; the identification of main authors for licensing reasons or tracing collaborative writing patterns over time, to name some. In this context, two main challenges arise. First, it is critical for such an authorship tracking system to be precise in its attributions, to be reliable for further processing. Second, it has to run efficiently even on very large datasets, such as Wikipedia. As a solution, we propose a graph-based model to represent revisioned content and an algorithm over this model that tackles both issues effectively. We describe the optimal implementation and design choices when tuning it to a Wiki environment. We further present a gold standard of 240 tokens from English Wikipedia articles annotated with their origin. This gold standard was created manually and confirmed by multiple independent users of a crowdsourcing platform. It is the first gold standard of this kind and quality and our solution achieves an average of 95\% precision on this data set. We also perform a first-ever precision evaluation of the state-of-the-art algorithm for the task, exceeding it by over 10\% on average. Our approach outperforms the execution time of the state-of-the-art by one order of magnitude, as we demonstrate on a sample of over 240 English Wikipedia articles. We argue that the increased size of an optional materialization of our results by about 10\% compared to the baseline is a favorable trade-off, given the large advantage in runtime performance.},
	journal = {International World Wide Web Conference Committee (IW3C2)},
	author = {Flöck, Fabian and Acosta, Maribel},
	year = {2014},
	note = {ISBN: 9781450327442},
	keywords = {authorship, collaborative authoring of text-based, collaborative writing, community-, content, content modeling, driven content creation, online collaboration, such as wiki pages, version control, wikipedia},
	pages = {843--853},
}

@inproceedings{sridhar_joint_2015,
	title = {Joint {Models} of {Disagreement} and {Stance} in {Online} {Debate}},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Sridhar, Dhanya and Foulds, James and Huang, Bert and Getoor, Lise and Walker, Marilyn},
	year = {2015},
	pages = {116--125},
}

@article{prabhakaran_predicting_1987,
	title = {Predicting the {Rise} and {Fall} of {Scientific} {Topics} from {Trends} in their {Rhetorical} {Framing}},
	doi = {10.18653/v1/P16-1111},
	abstract = {Computationally modeling the evolution of science by tracking how scientific top-ics rise and fall over time has important implications for research funding and pub-lic policy. However, little is known about the mechanisms underlying topic growth and decline. We investigate the role of rhetorical framing: whether the rhetori-cal role or function that authors ascribe to topics (as methods, as goals, as results, etc.) relates to the historical trajectory of the topics. We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010. We find that a topic's rhetorical function is highly predictive of its eventual growth or decline. For exam-ple, topics that are rhetorically described as results tend to be in decline, while top-ics that function as methods tend to be in early phases of growth.},
	number = {Figure 1},
	journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) (Volume 1: Long Papers)},
	author = {Prabhakaran, Vinodkumar and Hamilton, William L and McFarland, Dan and Jurafsky, Dan},
	year = {1987},
	note = {ISBN: 9781510827585 (ISBN)},
	pages = {1170--1180},
}

@article{lawrence_mining_2014,
	title = {Mining {Arguments} {From} 19th {Century} {Philosophical} {Texts} {Using} {Topic} {Based} {Modelling}},
	url = {http://www.aclweb.org/anthology/W/W14/W14-2111},
	abstract = {In this paper we look at the manual analysis of arguments and how this compares to the current state of automatic argument analysis. These considerations are used to develop a new approach combining a machine learning algorithm to extract propositions from text, with a topic model to determine argument structure. The results of this method are compared to a manual analysis.},
	journal = {Proceedings of the First Workshop on Argumentation Mining},
	author = {Lawrence, John and Reed, Chris and Allen, Colin and McAlister, Simon and Ravenscroft, Andrew},
	year = {2014},
	note = {ISBN: 9781941643068},
	pages = {79--87},
}

@article{sobhani_argumentation_2015,
	title = {From {Argumentation} {Mining} to {Stance} {Classification}},
	url = {http://www.aclweb.org/anthology/W15-0509},
	author = {Sobhani, Parinaz and Inkpen, Diana and Matwin, Stan},
	year = {2015},
	pages = {67--77},
}

@article{hasan_why_2014,
	title = {Why are {You} {Taking} this {Stance}? {Identifying} and {Classifying} {Reasons} in {Ideological} {Debates}},
	abstract = {Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
	author = {Hasan, Kazi Saidul and Ng, Vincent},
	year = {2014},
	pages = {751--762},
}

@article{boltuzic_back_2014,
	title = {Back up your {Stance} : {Recognizing} {Arguments} in {Online} {Discussions}},
	journal = {Proceedings of the First Workshop on Argumentation Mining},
	author = {Boltuzic, Filip and Snajder, Jan},
	year = {2014},
	pages = {49--58},
}

@phdthesis{phillips_investigation_2016,
	title = {An {Investigation} of {Police} {Brutality} in {News} {Media}: {Media} {Narratives} and {Narrative} {Icons} as {Argumentation} and {Communal} {Identity}},
	url = {http://repository.cmu.edu/dissertations/722/},
	school = {Carnegie Mellon University},
	author = {Phillips, Douglas},
	year = {2016},
}

@inproceedings{liu_implicit_2016,
	title = {Implicit {Discourse} {Relation} {Classification} via {Multi}-{Task} {Neural} {Networks}},
	isbn = {978-1-57735-760-5},
	abstract = {Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.},
	booktitle = {Proceedings of the 30th {Conference} on {Artificial} {Intelligence} ({AAAI} 2016)},
	author = {Liu, Yang and Li, Sujian and Zhang, Xiaodong and Sui, Zhifang},
	year = {2016},
	note = {arXiv: 1603.02776},
	keywords = {Technical Papers: Natural Language Processing and},
}

@article{Prasad2004,
	title = {Annotation and {Data} {Mining} of the {Penn} {Discourse} {TreeBank}},
	doi = {10.3115/1608938.1608950},
	number = {1},
	journal = {ACL 2004 Workshop on Discourse Annotation},
	author = {Prasad, Rashmi and Prasad, Rashmi and Miltsakaki, Eleni and Miltsakaki, Eleni and Joshi, Aravind and Joshi, Aravind and Webber, Bonnie and Webber, Bonnie},
	year = {2004},
	pages = {88--95},
}

@article{park_developing_2015,
	title = {Developing crash modification functions to assess safety effects of adding bike lanes for urban arterials with different roadway and socio-economic characteristics},
	volume = {74},
	issn = {00014575},
	url = {http://dx.doi.org/10.1016/j.aap.2014.10.024},
	doi = {10.1016/j.aap.2014.10.024},
	abstract = {Although many researchers have estimated crash modification factors (CMFs) for specific treatments (or countermeasures), there is a lack of studies that explored the heterogeneous effects of roadway characteristics on crash frequency among treated sites. Generally, the CMF estimated by before-after studies represents overall safety effects of the treatment in a fixed value. However, as each treated site has different roadway characteristics, there is a need to assess the variation of CMFs among the treated sites with different roadway characteristics through crash modification functions (CMFunctions). The main objective of this research is to determine relationships between the safety effects of adding a bike lane and the roadway characteristics through (1) evaluation of CMFs for adding a bike lane using observational before-after with empirical Bayes (EB) and cross-sectional methods, and (2) development of simple and full CMFunctions which are describe the CMF in a function of roadway characteristics of the sites. Data was collected for urban arterials in Florida, and the Florida-specific full SPFs were developed. Moreover, socio-economic parameters were collected and included in CMFunctions and SPFs (1) to capture the effects of the variables that represent volume of bicyclists and (2) to identify general relationship between the CMFs and these characteristics. In order to achieve better performance of CMFunctions, data mining techniques were used. The results of both before-after and cross-sectional methods show that adding a bike lane on urban arterials has positive safety effects (i.e., CMF {\textless} 1) for all crashes and bike crashes. It was found that adding a bike lane is more effective in reducing bike crashes than all crashes. It was also found that the CMFs vary across the sites with different roadway characteristics. In particular, annual average daily traffic (AADT), number of lanes, AADT per lane, median width, bike lane width, and lane width are significant characteristics that affect the variation in safety effects of adding a bike lane. Some socio-economic characteristics such as bike commuter rate and population density also have significant effect on the variation in CMFs. The findings suggest that full CMFunctions showed better model fit than simple CMFuncttions since they account for the heterogeneous effects of multiple roadway and socio-economic characteristics. The proposed CMFunctions provide insights into bike lane design and selection of sites for bike lane installation for reducing crashes.},
	journal = {Accident Analysis and Prevention},
	author = {Park, Juneyoung and Abdel-Aty, Mohamed and Lee, Jaeyoung and Lee, Chris},
	year = {2015},
	pmid = {25463959},
	note = {Publisher: Elsevier Ltd
ISBN: 0001-4575},
	keywords = {Adding a bike lane, Before-after method, Crash modification factors, Heterogeneous effect, Safety effectiveness, Simple and full crash modification functions},
	pages = {179--191},
}

@incollection{johnstone_participants_nodate,
	title = {Participants in discourse: relationships, roles, identities},
	booktitle = {Discourse {Analysis}, 3rd edition},
	author = {Johnstone, Barbara},
}

@incollection{blomgumperz,
	address = {New York},
	title = {Social meaning in linguistic structures: code-switching in {Northern} {Norway}},
	booktitle = {Directions in {Sociolinguistics}: {The} {Ethnography} of {Communication}},
	publisher = {Holt, Rinehart, and Winston},
	author = {Blom, Jan-Petter and Gumperz, John J.},
	editor = {Gumperz, John J. and Hymes, Del},
	year = {1972},
	pages = {407--434},
}

@article{Lui2012,
	title = {langid.py: {An} off-the-shelf language identification tool},
	url = {http://dl.acm.org/citation.cfm?id=2390475},
	abstract = {We present langid.py, an off-the-shelf language identification tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 long-document datasets, and 2 datasets from the microblog domain. We find that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data.},
	number = {July},
	journal = {Proceedings of the ACL 2012 System Demonstrations},
	author = {Lui, Marco and Baldwin, Timothy},
	year = {2012},
	pages = {25--30},
}

@incollection{comrie1991perspectives,
	title = {Agrammatism in {Arabic}},
	isbn = {978-90-272-7789-3},
	url = {https://books.google.com/books?id=EVBAAAAAQBAJ},
	booktitle = {Perspectives on {Arabic} {Linguistics}: {Papers} from the {Annual} {Symposium} on {Arabic} {Linguistics}. {Volume} {III}: {Salt} {Lake} {City}, {Utah} 1989},
	publisher = {John Benjamins Publishing Company},
	author = {Safi-Stagni, Sabah},
	year = {1991},
	note = {Series Title: Perspectives on Arabic Linguistics},
}

@article{li_cantonese-english_2000,
	title = {Cantonese-{English} code-switching research in {Hong} {Kong}: a {Y2K} review},
	volume = {19},
	issn = {0883-2919},
	url = {http://www.blackwell-synergy.com/links/doi/10.1111/1467-971X.00181},
	doi = {10.1111/1467-971X.00181},
	abstract = {This paper is a review of the major works in code-switching in Hong Kong to date. Four context-specific motivations commonly found in the Hong Kong Chinese press—euphemism, specificity, bilingual punning, and principle of economy—are adduced to show that English is one of the important linguistic resources used by Chinese Hongkongers to fulfill a variety of well-defined communicative purposes.},
	number = {3},
	journal = {World Englishes},
	author = {Li, David C. S.},
	year = {2000},
	pages = {305--322},
}

@inproceedings{Begum2010,
	title = {Functions of {Code}-{Switching} in {Tweets}: {An} {Annotation} {Scheme} and {Some} {Initial} {Experiments}},
	booktitle = {{LREC}},
	author = {Begum, Rafiya and Bali, Kalika and Choudhury, Monojit and Rudra, Koustav and Ganguly, Niloy},
	year = {2016},
	note = {Issue: i},
	keywords = {annotation, code-switching, corpus, creation, etc, hindi-english, multilinguality, pragmatic functions, social-media processing, twitter},
	pages = {1644--1650},
}

@book{bassiouney2006functions,
	title = {Functions of code switching in {Egypt}: {Evidence} from monologues},
	volume = {46},
	publisher = {Brill},
	author = {Bassiouney, Reem},
	year = {2006},
}

@article{jurgens_twitter_2014,
	title = {Twitter {Users} \#{CodeSwitch} {Hashtags} ! \#{MoltoImportante} \# wow\#},
	journal = {Proceedings of The First Workshop on Computational Approaches to Code Switching},
	author = {Jurgens, David and Dimitrov, Stefan and Ruths, Derek},
	year = {2014},
	pages = {51--61},
}

@incollection{martin2003working,
	title = {Negotiation: interacting in dialogue},
	isbn = {978-0-8264-5508-6},
	url = {https://books.google.com/books?id=Hoby40oSnUIC},
	booktitle = {Working with {Discourse}: {Meaning} {Beyond} the {Clause}},
	publisher = {Bloomsbury Academic},
	author = {Martin, J R and Rose, David},
	year = {2003},
	note = {Series Title: New Century Series},
}

@article{wrede_relationship_2003,
	title = {The relationship between dialogue acts and hot spots in meetings},
	doi = {10.1109/ASRU.2003.1318425},
	journal = {2003 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2003},
	author = {Wrede, Britta and Shriberg, Elizabeth},
	year = {2003},
	note = {ISBN: 0780379802},
	pages = {180--185},
}

@article{bucholtz_you_1999,
	title = {You da man: {Narrating} the racial other in the production of white masculinity},
	volume = {3},
	issn = {13606441},
	doi = {10.1111/1467-9481.00090},
	abstract = {Sociolinguistic research on the linguistic construction of identity has begun to attend to the construction of culturally normative, unmarked social categories such as whiteness and masculinity. The study of these categories involves the investigation of ideology as well as identity, because ideology produces hegemonic forms of white masculinity. Such ideologies of race and gender shape narratives of interracial con ̄ict told by middle-class European American boys at a California high school. The article focuses on one such narrative, told by a white boy who aligns with black youth culture and uses elements of African American Vernacular English in his speech. Via language crossing and other discursive strategies such as constructed dialogue, the narrative positions black masculinity, in contrast to white masculinity, as physically powerful and locally dominant. At the same time, the narrative preserves the racial hierarchy that enables white cultural appropriation of African American culture through language crossing.},
	number = {4},
	journal = {Journal of Sociolinguistics},
	author = {Bucholtz, Mary},
	year = {1999},
	note = {ISBN: 13606441},
	keywords = {frican American Vernacular English, language crossing, masculinity, narrative, whiteness, youth},
	pages = {443--460},
}

@article{Becker2011,
	title = {{DISCUSS}: a dialogue move taxonomy layered over semantic representations},
	url = {http://dl.acm.org/citation.cfm?id=2002669.2002702},
	journal = {Proceedings of the Ninth International Conference on Computational Semantics},
	author = {Becker, Lee and Ward, Wayne H and van Vuuren, Sarel and Palmer, Martha},
	year = {2011},
	pages = {310--314},
}

@incollection{Gavaldà2005,
	address = {Dordrecht},
	title = {Soup: {A} {Parser} for {Real}-{World} {Spontaneous} {Speech}},
	isbn = {978-1-4020-2295-1},
	url = {http://link.springer.com/10.1007/1-4020-2295-6_17},
	booktitle = {New {Developments} in {Parsing} {Technology}},
	publisher = {Springer Netherlands},
	author = {Gavaldà, Marsal},
	editor = {Bunt, Harry and Carroll, John and Satta, Giorgio},
	year = {2004},
	doi = {10.1007/1-4020-2295-6_17},
	pages = {339--350},
}

@article{rose_discourse_1995,
	title = {Discourse {Processing} of {Dialogues} with {Multiple} {Threads}},
	url = {http://arxiv.org/abs/cmp-lg/9504025},
	doi = {10.3115/981658.981663},
	abstract = {In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-to-speech translation system. We will demonstrate that theories of discourse which postulate a strict tree structure of discourse on either the intentional or attentional level are not totally adequate for handling spontaneous dialogues. We will present our extension to this approach along with its implementation in our plan-based discourse processor. We will demonstrate that the implementation of our approach outperforms an implementation based on the strict tree structure approach.},
	author = {Rose', Carolyn Penstein and Di Eugenio, Barbara and Levin, Lori S. and Van Ess-Dykema, Carol},
	year = {1995},
	note = {arXiv: cmp-lg/9504025},
	pages = {8},
}

@article{levy_liminality_2012,
	title = {Liminality in multitasking: {Where} talk and task collide in computer collaborations},
	volume = {41},
	issn = {0047-4045},
	doi = {10.1017/S0047404512000656},
	abstract = {This article investigates the effect of computer activity on talk during{\textbackslash}ncollaboration at the computer by two pairs of high school students{\textbackslash}nduring a web-based task. The work is located in relation to research in{\textbackslash}nthe wider world of the workplace and informal settings where{\textbackslash}nmultitasking involving talk and the operation of artifacts is known to{\textbackslash}noccur. The current study focuses on how, when two students are working{\textbackslash}nat the computer, talk continues or is disrupted during multitasking.{\textbackslash}nFive examples are described in detail, beginning with a relatively{\textbackslash}nstraightforward case of serial multitasking and leading up to an example{\textbackslash}nof complex simultaneous multitasking. Overwhelmingly in our data, only{\textbackslash}nroutine on-screen actions accompany talk, whereas complex actions occur{\textbackslash}nwith hitches or restarts in the talk, and true simultaneous multitasking{\textbackslash}nhappens on just three occasions in the data set.},
	number = {5},
	journal = {Language in Society},
	author = {Levy, Mike and Gardner, Rod},
	year = {2012},
	keywords = {Collaborative activity; computers; Conversation An},
	pages = {557--587},
}

@article{ehrlich_text_2012,
	title = {Text trajectories, legal discourse and gendered inequalities},
	volume = {3},
	issn = {18686303},
	url = {http://content.ebscohost.com/ContentServer.asp?T=P&P=AN&K=77831041&S=R&D=ufh&EbscoContent=dGJyMNLe80SeprI4zdnyOLCmr0qeprZSs6q4SbCWxWXS&ContentCustomer=dGJyMPGusky1rrdKucPfgeyx44Dt6fIA\nhttp://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=77831041&},
	doi = {10.1515/applirev-2012-0003},
	abstract = {Following Blommaert (2005), this paper examines what he calls a 'forgotten' context within Critical Discourse Analysis (CDA) and Conversation Analysis (CA) - that of text trajectories. For Blommaert, a limitation of both CDA and CA is their focus on 'the unique, one-time' instance of a given text and, by extension, the (limited) context associated with such an instance of text. Such a focus, according to Blommaert, ignores a salient feature of communication in contemporary societies - the fact that texts and discourses move around, are repeatedly recontextualized in new interpretive spaces, and in the process undergo significant transformations in meaning. The text trajectory investigated in this paper begins in a legal institution, more specifically, with a 2004 American rape trial, Maouloud Baby v. the State of Maryland. This legal case garnered much media attention and, as a result of such exposure, references to the case have appeared in both mainstream and social media outlets. Hence, as a 'text' that has displayed considerable movement across different contexts within the legal system and, subsequently, beyond the legal system to mainstream and popular forms of media, the Maouloud Baby trial constitutes fertile ground for the exploration of a text's trajectory. Indeed, in keeping with Blommaert's claims, I show how this trial's 'text' undergoes significant transformations in meaning as it is recontextualized in different kinds of interpretive spaces (both within the legal system and outside of it) and how these transformations in meaning reproduce larger patterns of gendered inequalities. [ABSTRACT FROM AUTHOR]},
	number = {1},
	journal = {Applied Linguistics Review},
	author = {Ehrlich, Susan},
	year = {2012},
	keywords = {1, 1515, 2012, 47, 73, and the law, applied linguistics review 3, applirev-2012-0003, doi 10, language, language and gender, language ideologies, sexual violence, text trajectory},
	pages = {47--73},
}

@incollection{Goffman1981,
	address = {Philadelphia},
	title = {Footing},
	booktitle = {Forms of {Talk}},
	publisher = {University of Pennsylvania Press},
	author = {Goffman, Erving},
	editor = {Goffman, Erving and Hymes, Dell},
	year = {1981},
	pages = {124--159},
}

@article{Tannen1987,
	title = {Interactive frames and knowledge schemas in interaction: examples from a medical examination/interview},
	volume = {50},
	number = {2},
	journal = {Social Psychology Quarterly},
	author = {Tannen, Deborah and Wallat, Cynthia},
	year = {1987},
	pages = {205--216},
}

@article{mayfield_recognizing_2011,
	title = {Recognizing authority in dialogue with an integer linear programming constrained model},
	abstract = {We present a novel computational formulation of speaker authority in discourse. This notion, which focuses on how speakers position themselves relative to each other in discourse, is first developed into a reliable coding scheme (0.71 agreement between human annotators). We also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming. We show that this constrained model's analyses of speaker authority correlates very strongly with expert human judgments (r2 coefficient of 0.947).},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.},
	author = {Mayfield, Elijah and Rosé, Carolyn Penstein},
	year = {2011},
	note = {ISBN: 978-1-932432-87-9},
	pages = {1018--1026},
}

@incollection{Levinson1983,
	title = {Conversational structure},
	booktitle = {Pragmatics},
	publisher = {Cambridge University Press},
	author = {Levinson, Stephen C.},
	year = {1983},
	pages = {284--333},
}

@book{sidnell2011conversation,
	title = {Conversation {Analysis}: {An} {Introduction}},
	isbn = {978-1-4443-5884-1},
	url = {https://books.google.com/books?id=uS-eHxYck3EC},
	publisher = {Wiley},
	author = {Sidnell, Jack},
	year = {2011},
	note = {Series Title: Language in Society},
}

@article{Litman2006,
	title = {Correlations between dialogue acts and learning in spoken tutoring dialogues},
	volume = {12},
	issn = {1351-3249},
	url = {http://www.journals.cambridge.org/abstract_S1351324906004165},
	doi = {10.1017/S1351324906004165},
	abstract = {We examine correlations between dialogue behaviors and learning in tutoring, using two corpora of spoken tutoring dialogues: a human-human corpus and a human-computer corpus. To formalize the notion of dialogue behavior, we manually annotate our data using a tagset of student and tutor dialogue acts relative to the tutoring domain. A unigram analysis of our annotated data shows that student learning correlates both with the tutor's dialogue acts and with the student's dialogue acts. A bigram analysis shows that student learning also correlates with joint patterns of tutor and student dialogue acts. In particular, our human-computer results show that the presence of student utterances that display reasoning (whether correct or incorrect), as well as the presence of reasoning questions asked by the computer tutor, both positively correlate with learning. Our human-human results show that student introductions of a new concept into the dialogue positively correlates with learning, but student attempts at deeper reasoning (particularly when incorrect), and the human tutor's attempts to direct the dialogue, both negatively correlate with learning. These results suggest that while the use of dialogue act n-grams is a promising method for examining correlations between dialogue behavior and learning, specific findings can differ in human versus computer tutoring, with the latter better motivating adaptive strategies for implementation.},
	number = {02},
	journal = {Natural Language Engineering},
	author = {Litman, Diane J and Forbes-Riley, Katherine},
	year = {2006},
	note = {ISBN: 1351324906004},
	pages = {161--176},
}

@article{cameron1998there,
	title = {Is there any ketchup, {Vera}?': {Gender}, power and pragmatics},
	volume = {9},
	number = {4},
	journal = {Discourse \& Society},
	author = {Cameron, Deborah},
	year = {1998},
	note = {Publisher: Sage Publications},
	pages = {437--455},
}

@article{lakoff1973language,
	title = {Language and woman's place},
	volume = {2},
	number = {01},
	journal = {Language in society},
	author = {Lakoff, Robin},
	year = {1973},
	note = {Publisher: Cambridge Univ Press},
	pages = {45--79},
}

@book{lakoff2004language,
	title = {Language and woman's place: {Text} and commentaries},
	volume = {3},
	publisher = {Oxford University Press, USA},
	author = {Lakoff, Robin Tolmach and Bucholtz, Mary},
	year = {2004},
}

@incollection{labov_logic_1972,
	title = {The logic of nonstandard {English}},
	booktitle = {Language in the inner city: studies in the {Black} {English} {Vernacular}},
	author = {Labov, William},
	year = {1972},
	pages = {201--405},
}

@article{Kalchbrenner2013,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Discourse} {Compositionality}},
	abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sen-tences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model cor-responding to the two levels of composi-tionality. The sentence model adopts con-volution as the central operation for com-posing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recur-rent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequen-tiality of sentences and the interaction be-tween different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model cou-pled to the sentence model obtains state of the art performance on a dialogue act clas-sification experiment.},
	journal = {ACL WS on Continuous Vector Space Models and their Compositionality},
	author = {Kalchbrenner, Nal and Blunsom, Phil},
	year = {2013},
	note = {arXiv: 1306.2795v1
ISBN: 9781937284671},
	pages = {119--126},
}

@article{Ivanovic2005,
	title = {Dialogue act tagging for instant messaging chat sessions},
	url = {http://portal.acm.org/citation.cfm?doid=1628960.1628976},
	doi = {10.3115/1628960.1628976},
	abstract = {Instant Messaging chat sessions are real-time text-based conversations which can be analyzed using dialogue-act models. We describe a statistical approach for modelling and detecting dialogue acts in Instant Messaging dialogue. This in- volved the collection of a small set of task-based dialogues and annotating them with a revised tag set. We then dealt with segmentation and synchronisation issues which do not arise in spoken dialogue. The model we developed combines naive Bayes and dialogue-act n-grams to obtain better than 80\% accuracy in our tagging experiment.},
	number = {June},
	journal = {Proceedings of the ACL Student Research Workshop on - ACL '05},
	author = {Ivanovic, Edward},
	year = {2005},
	note = {ISBN: 1932432515},
	pages = {79},
}

@article{davies1990positioning,
	title = {Positioning: {The} discursive production of selves},
	volume = {20},
	number = {1},
	journal = {Journal for the theory of social behaviour},
	author = {Davies, Bronwyn and Harré, Rom},
	year = {1990},
	note = {Publisher: Wiley Online Library},
	pages = {43--63},
}

@article{Ritter2010,
	title = {Unsupervised modeling of twitter conversations},
	issn = {03076946},
	url = {http://nparc.cisti-icist.nrc-cnrc.gc.ca/npsi/ctrl?action=rtdoc&an=16885300\nhttp://dl.acm.org/citation.cfm?id=1858019},
	abstract = {We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium.},
	number = {June},
	journal = {The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Ritter, Alan and Cherry, Colin and Dolan, Bill},
	year = {2010},
	note = {ISBN: 1932432655},
	pages = {172--180},
}

@article{Erkens2008,
	title = {Automatic coding of dialogue acts in collaboration protocols},
	volume = {3},
	issn = {15561607},
	doi = {10.1007/s11412-008-9052-6},
	abstract = {Although protocol analysis can be an important tool for researchers to investigate the process of collaboration and communication, the use of this method of analysis can be time consuming. Hence, an automatic coding procedure for coding dialogue acts was developed. This procedure helps to determine the communicative function of messages in online discussions by recognizing discourse markers and cue phrases in the utterances. Five main communicative functions are distinguished: argumentative, responsive, informative, elicitative, and imperative. A total of 29 different dialogue acts are specified and recognized automatically in collaboration protocols. The reliability of the automatic coding procedure was determined by comparing automatically coded dialogue acts to hand-coded dialogue acts by a human rater. The validity of the automatic coding procedure was examined using three different types of analyses. First, an examination of group differences was used (dialogue acts used by female versus male students). Ideally, the coding procedure should be able to distinguish between groups who are likely to communicate differently. Second, to examine the validity of the automatic coding procedure through examination of experimental intervention, the results of the automatic coding procedure of students, with access to a tool that visualizes the degree of participation of each student, were compared to students who did not have access to this tool. Finally, the validity of the automatic coding procedure of dialogue acts was examined using correlation analyses. Results of the automatic coding procedure of dialogue acts of utterances (form) were related to results of a manual coding procedure of the collaborative activities to which the utterances refer (content). The analyses presented in this paper indicate promising results concerning the reliability and validity of the automatic coding procedure for dialogue acts. However, limitations of the procedure were also found and discussed.},
	number = {4},
	journal = {International Journal of Computer-Supported Collaborative Learning},
	author = {Erkens, Gijsbert and Janssen, Jeroen},
	year = {2008},
	note = {ISBN: 1556-1607},
	keywords = {Collaborative learning, Computer-supported collaborative learning, Dialogue acts, Protocol analysis},
	pages = {447--470},
}

@article{jurafsky_extracting_2009,
	title = {Extracting social meaning: {Identifying} interactional style in spoken conversation},
	url = {http://dl.acm.org/citation.cfm?id=1620847},
	abstract = {Automatically extracting social meaning and intention from spoken dialogue is an impor- tant task for dialogue systems and social com- puting. We describe a system for detecting elements of interactional style: whether a speaker is awkward, friendly, or flirtatious. We create and use a new spoken corpus of 991 4-minute speed-dates. Participants rated their interlocutors for these elements of style. Us- ing rich dialogue, lexical, and prosodic fea- tures, we are able to detect flirtatious, awk- ward, and friendly styles in noisy natural con- versational data with up to 75\% accuracy, compared to a 50\% baseline. We describe sim- pleways to extract relatively rich dialogue fea- tures, and analyze which features performed similarly for men and women and which were gender-specific. 1},
	number = {June},
	journal = {NAACL '09 Proceedings of Human Language Technologies},
	author = {Jurafsky, Dan and Ranganath, R and McFarland, D},
	year = {2009},
	note = {ISBN: 9781932432411},
	pages = {638--646},
}

@article{Brychcin2016,
	title = {Unsupervised {Dialogue} {Act} {Induction} using {Gaussian} {Mixtures}},
	url = {http://arxiv.org/abs/1612.06572},
	abstract = {This paper introduces a new unsupervised approach for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the dialogue as Hidden Markov model with emission probabilities estimated by Gaussian mixtures. We use Gibbs sampling for posterior inference. We present the results on the standard Switchboard-DAMSL corpus. Our algorithm achieves promising results compared with strong supervised baselines and outperforms other unsupervised algorithms.},
	number = {2010},
	author = {Brychcín, Tomáš and Král, Pavel},
	year = {2016},
	note = {arXiv: 1612.06572},
}

@article{Vosoughi2016,
	title = {Tweet {Acts} : {A} {Speech} {Act} {Classifier} for {Twitter}},
	number = {ICWSM},
	journal = {Proceedings of the 10th AAAI Conference on Weblogs and Social Media},
	author = {Vosoughi, Soroush and Roy, Deb},
	year = {2016},
	note = {arXiv: 1605.05156
ISBN: 9781577357582},
	keywords = {Poster Papers},
	pages = {1--4},
}

@article{Misra2013,
	title = {Topic {Independent} {Identification} of {Agreement} and {Disagreement} in {Social} {Media} {Dialogue}},
	url = {http://www.aclweb.org/anthology/W13-4006},
	number = {August},
	journal = {Proceedings of the SIGDIAL 2013 Conference},
	author = {Misra, Amita and Walker, Marilyn},
	year = {2013},
	note = {ISBN: 9781937284954},
	pages = {41--50},
}

@article{Louwerse2002,
	title = {Good {Computational} {Manners}: {Mixed}-{Initiative} {Dialog} in {Conversational} {Agents}},
	journal = {Etiquette for Human-Computer Work: Papers from the AAAI Fall Symposium},
	author = {Louwerse, M M and Graesser, a C and Olney, a and Group, the Tutoring Research},
	year = {2002},
	note = {ISBN: FS-02-02},
	pages = {71--76},
}

@book{searle1969speech,
	title = {Speech {Acts}: {An} {Essay} in the {Philosophy} of {Language}},
	isbn = {978-0-521-09626-3},
	url = {https://books.google.com/books?id=t3_WhfknvF0C},
	publisher = {Cambridge University Press},
	author = {Searle, J R},
	year = {1969},
	note = {Series Title: Cam: Verschiedene Aufl},
}

@incollection{O’Shea2012,
	address = {Berlin, Heidelberg},
	title = {A {Multi}-classifier {Approach} to {Dialogue} {Act} {Classification} {Using} {Function} {Words}},
	isbn = {978-3-642-32066-8},
	url = {http://dx.doi.org/10.1007/978-3-642-32066-8_6},
	booktitle = {Transactions on {Computational} {Collective} {Intelligence} {VII}},
	publisher = {Springer Berlin Heidelberg},
	author = {O'Shea, James and Bandar, Zuhair and Crockett, Keeley},
	editor = {Nguyen, Ngoc Thanh},
	year = {2012},
	doi = {10.1007/978-3-642-32066-8_6},
	pages = {119--143},
}

@incollection{Traum1999,
	address = {Dordrecht},
	title = {Speech {Acts} for {Dialogue} {Agents}},
	isbn = {978-94-015-9204-8},
	url = {http://dx.doi.org/10.1007/978-94-015-9204-8_8},
	booktitle = {Foundations of {Rational} {Agency}},
	publisher = {Springer Netherlands},
	author = {Traum, David R},
	editor = {Wooldridge, Michael and Rao, Anand},
	year = {1999},
	doi = {10.1007/978-94-015-9204-8_8},
	pages = {169--201},
}

@inproceedings{Appling2013,
	title = {Towards automated personality identification using speech acts},
	isbn = {978-1-57735-611-0},
	abstract = {The way people communicate- be it verbally, visually, or via text- is indicative of personality traits. In social media the concept of the status update is used for individuals to communicate to their social networks in an always-on fashion. In doing so individuals utilize various kinds of speech acts that, while primarily communicating their content, also leave traces of their personality dimensions behind. We human-coded a set of Facebook status updates from the myPersonality dataset in terms of speech acts label and then experimented with surface level linguistic features including lexical, syntactic, and simple sentiment detection to automatically label status updates as their appropriate speech act. We apply supervised learning to the dataset and using our features are able to classify with high accuracy two dominant kinds of acts that have been found to occur in social media. At the same time we used the coded data to perform a regression analysis to determine which speech acts are significant of certain personality dimensions. The implications of our work allow for automatic large-scale personality identification through social media status updates. Copyright \&copy; 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	booktitle = {{AAAI} {Workshop} - {Technical} {Report}},
	author = {Appling, D Scott and Briscoe, Erica J and Hayes, Heather and Mappus, Rudolph L},
	year = {2013},
	keywords = {Regression analysis, Social networking (online)},
}

@article{schiffrin1984jewish,
	title = {Jewish argument as sociability},
	volume = {13},
	number = {03},
	journal = {Language in society},
	author = {Schiffrin, Deborah},
	year = {1984},
	note = {Publisher: Cambridge Univ Press},
	pages = {311--335},
}

@book{winograd1986understanding,
	title = {Understanding {Computers} and {Cognition}: {A} {New} {Foundation} for {Design}},
	isbn = {978-0-89391-050-1},
	url = {https://books.google.com/books?id=2sRC8vcDYNEC},
	publisher = {Ablex Publishing Corporation},
	author = {Winograd, T and Flores, F},
	year = {1986},
	note = {Series Title: Language and being},
}

@article{Jurafsky1998,
	title = {Automatic detection of discourse structure for speech recognition and understanding},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=658992},
	doi = {10.1109/ASRU.1997.658992},
	abstract = {We describe a new approach for statistical modeling and detection of discourse structure for natural conversational speech. Our model is based on 42 dialog acts (DAs), (question, answer, backchannel, agreement, disagreement, apology, etc.). We labeled 1155 conversations from the Switchboard (SWBD) database (Godfrey et al., 1992) of human-to-human telephone conversations with these 42 types and trained a dialog act detector based on three distinct knowledge sources: sequences of words which characterize a dialog act; prosodic features which characterize a dialog act; and a statistical discourse grammar. Our combined detector, although still in preliminary stages, already achieves a 65\% dialog act detection rate based on acoustic waveforms, and 72\% accuracy based on word transcripts. Using this detector to switch among the 42 dialog-act-specific trigram LMs also gave us an encouraging but not statistically significant reduction in SWBD word error},
	journal = {1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings},
	author = {Jurafsky, Daniel and Bates, Rebecca and Coccaro, N. and Martin, R. and Meteer, M. and Ries, K. and Shriberg, E. and Stolcke, A. and Taylor, Paul and Van Ess-Dykema, C.},
	year = {1998},
	note = {ISBN: 0-7803-3698-4},
	pages = {88--95},
}

@article{Khanpour2016,
	title = {Dialogue {Act} {Classification} in {Domain}-{Independent} {Conversations} {Using} a {Deep} {Recurrent} {Neural} {Network}},
	author = {Khanpour, Hamed and Guntakandla, Nishitha and Nielsen, Rodney},
	year = {2016},
	pages = {2012--2021},
}

@article{schiffrin1984story,
	title = {How a story says what it means and does},
	volume = {4},
	number = {4},
	journal = {Text-Interdisciplinary Journal for the Study of Discourse},
	author = {Schiffrin, Deborah},
	year = {1984},
	pages = {313--346},
}

@article{cimiano_role_nodate,
	title = {On the {Role} of {Senses} in {Ontology}-{Lexicon}},
	author = {Cimiano, Philipp and McCrae, John and Buitelaar, Paul and Montiel-Ponsoda, Elena},
	pages = {1--18},
}

@article{buttny_drawing_2007,
	title = {Drawing on the words of others at public hearings: {Zoning}, {Wal}-{Mart}, and the threat to the aquifer},
	volume = {36},
	issn = {0047-4045},
	doi = {10.1017/S0047404507070674},
	abstract = {This study examines two public hearings on a zoning proposal that would allow the construction of a Super Wal-Mart Center on a field over the townâ€™s aquifer. Many citizens speak out against the zoning change because of the risk to drinking water, as well as other issues. Citizens face the speakerâ€™s problem of how to make their presentations convincing, given the technical matters involved and the fact that Town Board members have likely already heard about these issues. Some speakers draw on the words of others in their presentations. Using anotherâ€™s words allows the speaker to cite an authoritative source or to respond to what another has said, to evaluate it, and often to challenge it. Speakers use other devices in addition to quotes, such as formulations, repetition, and membership categorizations to develop their evaluative stances in the reporting context. The studyâ€™s focus is the discursive construction and rhetoric of using othersâ€™ words for the speakerâ€™s own purposes.},
	number = {05},
	journal = {Language in Society},
	author = {Buttny, Richard and Cohen, Jodi R.},
	year = {2007},
	note = {ISBN: 0047404507070},
	keywords = {public hearings, risk, reported speech, quotes, Wa},
	pages = {735--756},
}

@book{frankenberg1997displacing,
	title = {Displacing {Whiteness}: {Essays} in {Social} and {Cultural} {Criticism}},
	isbn = {978-0-8223-2021-0},
	url = {https://books.google.com/books?id=dGgiqHMs3vMC},
	publisher = {Duke University Press},
	author = {Frankenberg, R},
	year = {1997},
}

@article{wallace_generative_2013,
	title = {A {Generative} {Joint}, {Additive}, {Sequential} {Model} of {Topics} and {Speech} {Acts} in {Patient}-{Doctor} {Communication}},
	number = {October},
	journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	author = {Wallace, Byron C and Trikalinos, Thomas a and Laws, M Barton and Wilson, Ira B and Charniak, Eugene},
	year = {2013},
	note = {ISBN: 9781937284978},
	pages = {1765--1775},
}

@article{oya_extractive_2014,
	title = {Extractive {Summarization} and {Dialogue} {Act} {Modeling} on {Email} {Threads} : {An} {Integrated} {Probabilistic} {Approach}},
	number = {June},
	journal = {Sigdial},
	author = {Oya, Tatsuro and Carenini, Giuseppe},
	year = {2014},
	note = {ISBN: 9781941643211},
	pages = {133--140},
}

@article{petukhova_towards_2009,
	title = {Towards a multidimensional semantics of discourse markers in spoken dialogue},
	url = {http://portal.acm.org/citation.cfm?doid=1693756.1693773},
	doi = {10.3115/1693756.1693773},
	number = {January},
	journal = {Proceedings of the Eighth International Conference on Computational Semantics - IWCS-8 '09},
	author = {Petukhova, Volha and Bunt, Harry},
	year = {2009},
	note = {ISBN: 9789074029346},
	pages = {157},
}

@inproceedings{mitchell_vector-based_2008,
	title = {Vector-based {Models} of {Semantic} {Composition}.},
	volume = {8},
	isbn = {978-1-932432-04-6},
	abstract = {This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.},
	booktitle = {Acl},
	author = {Mitchell, Jeff and Lapata, Mirella},
	year = {2008},
	note = {Issue: June},
	pages = {236--244},
}

@incollection{Labov1972,
	address = {Philadelphia},
	title = {The {Social} {Stratification} of (r) in {New} {York} {City} {Department} {Stores}},
	booktitle = {Sociolinguistic {Patterns}},
	publisher = {University of Pennsylvania Press},
	author = {Labov, William},
	editor = {Labov, William},
	year = {1972},
	pages = {43--69},
}

@article{cao_novel_2015,
	title = {A {Novel} {Neural} {Topic} {Model} and {Its} {Supervised} {Extension}},
	abstract = {Topic modeling techniques have the benefits of model- ing words and documents uniformly under a probabilis- tic framework. However, they also suffer from the limi- tations of sensitivity to initialization and unigram topic distribution, which can be remedied by deep learning techniques. To explore the combination of topic mod- eling and deep learning techniques, we first explain the standard topic model from the perspective of a neural network. Based on this, we propose a novel neural topic model (NTM) where the representation of words and documents are efficiently and naturally combined into a uniform framework. Extending from NTM, we can eas- ily add a label layer and propose the supervised neu- ral topic model (sNTM) to tackle supervised tasks. Ex- periments show that our models are competitive in both topic discovery and classification/regression tasks. Introduction},
	journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence A},
	author = {Cao, Ziqiang},
	year = {2015},
	note = {ISBN: 9781577357018},
	keywords = {NLP and Machine Learning Track},
	pages = {2210--2216},
}

@incollection{Eckert,
	title = {The social order of {Belten} {High}},
	booktitle = {Language {Variation} as {Social} {Practice}: {The} {Linguistic} {Construction} of {Identity} in {Belten} {High}},
	author = {Eckert, Penelope},
	year = {2000},
}

@article{elsner_unified_2007,
	title = {A {Unified} {Local} and {Global} {Model} for {Discourse} {Coherence}},
	abstract = {NOTE TO READERS: We have recently detected a software bug which affects the results of our standalone entity grid experiments. (The bug was in our syntactic analysis code, which incorrectly failed to label the second object of a conjoint VP; in the phrase wash the dishes and clean the sink, dishes would be correctly labeled as O but sink mislabeled as X.) This bug happened to have an unfortunate interaction with the This is preliminary information preamble mentioned in section 5. The results in table 2 above the line are incorrect; our relaxed entity grid does not outperform the naive grid on the discriminative test. This implies that our argument motivating the relaxed model at the end of section 2 is misguided. The design and performance of the joint model is unaffected. We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model},
	number = {April},
	journal = {In Proceedings of HLT-NAACL},
	author = {Elsner, Micha and Austerweil, Joseph and Charniak, Eugene},
	year = {2007},
	pages = {436--443},
}

@article{yannakoudakis_modeling_2012,
	title = {Modeling coherence in {ESOL} learner texts},
	abstract = {To date, few attempts have been made to de- velop new methods and validate existing ones for automatic evaluation of discourse coher- ence in the noisy domain of learner texts. We present the first systematic analysis of several methods for assessing coherence un- der the framework of automated assessment (AA) of learner free-text responses. We ex- amine the predictive power of different coher- ence models by measuring the effect on per- formance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner’s level of at- tainment. Additionally, we identify new tech- niques that outperform previously developed ones and improve on the best published result forAAon a publically-available dataset of En- glish learner free-text examination scripts.},
	journal = {Proceedings of the Seventh Workshop on Building Educational Applications Using NLP},
	author = {Yannakoudakis, Helen and Briscoe, Ted},
	year = {2012},
	pages = {33--43},
}

@article{zufferey_towards_2004,
	title = {Towards automatic identification of discourse markers in dialogs: {The} case of 'like'},
	url = {http://www.aclweb.org/anthology/W04-2313},
	number = {July},
	journal = {Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue},
	author = {Zufferey, Sandrine and Popescu-Belis, Andrei},
	year = {2004},
	pages = {63--71},
}

@article{asr_information_2013,
	title = {On the {Information} {Conveyed} by {Discourse} {Markers}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.377.3017},
	abstract = {Discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text. Using the Penn Discourse Treebank, we investigate what information relevant to inerring discourse relations is conveyed by discourse connectives, and whether the specificity of discourse relations reflects general cognitive biases for estab- lishing coherence. We also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy. This will open a way to the computational modeling of discourse processing.},
	journal = {Workshop on Cognitive Modeling and Computational Linguistics},
	author = {Asr, Fatemeh Torabi and Demberg, Vera},
	year = {2013},
	pages = {84--93},
}

@article{elsner_coreference-inspired_2008,
	title = {Coreference-inspired {Coherence} {Modeling}},
	url = {http://www.aclweb.org/anthology/P/P08/P08-2011},
	doi = {10.3115/1557690.1557702},
	abstract = {Research on coreference resolution and summarization has modeled the way entities are realized as concrete phrases in discourse. In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents, and models describing the likely distance between a pronoun and its antecedent. However, models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information. We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence.},
	number = {June},
	journal = {Proceedings of ACL-08: HLT, Short Papers},
	author = {Elsner, Micha and Charniak, Eugene},
	year = {2008},
	note = {ISBN: 9781932432046},
	pages = {41--44},
}

@article{spenader_reliable_2009,
	title = {Reliable discourse markers for contrast relations},
	url = {http://portal.acm.org/citation.cfm?doid=1693756.1693777},
	doi = {10.3115/1693756.1693777},
	number = {January},
	journal = {Proceedings of the Eighth International Conference on Computational Semantics - IWCS-8 '09},
	author = {Spenader, Jennifer and Lobanova, Anna},
	year = {2009},
	note = {ISBN: 9789074029346},
	pages = {210},
}

@article{knott_classification_1998,
	title = {The classification of coherence relations and their linguistic markers: {An} exploration of two languages},
	volume = {30},
	issn = {03782166},
	url = {http://www.sciencedirect.com/science/article/pii/S037821669800023X},
	doi = {10.1016/S0378-2166(98)00023-X},
	abstract = {It has become popular among discourse linguists to explain a text's coherence by identifying ‘coherence relations’ which apply at various levels between its component spans. However, there is currently no overall agreement about how to define a standard set of coherence relations, and even about what the coherence relations in a text are intended to represent. In this paper, both questions are addressed: we outline a conception of relations as modelling psychological constructs used by readers and writers, and suggest how a limited set of categories of coherence relations can be identified. We relate two independent methods for investigating relations, one drawing mainly on psycholinguistic experiments on Dutch speaking subjects, the other starting from a study of the ‘cue phrases’ used to signal relations in English text. Both approaches lead to classifications of relations and cue phrases. We examine to what extent these classifications converge - and to what extent they accord with the psychologically motivated classification - in a comparative study of a set of cue phrases in English and Dutch. Interesting similarities are noted on both counts.},
	number = {2},
	journal = {Journal of Pragmatics},
	author = {Knott, Alistair and Sanders, Ted},
	year = {1998},
	note = {ISBN: 0378-2166},
	pages = {135--175},
}

@article{collobert_unified_2008,
	title = {A unified architecture for natural language processing},
	volume = {20},
	issn = {07224028},
	url = {http://portal.acm.org/citation.cfm?id=1390177%5Cnhttp://portal.acm.org/citation.cfm?doid=1390156.1390177},
	doi = {10.1145/1390156.1390177},
	abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
	number = {1},
	journal = {Proceedings of the 25th international conference on machine learning - ICML '08},
	author = {Collobert, Ronan and Weston, Jason},
	year = {2008},
	pmid = {2975184},
	note = {ISBN: 9781605582054},
	pages = {160--167},
}

@article{dias_enriching_2015,
	title = {Enriching entity grids and graphs with discourse relations : the impact in local coherence evaluation},
	author = {Dias, Márcio De S and Pardo, Thiago A S},
	year = {2015},
	pages = {151--160},
}

@article{visser_using_1996,
	title = {Using {Sentence} connectors for evaluation {MT} output},
	journal = {Proceedings of the 16th International{\textbackslash} Conference{\textbackslash} on Computational Linguistics (CoLing 96)},
	author = {Visser, Eric M and Fuji, Masaru},
	year = {1996},
	keywords = {langweilig},
	pages = {1066--1069},
}

@article{delfino_fighting_2016,
	title = {Fighting words? {Joning} as conflict talk and identity performance among {African} {American} preadolescents},
	volume = {20},
	issn = {13606441},
	url = {http://doi.wiley.com/10.1111/josl.12214},
	doi = {10.1111/josl.12214},
	number = {5},
	journal = {Journal of Sociolinguistics},
	author = {Delfino, Jennifer B.},
	year = {2016},
	keywords = {african american vernacular, children and youth, classroom discourse, english, face-to-face, interaction, language socialization, verbal art},
	pages = {631--653},
}

@article{feng_impact_2014,
	title = {The {Impact} of {Deep} {Hierarchical} {Discourse} {Structures} in the {Evaluation} of {Text} {Coherence}},
	journal = {Coling2014},
	author = {Feng, Vanessa Wei and Lin, Ziheng and Hirst, Graeme},
	year = {2014},
	note = {ISBN: 9781941643266},
	pages = {940--949},
}

@article{wolf_representing_2005,
	title = {Representing {Discourse} {Coherence}: {A} {Corpus}-{Based} {Study}},
	volume = {31},
	issn = {0891-2017},
	doi = {10.1162/0891201054223977},
	abstract = {This article aims to present a set of discourse structure relations that are easy to code and to develop criteria for an appropriate data structure for representing these relations. Discourse structure here refers to informational relations that hold between sentences in a discourse. The set of discourse relations introduced here is based on Hobbs (1985).},
	number = {2},
	journal = {Computational Linguistics},
	author = {Wolf, Florian and Gibson, Edward},
	year = {2005},
	note = {ISBN: 0891-2017},
	pages = {249--287},
}

@article{asr_implicitness_2012,
	title = {Implicitness of {Discourse} {Relations}},
	number = {December 2012},
	journal = {Coling},
	author = {Asr, Fatemeh Torabi and Demberg, Vera},
	year = {2012},
	keywords = {2684, causality, coling 2012, continuity, corpus study, december 2012, discourse cues, discourse relations, implicit discourse, mumbai, pages 2669, proceedings of coling 2012, relations, technical papers, uniform information density},
	pages = {2669--2684},
}

@article{taboada_discourse_2006,
	title = {Discourse markers as signals (or not) of rhetorical relations},
	volume = {38},
	issn = {03782166},
	url = {http://www.sciencedirect.com/science/article/pii/S0378216605002249},
	doi = {10.1016/j.pragma.2005.09.010},
	abstract = {Ever since the formulation of Rhetorical Structure Theory (RST) by Mann and Thompson, researchers have debated about what is the [`]right' number of relations. One proposal is based on the discourse markers (connectives) signalling the presence of a particular relationship. In this paper, I discuss the adequacy of such a proposal, in the light of two different corpus studies: a study of conversations, and a study of newspaper articles. The two corpora were analysed in terms of rhetorical relations, and later coded for external signals of those relations. The conclusion in both studies is that a high number of relations (between 60 and 70\% of the total, on average) are not signalled. A comparison between the two corpora suggests that genre-specific factors may affect which relations are signalled, and which are not.},
	number = {4},
	journal = {Journal of Pragmatics},
	author = {Taboada, Maite},
	year = {2006},
	note = {ISBN: 0378-2166},
	keywords = {1, Coherence relations, Conjunctions, Connectives, Conversation, Discourse markers, Discourse signalling, Newspaper text, Rhetorical Structure Theory, coherence relations, conjunctions, connectives, conversation, discourse markers, discourse signalling, markers, markers is part of, newspaper text, of discourse, rhetorical relations and discourse, rhetorical structure theory, the analysis of discourse, the more general analysis},
	pages = {567--592},
}

@article{iyyer_political_2014,
	title = {Political {Ideology} {Detection} {Using} {Recursive} {Neural} {Networks}},
	abstract = {An individual's words often reveal their po-litical ideology. Existing automated tech-niques to identify ideology from text focus on bags of words or wordlists, ignoring syn-tax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the po-litical position evinced by a sentence. To show the importance of modeling subsen-tential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset.},
	journal = {1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
	author = {Iyyer, Mohit and Enns, Peter and Boyd-graber, Jordan and Resnik, Philip},
	year = {2014},
	pages = {1113--1122},
}

@article{Snell2010,
	title = {From sociolinguistic variation to socially strategic stylisation},
	volume = {14},
	issn = {13606441},
	doi = {10.1111/j.1467-9841.2010.00457.x},
	abstract = {This article investigates the indexical relation between language, interactional stance and social class. Quantitative sociolinguistic analysis of a linguistic variable (the first person possessive singular) is combined with interactional analysis of the way one particular variant (possessive 'me', as in Me pencil's up me jumper) is used by speakers in 'stylised' interactional performances. The aim of this analysis is to explore: (1) how possessive 'me' is implicated in the construction and management of local identities and relationships; and (2) how macro-social categories, such as social class, relate to linguistic choice. The data for this analysis comes from an ethnographic study of the language practices of nine-to ten-year-old children in two socially-differentiated primary schools in north-east England. A secondary aim of the article is to spotlight the sociolinguistic sophistication of these young children, in particular, the working-class participants, who challenge the notion that the speech of working-class children is in any way 'impoverished'.},
	number = {5},
	journal = {Journal of Sociolinguistics},
	author = {Snell, Julia},
	year = {2010},
	pmid = {55450232},
	note = {ISBN: 1467-9841{\textbackslash}n1360-6441},
	keywords = {Ethnography, Indexicality, Morphological variation, Social class, Stance, Stylisation},
	pages = {630--656},
}

@article{goyal_structured_2013,
	title = {A structured distributional semantic model: integrating structure with semantics},
	journal = {Acl  …},
	author = {Goyal, Kartik and Jauhar, Sujay Kumar SK and Li, Huiying and Sachan, Mrinmaya and Srivastava, Shashank and Hovy, Eduard},
	year = {2013},
	note = {ISBN: 9781937284671},
	pages = {20},
}

@inproceedings{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	isbn = {978-1-61567-911-9},
	doi = {10.1.1.100.1089},
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	booktitle = {Neural {Information} {Processing} {Systems} 22},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
	year = {2009},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISSN: 1098-6596},
	pages = {288--296},
}

@article{solem_negotiating_2016,
	title = {Negotiating knowledge claims : {Students} ’ assertions in classroom interactions},
	volume = {18},
	issn = {1461-4456},
	doi = {10.1177/1461445616668072},
	abstract = {This study examines interactional sequences in which students make assertions about topic- relevant matters in classroom interactions. Using a Conversation Analytical approach, I show how the students’ knowledge claims lead to negotiations of sequential and epistemic rights to make such claims. Through these negotiations, the students upgrade their epistemic stance by repeating or backing their claims with accounts and providing evidence of them. The teachers’ acceptance or rejection of the students’ initiatives displays an orientation to the sequential and topical relevance of the information provided by the students. This study contributes to a better understanding of student initiatives in the classroom, a topic that until now has received scarce attention. Additionally, it contributes knowledge about the negotiation of epistemic authority in relation to assertions and their responses, which may have more general implications for the study of talk-in-interaction. Keywords},
	number = {6},
	journal = {Discourse Studies},
	author = {Solem, Marit Skarbø},
	year = {2016},
	keywords = {assertions, classroom interactions, conversation analysis, epistemics, negotiations, student},
}

@incollection{johnstone_discourse_nodate,
	title = {Discourse {Structure}: {Parts} and {Sequences}},
	isbn = {978-0-511-80574-5},
	booktitle = {Discourse {Analysis}},
	author = {Johnstone, Barbara},
	pages = {29--53},
}

@article{wen_transactivity_nodate,
	title = {Transactivity as a {Predictor} of {Future} {Collaborative} {Knowledge} {Integration} in {Team}-{Based} {Learning} in {Online} {Courses}},
	author = {Wen, Miaomiao and Maki, Keith and Wang, Xu and Dow, Steven P and Herbsleb, James and Rose, Carolyn},
	pages = {533--538},
}

@inproceedings{ai_finding_2010,
	title = {Finding transactive contributions in whole group classroom discussions},
	volume = {1},
	booktitle = {International {Conference} of the {Learning} {Sciences}},
	author = {Ai, Hua and Sionti, Marietta and Wang, Yi-Chia and Rosé, Carolyn Penstein},
	year = {2010},
	pages = {976--983},
}

@article{Stolcke2000,
	title = {Dialogue act modeling for automatic tagging and recognition of conversational speech},
	volume = {26},
	issn = {0891-2017},
	doi = {10.1162/089120100561737},
	abstract = {We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, Question, BACKCHANNEL, Agreement, Disagreement, and Apology. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65\% based on errorful, automatically recognized words and prosody, and 71\% based on word transcripts, compared to a chance baseline accuracy of 35\% and human accuracy of 84\%) and a small reduction in word recognition error.},
	number = {3},
	journal = {Computational linguistics},
	author = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and Ess-Dykema, C.V. and Meteer, Marie},
	year = {2000},
	note = {arXiv: cs/0006023
ISBN: 089120100561737},
	pages = {339--373},
}

@article{wharry_amen_2003,
	title = {Amen and {Hallelujah} preaching: {Discourse} functions in {African}  {American} sermons},
	volume = {32},
	issn = {0047-4045},
	doi = {10.1017/S0047404503322031},
	number = {1958},
	journal = {Language in Society},
	author = {Wharry, Cheryl},
	year = {2003},
	note = {Publisher: Carnegie Mellon University},
	keywords = {African American English, sermons, religious disco},
	pages = {203--225},
}

@misc{yarowsky_word-sense_nodate,
	title = {Word-{Sense} {Disambiguation} {Using} {Statistical} {Models} of {Roget}'s {Categories} {Trained} on {Large} {Corpora}},
	author = {Yarowsky, David},
}

@article{johnstone_intention_nodate,
	title = {Intention and {Interpretation}},
	author = {Johnstone, Barbara},
}

@article{Joshi2007,
	title = {Using transactivity in conversation for summarization of educational dialogue},
	url = {http://www.isca-speech.org/archive_open/slate_2007/sle7_053.html},
	abstract = {We present our ongoing work towards using the concept of transactivity for automatically assessing learning of students working together in a collaborative setting. Transactive segments of student dialogue are proposed as useful components of conversation summaries generated for instructors. Experimental evaluation of this hypothesis shows promising results. Further, initial results are presented for automatic identification of transactive contributions in student dialogue.},
	number = {SLaTE},
	journal = {Proceedings of the Speech and Language Technology in Education (SLaTE) Workshop 2007},
	author = {Joshi, Mahesh and Rosé, Carolyn},
	year = {2007},
	pages = {53--56},
}

@article{eisenstein_diffusion_2014,
	title = {Diffusion of lexical change in social media},
	volume = {9},
	issn = {19326203},
	doi = {10.1371/journal.pone.0113114},
	abstract = {Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity - especially with regard to race - plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified "netspeak" dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English.},
	number = {11},
	journal = {PLoS ONE},
	author = {Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A. and Xing, Eric P.},
	year = {2014},
	pmid = {25409166},
	note = {arXiv: 1210.5268v4
ISBN: 1932-6203 (Electronic){\textbackslash}n1932-6203 (Linking)},
	pages = {1--13},
}

@article{kittur_harnessing_2008,
	title = {Harnessing the wisdom of crowds in wikipedia: quality through coordination},
	doi = {10.1145/1460563.1460572},
	abstract = {Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy ...},
	journal = {Proceedings of the 2008 ACM conference on computer supported cooperative work},
	author = {Kittur, A and Kraut, Robert E},
	year = {2008},
	note = {ISBN: 9781605580074},
	pages = {37--46},
}

@article{gildea_automatic_2002,
	title = {Automatic labeling of semantic roles},
	volume = {28},
	issn = {08912017},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.137.1060},
	doi = {10.1.1.137.1060},
	abstract = {We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. 1},
	number = {3},
	journal = {Computational Linguistics},
	author = {Gildea, Daniel and Jurafsky, Dan},
	year = {2002},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 0891-2017},
	pages = {245--288},
}

@article{okeefe_editing_2015,
	title = {“{Editing}” {Genes}: {A} {Case} {Study} {About} {How} {Language} {Matters} in {Bioethics}},
	volume = {15},
	issn = {1526-5161},
	url = {http://www.tandfonline.com/doi/full/10.1080/15265161.2015.1103804},
	doi = {10.1080/15265161.2015.1103804},
	abstract = {Metaphors used to describe new technologies mediate public understanding of the innovations. Analyzing the linguistic, rhetorical, and affective aspects of these metaphors opens the range of issues available for bioethical scrutiny and increases public accountability. This article shows how such a multidisciplinary approach can be useful by looking at a set of texts about one issue, the use of a newly developed technique for genetic modification, CRISPRcas9.},
	number = {12},
	journal = {The American Journal of Bioethics},
	author = {O'Keefe, Meaghan and Perrault, Sarah and Halpern, Jodi and Ikemoto, Lisa and Yarborough, Mark},
	year = {2015},
	pmid = {26632354},
	note = {ISBN: 1526-5161},
	keywords = {crispr, genomic, germline modification, metaphor, public engagement, rhetoric},
	pages = {3--10},
}

@article{hu_harnessing_2016,
	title = {Harnessing {Deep} {Neural} {Networks} with {Logic} {Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	doi = {10.18653/v1/P16-1228},
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	journal = {Acl},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	year = {2016},
	note = {arXiv: 1603.06318},
}

@article{fillmore_frame_1967,
	title = {Frame semantics and the nature of language},
	author = {Fillmore, Charles J},
	year = {1967},
	pages = {20--32},
}

@inproceedings{guarino_organizing_1997,
	title = {Some {Organizing} {Principles} {For} {A} {Unified} {Top}-{Level} {Ontology}},
	url = {http://www.ladseb.pd.cnr.it/infor/Ontology/ontology.html},
	abstract = {Currently, a number of efforts in the ontological engineering community are aimed to the development of large ontologies, to be used for a variety of tasks (CYC Guha and Lenat 1990, Lenat and Guha 1990, Penman/Pangloss/SENSUS Bateman et al. 1990, Knight and Luk 1994, Swartout et al. 1996, MikroKosmos Mahesh 1996, Wordnet Miller 1995). An important challenge would be the realization of a unified top level for such ontologies, which should be the result of an integration work. I shall discuss in this paper some general organizing principles which can be of help in this integration effort, based on the quest for semantic rigour, clear ontological foundation, and cognitive plausibility. The general perspective I have in mind is that of Formal Ontology Guarino 1995, Smith 1995, which can be intended as the theory of formal distinctions between the elements of a domain, independently of their actual reality. I shall briefly present what I call the basic conceptual tools of formal o},
	booktitle = {{AAAI} {Spring} {Symposium} on {Ontological} {Engineering}},
	author = {Guarino, Nicola},
	year = {1997},
	note = {Issue: March},
	pages = {57--63},
}

@article{moelleken_language_1983,
	title = {Language {Maintenance} and {Language} {Shift} in {Pennsylvania} {German} : {A} {Comparative} {Investigation}},
	volume = {75},
	url = {http://www.jstor.org/stable/30157},
	number = {2},
	journal = {Monatshefte},
	author = {Moelleken, Wolfgang W.},
	year = {1983},
	pages = {172--186},
}

@incollection{johnstone_chapter_nodate,
	title = {Chapter 2: {Discourse} and {World}},
	booktitle = {Discourse {Analysis}},
	publisher = {Wiley},
	author = {Johnstone, Barbara},
}

@misc{ferguson_06-ferguson_fishman.pdf_nodate,
	title = {06-{Ferguson}\_Fishman.pdf},
	author = {Ferguson, Charles and Fishman, Joshua A.},
}

@article{chalupsky_sl:_nodate,
	title = {{SL}: a subjective, intensional logic of belief},
	author = {Chalupsky, Hans and Shapiro, Stuart C.},
}

@article{Simpson1997,
	title = {Metapragmatic {Discourse} and the {Ideology} of {Impolite} {Pronouns} in {Thai}},
	volume = {7},
	issn = {1548-1395},
	url = {http://dx.doi.org/10.1525/jlin.1997.7.1.38},
	doi = {10.1525/jlin.1997.7.1.38},
	abstract = {The complex system of person-referring expressions in Thai sheds light on relationships between social structures, language use, and language ideologies. This article looks critically at the nature and implications of native speakers' metapragmatic awareness of person-referring expressions. The analysis focuses on two transcripts of spontaneous metapragmatic discourse about the appropriate use of so-called impolite pronouns, highlighting gender differences in usage and attitudes and situating this empirical evidence within the broader theoretical framework of language ideology.},
	number = {1},
	journal = {Journal of Linguistic Anthropology},
	author = {Simpson, Rita C},
	year = {1997},
	pages = {38--62},
}

@article{johnstone_chapter_nodate-1,
	title = {Chapter 1 : {Introduction}},
	issn = {15710661},
	doi = {10.1016/S0165-1250(05)80001-X},
	journal = {Discourse Analysis, 3rd edition},
	author = {Johnstone, Barbara},
	pmid = {20314319},
	note = {arXiv: 1011.1669v3
ISBN: 9781405110914},
	pages = {1--170},
}

@misc{noauthor_lappin_nodate,
	title = {Lappin {Oxford} handbook},
}

@article{zafar_message_2016,
	title = {Message impartiality in social media discussions},
	abstract = {Discourse on social media platforms is often plagued by acute polarization, with different camps promoting different perspectives on the issue at hand—compare, for example, the differences in the liberal and conservative discourse on the U.S. immigration debate. A large body of research has studied this phenomenon by focusing on the affiliation of groups and individuals. We propose a new finer-grained perspective: studying the impartiality of individual messages. While the notion of message impartiality is quite intuitive, the lack of an objective definition and of a way to measure it directly has largely obstructed scientific examination. In this work we operationalize message impartiality in terms of how discernible the affiliation of its author is, and introduce a methodology for quantifying it automatically. Unlike a supervised machine learning approach, our method can be used in the context of emerging events where impartiality labels are not immediately available. Our framework enables us to study the effects of (im)partiality on social media discussions at scale. We show that this phenomenon is highly consequential, with partial messages being twice more likely to spread than impartial ones, even after controlling for author and topic. By taking this fine-grained approach to polarization, we also provide new insights into the temporal evolution of online discussions centered around major political and sporting events.},
	journal = {Icwsm},
	author = {Zafar, Muhammad Bilal and Gummadi, Krishna P and Danescu-niculescu-mizil, Cristian},
	year = {2016},
	note = {ISBN: 9781577357582},
}

@article{Yang2016,
	title = {Who {Did} {What}: {Editor} {Role} {Identification} in {Wikipedia}},
	journal = {Proc. ICWSM},
	author = {Yang, Diyi and Halfaker, Aaron and Kraut, Robert and Hovy, Eduard},
	year = {2016},
	note = {ISBN: 9781577357582},
	keywords = {Full Papers},
	pages = {446--455},
}

@incollection{eckert_we_nodate,
	title = {We are what we do},
	author = {Eckert, Penelope},
}

@incollection{eckert_interpreting_nodate,
	title = {Interpreting the {Meaning} of {Variation}},
	author = {Eckert, Penelope},
}

@incollection{eckert_style_nodate,
	title = {Style, social meaning, and sound change},
	author = {Eckert, Penelope},
}

@book{brown_politeness:_1987,
	address = {Cambridge, UK},
	title = {Politeness: some universals in language usage},
	publisher = {Cambridge University Press},
	author = {Brown, Penelope and Levinson, Stephen C.},
	year = {1987},
}

@incollection{Coupland2007,
	title = {Sociolinguistic resources for styling; {Styling} social identities},
	booktitle = {Style: language variation and identity},
	author = {Coupland, Nikolas},
	year = {2007},
	pages = {82--145},
}

@article{Gumperz1968,
	title = {The {Speech} {Community}},
	doi = {10.1111/b.9781405116923.2003.x},
	abstract = {The speech community (SpCom), a core concept in empirical linguistics, is at the intersection of many principal problems in sociolinguistic theory and method. This paper traces its history of development and divergence, surveys general problems with contemporary notions, and discusses links to key issues in investigating language variation and change. It neither offers a new and correct definition nor rejects the concept (both are seen as misguided efforts), nor does it exhaustively survey the applications in the field (an impossibly large task).},
	journal = {International Encyclopedia of the Social Sciences Vol. 9},
	author = {Gumperz, John J.},
	year = {1968},
	pmid = {4065649},
	note = {ISBN: 9781405126335},
	pages = {381--386},
}

@article{Gal1978,
	title = {Peasant men can't get wives: language change and sex roles in a bilingual community},
	volume = {7},
	issn = {0047-4045},
	doi = {10.1017/S0047404500005303},
	abstract = {Language shift from German-Hungarian bilingualism to the exclusive use of German is occurring in the community discussed. Young women are further along in the direction of this change than older people and young men. The linguistic contrast between German and Hungarian is shown to represent the social dichotomy between a newly available worker status and traditional peasant status; thus the choice of language in interaction is part of a speaker's presentation of self. Young women's stated preferences con- cerning this social dichotomy and their changing marriage strategies indicate that their greater use of German in interaction is one aspect of their general preference for the worker's way of life it symbolizes. Rather than simply isolating a linguistic correlate of sex, the present study suggests that women's speech choices must be explained within the context of their social position, their strategic life choices and the symbolic values of the available linguistic alternative.},
	number = {01},
	journal = {Language in Society},
	author = {Gal, Susan},
	year = {1978},
	note = {ISBN: 0312175736},
	pages = {1},
}

@article{Johnstone2008,
	title = {Indexicality and experience: {Exploring} the meanings of /aw/-monophthongization in {Pittsburgh}},
	volume = {12},
	issn = {13606441},
	doi = {10.1111/j.1467-9841.2008.00351.x},
	abstract = {In this paperwe test the hypothesis that monophthongal /aw/ is semiotically associated with local identity in Pittsburgh. We compare results of an experimental task that directly elicits participants’ sense of the indexical value of /aw/-monophthongization with the occurrence of this variant in the same people’s speech. Peoplewho hear monophthongal /aw/ as an index of localness are unlikely to have this feature in their own speech, and many of the people who do monophthongize /aw/ do not associate this variant with localness. Exploringhowfour of these participants talk about this feature and its meanings, we show that the indexical meanings of speech features can vary widely within a community, and we illustrate the danger of confusing the meaning assigned by hearers to a linguistic formwith the meaning users would assign to it.We suggest that a phenomenological approach, attending to themultiplicity and indeterminacy of indexical relations and to howsuch relations arise historicallyandin livedexperience,canlead toamorenuanced accountofthedistributionofsocialmeaningsofvariantformsthancanstudies of perception or production alone.},
	number = {1},
	journal = {Journal of Sociolinguistics},
	author = {Johnstone, Barbara and Kiesling, Scott F.},
	year = {2008},
	note = {ISBN: 1360-6441},
	keywords = {Dialect awareness, Experience, Indexicality, Phenomenology, Pittsburgh, Place identitity},
	pages = {5--33},
}

@article{Dubois1999,
	title = {When the music changes, you change too: {Gender} and language change in {Cajun} {English}},
	volume = {11},
	issn = {0954-3945},
	doi = {10.1017/S0954394599113036},
	abstract = {The role of gender in language change, as discussed in Penelope Eckert (1989) \& William Labov (1990), forms the context for an exploration of the role of gender in the development of Cajun English. Neither Principle I, Ia, or II predicts the role of gender in Cajun English, which leads us to question the generalizability of the principles to the specific sociolinguistic setting of this study - a closed cultural enclave. The study of four sociolinguistic variables \& three generations of speakers reveals two patterns of language change: a curvilinear or v-shaped age pattern \& a linear age pattern. These patterns relate in a complex way to changes from above \& below the level of consciousness. We support Eckert's call for a finer specification of the social categories but suggest alternatives to the ethnographic method. Using a variety of sources of information on the social life \& sociohistory of three generations, we find an intimate association between the sociohistory of this Cajun community \& the linguistic behavior of each generation},
	number = {03},
	journal = {Language Variation and Change},
	author = {Dubois, Sylvie and Horvath, Barbara},
	year = {1999},
	keywords = {0954-3945 0 00, 11, 2000, 2000 cambridge university press, 287, 313, 50, 9, a, gender and language change, guage variation and change, in cajun english, printed in the u, s, when the music changes, you change too},
	pages = {287--313},
}

@article{campbell-kibler_ill_2008,
	title = {I'll be the judge of that: {Diversity} in social perceptions of ({ING})},
	volume = {37},
	issn = {0047-4045},
	doi = {10.1017/S0047404508080974},
	abstract = {This article examines divergent listener perceptions with an expanded form of the Matched Guise Technique, using 32 matched pairs of short recordings of natural speech. Social evaluations were collected in open-ended interviews (N = 55) and an online experiment (N = 124). Three speakers are described who prompted disagreement about the English variable (ING). One's -ing use is seen by some as more intelligent and by others as annoying, less intelligent, and trying to impress. Another's -in guise is seen as compassionate by some and as condescending by others, while a third, when using -in, is seen by some as annoying and less masculine, while others describe him as a masculine ``jock.'' These findings show that listeners shift their interpretations of a linguistic resource, highlighting the ambiguous role intention plays in social meaning and calling into question long-held assumptions about the need for conscious introspection in sociolinguistic perception.},
	journal = {Language in Society},
	author = {Campbell-Kibler, Kathryn},
	year = {2008},
	note = {ISBN: 0047-4045},
	pages = {637--659},
}

@book{Brown1960,
	title = {The {Pronouns} of {Power} and {Solidarity}},
	isbn = {0-631-22717-2},
	abstract = {Discusses use of formal and informal pronouns like tu and usted in spanish and how their usage has changed over time. Apparently everyone wants to be informal and the causes and implicTions of that.},
	author = {Brown, Roger and Gilman, Albert},
	year = {1960},
	note = {Publication Title: Style in Language},
}

@article{piller_neoliberalism_2013,
	title = {Neoliberalism as language policy},
	volume = {42},
	issn = {0047-4045},
	url = {http://www.journals.cambridge.org/abstract_S0047404512000887},
	doi = {10.1017/S0047404512000887},
	abstract = {This article explores howan economic ideology—neoliberalism—serves as a covert language policy mechanism pushing the global spread of English. Our analysis builds on a case study of the spread of English as a medium of in- struction (MoI) in South Korean higher education. The Asian financial crisis of 1997/98 was the catalyst for a set of socioeconomic transformations that led to the imposition of “competitiveness” as a core value.Competition is heavily structured through a host of testing, assessment, and rankingmechan- isms, manyof which explicitly privilege English as a terrainwhere individual and societal worth are established. University rankings are one suchmechan- ism structuring competition and constituting a covert form of language policy. One ranking criterion—internationalization—is particularly easy to manipulate and strongly favors English MoI. We conclude by reflecting on the social costs of elevating competitiveness to a core value enacted on the terrain of language choice.},
	number = {01},
	journal = {Language in Society},
	author = {Piller, Ingrid and Cho, Jinhyun},
	year = {2013},
	note = {ISBN: doi:10.1017/S0047404512000887},
	keywords = {English as a global language, South Korea, globalization, higher education, medium of instruction (MoI), neoliberalism, university rankings},
	pages = {23--44},
}

@article{timm_diglossia_1981,
	title = {Diglossia old and new--a critique},
	volume = {23},
	number = {8},
	journal = {Anthropological Linguistics},
	author = {Timm, Lenora A.},
	year = {1981},
	pages = {356--367},
}

@article{bailey_language_2001,
	title = {The {Language} of {Multiple} {Identities} among {Dominican} {Americans}},
	volume = {10},
	number = {2},
	journal = {Journal of Linguistic Anthropology},
	author = {Bailey, Benjamin},
	year = {2001},
	pages = {190--223},
}

@article{Labov1963,
	title = {The social motivation of a sound change},
	volume = {19},
	number = {3},
	journal = {Word},
	author = {Labov, William},
	year = {1963},
	pages = {273--309},
}

@article{Molina2016,
	title = {Overview for the {Second} {Shared} {Task} on {Language} {Identification} in {Code}-{Switched} {Data}},
	journal = {Proceedings of The Second Workshop on Computational Approaches to Code Switching, held in conjunction with EMNLP 2016.},
	author = {Molina, Giovanni and {Rey-Villamizar} and Solorio, Thamar and AlGhamdi, Fahad and Gohneim, Mahmoud and Hawwari, Abdelati and Diab, Mona},
	year = {2016},
	keywords = {Code-Switching},
	pages = {62--72},
}

@article{Solorio2014,
	title = {Overview for the {First} {Shared} {Task} on {Language} {Identification} in {Code}-{Switched} {Data}},
	journal = {Proceedings of The First Workshop on Computational Approaches to Code Switching, held in conjunction with EMNLP 2014.},
	author = {Solorio, Thamar and Blair, Elizabeth and Maharjan, Suraj and Bethard, Steven and Diab, Mona and Gohneim, Mahmoud and Hawwari, Abdelati and AlGhamdi, Fahad and Hirschberg, Julia and Chang, Alison and Fung, Pascale},
	year = {2014},
	keywords = {Code-Switching},
	pages = {62--72},
}

@article{habash_introduction_2010,
	title = {Introduction to {Arabic} {Natural} {Language} {Processing}},
	volume = {3},
	issn = {1947-4040},
	doi = {10.2200/S00277ED1V01Y201008HLT010},
	abstract = {Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is "Arabic"? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...},
	number = {1},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Habash, Nizar Y.},
	year = {2010},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9781598297959},
	pages = {1--187},
}

@article{xia_codeswitching_2016,
	title = {Codeswitching language identification using {Subword} {Information} {Enriched} {Word} {Vectors}},
	author = {Xia, Meng Xuan},
	year = {2016},
	pages = {132--136},
}

@article{MacSwan2000,
	title = {The architecture of the bilingual language faculty: evidence from intrasentential code switching},
	volume = {3},
	issn = {13667289},
	doi = {10.1017/S1366728900000122},
	abstract = {In this article, the author addresses the question of how the mind represents two languages in simultaneous bilingualism. Some linguistic theories of intrasentential code switching are reviewed, with a focus on the Minimalist approach of MacSwan (1999b); the author concludes that evidence from code switching suggests that bilinguals have discrete and separate Lexicons for the languages they speak, each with its own internal principles of word formation, as well as separate phonological systems. However, the author argues that computational resources common to the two languages generate monolingual and bilingual syntactic derivations alike. Advantages of the Minimalist Program for the analysis of code switching data are discussed at some length.},
	number = {1},
	journal = {Bilingualism: Language and Cognition},
	author = {MacSwan, Jeff},
	year = {2000},
	note = {ISBN: 1366-7289},
	pages = {37--54},
}

@article{Piergallini2016,
	title = {Word-{Level} {Language} {Identification} and {Predicting} {Codeswitching} {Points} in {Swahili}-{English} {Language} {Data}},
	author = {Piergallini, Mario and Shirvani, Rouzbeh and Gautam, Gauri S and Chouikha, Mohamed},
	year = {2016},
	pages = {21--29},
}

@article{Solorio2008,
	title = {Learning to predict code-switch points},
	issn = {00237205},
	doi = {10.16373/j.cnki.ahr.150049},
	abstract = {Cognitive therapists suggest panic disorder to result from 'catastrophic' misinterpretation of bodily sensations. The patient suffering from panic disorder consistently misinterprets normal anxiety responses, such as racing heart, breathlessness or dizziness, as indicating impending disaster. Cognitive therapists, who challenge the traditional view of anxiety as 'free-floating' and irrational, argue that the patient's anxiety is an understandable response to their misinterpretations, and advocate a treatment method based on the patient's specific cognitive make-up and on the principle of collaborative empiricism. The patient is gently guided to identify and challenge idiosyncratic cognitions, and to consider alternative interpretations of danger signs. The article provides an outline of the treatment method and its empirical support.},
	journal = {EMNLP '08 Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Solorio, Thamar and Liu, Yang},
	year = {2008},
	pmid = {22352717},
	note = {ISBN: 0894-0282},
	pages = {973--981},
}

@article{bhat_language_2014,
	title = {Language {Identification} in {Code}-{Switching} {Scenario}},
	journal = {Proceedings of the First Workshop on Computational Approaches to Code Switching},
	author = {Bhat, Riyaz Ahmad},
	year = {2014},
	pages = {87--93},
}

@article{ling_word_2016,
	title = {Word {Embeddings} with {Limited} {Memory}},
	abstract = {This paper studies the effect of limited pre-cision data representation and computa-tion on word embeddings. We present a systematic evaluation of word embeddings with limited memory and discuss method-s that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks.},
	journal = {Acl},
	author = {Ling, Shaoshi and Song, Yangqiu and Roth, Dan},
	year = {2016},
	pages = {387--392},
}

@article{Rudra2016,
	title = {Understanding {Language} {Preference} for {Expression} of {Opinion} and {Sentiment}: {What} do {Hindi}-{English} {Speakers} do on {Twitter}?},
	author = {Rudra, Koustav and Rijhwani, Shruti and Begum, Rafiya and Bali, Kalika and Choudhury, Monojit and Ganguly, Niloy},
	year = {2016},
	pages = {1131--1141},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	issn = {10495258},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journal = {arXiv preprint arXiv: …},
	author = {Goodfellow, Ij and Pouget-Abadie, J and Mirza, Mehdi},
	year = {2014},
	note = {arXiv: 1406.2661v1
ISBN: 1406.2661},
	pages = {1--9},
}

@article{viser_attempted_2003,
	title = {Attempted {Objectivity}: {An} {Analysis} of the {New} {York} {Timesand} {Ha}'aretz and their {Portrayals} of the {Palestinian}-{Israeli} {Conflict}},
	volume = {8},
	issn = {1081180X},
	url = {http://hij.sagepub.com/cgi/doi/10.1177/1081180X03256999},
	doi = {10.1177/1081180X03256999},
	number = {4},
	journal = {The Harvard International Journal of Press/Politics},
	author = {Viser, Matt},
	year = {2003},
	keywords = {and lepper demonstrated that, likely to perceive the, media, media bias, media coverage, nearly twenty years ago, on both sides of, palestinian-israeli conflict, partisans, ross, the palestinian-israeli conflict are, vallone},
	pages = {114--120},
}

@article{goldberg_word2vec_2014,
	title = {word2vec {Explained}: {Deriving} {Mikolov} et al.'s {Negative}-{Sampling} {Word}-{Embedding} {Method}},
	url = {http://arxiv.org/abs/1402.3722},
	abstract = {The word2vec software of Tomas Mikolov and colleagues1 has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers [1, 2]. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in “Dis- tributed Representations ofWords and Phrases and their Compositionality” by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean [2].},
	number = {2},
	journal = {arXiv preprint arXiv:1402.3722},
	author = {Goldberg, Yoav and Levy, Omer},
	year = {2014},
	note = {arXiv: 1402.3722v1},
	pages = {1--5},
}

@article{ma_natural_2002,
	title = {Natural {Language} {Processing} with {Neural} {Networks}},
	journal = {Proceedings of the Language Engineering Conference (LEC '02)},
	author = {Ma, Qing},
	year = {2002},
	note = {ISBN: 0769518850},
	pages = {1--12},
}

@article{steinmacher_social_2015,
	title = {Social {Barriers} {Faced} by {Newcomers} {Placing} {Their} {First} {Contribution} in {Open} {Source} {Software} {Projects}},
	issn = {9781450329224},
	url = {http://dl.acm.org/citation.cfm?doid=2675133.2675215},
	doi = {10.1145/2675133.2675215},
	abstract = {Newcomers' seamless onboarding is important for online communities that depend upon leveraging the contribution of outsiders. Previous studies investigated aspects of the joining process and motivation in open collaboration communities, but few have},
	journal = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing - CSCW '15},
	author = {Steinmacher, Igor and Conte, Tayana and Gerosa, Marco Aurélio and Redmiles, David},
	year = {2015},
	note = {ISBN: 9781450329224},
	pages = {1379--1392},
}

@article{singh_minimally-supervised_2010,
	title = {Minimally-{Supervised} {Extraction} of {Entities} from {Text} {Advertisements}},
	url = {http://www.aclweb.org/anthology/N/N10/N10-1009},
	abstract = {Extraction of entities from ad creatives is an important problem that can benefit many computational advertising tasks. Supervised and semi-supervised solutions rely on labeled data which is expensive, time consuming, and difficult to procure for ad creatives. A small set of manually derived constraints on feature expectations over unlabeled data can be used to partially and probabilistically label large amounts of data. Utilizing recent work in constraint-based semi-supervised learning, this paper injects light weight supervision specified as these "constraints" into a semi-Markov conditional random field model of entity extraction in ad creatives. Relying solely on the constraints, the model is trained on a set of unlabeled ads using an online learning algorithm. We demonstrate significant accuracy improvements on a manually labeled test set as compared to a baseline dictionary approach. We also achieve accuracy that approaches a fully supervised classifier.},
	number = {June},
	journal = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Singh, Sameer and Hillard, Dustin and Leggetter, Chris},
	year = {2010},
	note = {ISBN: 1-932432-65-5},
	pages = {73--81},
}

@article{Bracewell2012,
	title = {Annotation of {Adversarial} and {Collegial} {Social} {Actions} in {Discourse}},
	number = {July},
	journal = {6th Linguistic Annotation Workshop},
	author = {Bracewell, David B and Tomlinson, Marc T and Brunson, Mary and Plymale, Jesse and Bracewell, Jiajun and Boerger, Daniel},
	year = {2012},
	note = {ISBN: 9781937284329},
	pages = {184--192},
}

@article{Swayamdipta2012,
	title = {The pursuit of power and its manifestation in written dialog},
	doi = {10.1109/ICSC.2012.49},
	abstract = {In this paper we explore the written dialog behavior of participants in anon line discussion for automatic identification of participants who pursue power within the discussion group. We employ various standard unsupervised machine learning approaches to make this prediction. Our approach relies on the identification of certain discourse structures and linguistic techniques used by participants in the discussion. We achive an F-measure of 69.5\% using unsupervised methods.},
	journal = {Proceedings - IEEE 6th International Conference on Semantic Computing, ICSC 2012},
	author = {Swayamdipta, Swabha and Rambow, Owen},
	year = {2012},
	note = {ISBN: 9780769548593},
	pages = {22--29},
}

@article{Slavin1980,
	title = {Cooperative learning},
	volume = {50},
	issn = {0034-6543},
	doi = {10.1016/B978-0-08-044894-7.00494-2},
	abstract = {Cooperative learning refers to instructional methods in which students work in small groups to help each other learn. This article reviews four major theoretical perspectives on achievement effects of cooperative learning: motivational, social cohesion, cognitive developmental, and cognitive elaboration. Evidence from classroom research supports the motivational perspective, which emphasizes the use of group goals and individual accountability for group success. However, there are conditions under which methods derived from all four theoretical perspectives contribute to achievement gain. This article reconciles these perspectives in a unified theory of cooperative learning effects. ?? 2010 Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {International Encyclopedia of Education},
	author = {Slavin, R. E.},
	year = {1980},
	note = {ISBN: 9780080448947},
	keywords = {Academic achievement, Cognitive theories of learning, Collaborative learning, Cooperative learning, Motivation, Peer tutoring, Social cohesion},
	pages = {315--342},
}

@article{Ferschke2012,
	title = {Behind the {Article} : {Recognizing} {Dialog} {Acts} in {Wikipedia} {Talk} {Pages}},
	author = {Ferschke, Oliver and Gurevych, Iryna and Chebotar, Yevgen},
	year = {2012},
	pages = {777--786},
}

@article{Priedhorsky2007,
	title = {Creating, destroying, and restoring value in wikipedia},
	url = {http://portal.acm.org/citation.cfm?doid=1316624.1316663},
	doi = {10.1145/1316624.1316663},
	abstract = {Wikipedia’s brilliance and curse is that any user can edit any of the encyclopedia entries. We introduce the notion of the impact of an edit, measured by the number of times the edited version is viewed. Using several datasets, including recent logs of all article views, we show that an overwhelm- ing majority of the viewed words were written by frequent editors and that this majority is increasing. Similarly, using the same impact measure, we show that the probability of a typical article view being damaged is small but increasing, and we present empirically grounded classes of damage. Fi- nally, we make policy recommendations for Wikipedia and other wikis in light of these findings},
	journal = {Proceedings of the 2007 international ACM conference on Conference on supporting group work - GROUP '07},
	author = {Priedhorsky, Reid and Chen, Jilin and Lam, Shyong Tony K and Panciera, Katherine and Terveen, Loren and Riedl, John},
	year = {2007},
	note = {ISBN: 9781595938459},
	keywords = {collaboration, damage, vandalism, wiki, wikipedia},
	pages = {259},
}

@article{al-sabbagh_yadac:_2012,
	title = {{YADAC}: {Yet} another {Dialectal} {Arabic} {Corpus}.},
	abstract = {This paper presents the first phase of building YADAC – a multi-genre Dialectal Arabic (DA) corpus – that is compiled using Web data from microblogs (i.e. Twitter), blogs/forums and online knowledge market services in which both questions and answers are user-generated. In addition to introducing two new genres to the current efforts of building DA corpora (i.e. microblogs and question-answer pairs extracted from online knowledge market services), the paper highlights and tackles several new issues related to building DA corpora that have not been handled in previous studies: function-based Web harvesting and dialect identification, vowel-based spelling variation, linguistic hypercorrection and its effect on spelling variation, unsupervised Part-of-Speech (POS) tagging and base phrase chunking for DA. Although the algorithms for both POS tagging and base-phrase chunking are still under development, the results are promising.},
	journal = {Lrec},
	author = {Al-Sabbagh, R and Girju, Roxana},
	year = {2012},
	note = {ISBN: 978-2-9517408-7-7},
	keywords = {dialect identification, dialectal arabic, pos tagging},
	pages = {2882--2889},
}

@article{Arazy2017,
	title = {Turbulent {Stability} of {Emergent} {Roles}: {The} {Dualistic} {Nature} of {Self}-{Organizing} {Knowledge} {Co}-{Production}},
	journal = {Information Systems Research},
	author = {Arazy, Ofer and Daxenberger, Johannes and Lifshitz-Assaf, Hila and Nov, Oded and Gurevych, Iryna},
	year = {2017},
	pages = {to appear},
}

@article{samih_arabic-moroccan_2016,
	title = {An {Arabic}-{Moroccan} {Darija} {Code}-{Switched} {Corpus}},
	author = {Samih, Younes and Maier, Wolfgang},
	year = {2016},
	keywords = {code-switching, language identification, moroccan arabic},
	pages = {4170--4175},
}

@article{du_dirichlet-hawkes_2015,
	title = {Dirichlet-{Hawkes} {Processes} with {Applications} to {Clustering} {Continuous}-{Time} {Document} {Streams} {Categories} and {Subject} {Descriptors}},
	doi = {10.1145/2783258.2783411},
	abstract = {Clusters in document streams, such as online news articles, can be induced by their textual contents, as well as by the temporal dynamics of their arriving patterns. Can we lever-age both sources of information to obtain a better clustering of the documents, and distill information that is not possi-ble to extract using contents only? In this paper, we pro-pose a novel random process, referred to as the Dirichlet-Hawkes process, to take into account both information in a unified framework. A distinctive feature of the proposed model is that the preferential attachment of items to clusters according to cluster sizes, present in Dirichlet processes, is now driven according to the intensities of cluster-wise self-exciting temporal point processes, the Hawkes processes. This new model establishes a previously unexplored connec-tion between Bayesian Nonparametrics and temporal Point Processes, which makes the number of clusters grow to ac-commodate the increasing complexity of online streaming contents, while at the same time adapts to the ever chang-ing dynamics of the respective continuous arrival time. We conducted large-scale experiments on both synthetic and real world news articles, and show that Dirichlet-Hawkes processes can recover both meaningful topics and temporal dynamics, which leads to better predictive performance in terms of content perplexity and arrival time of future docu-ments.},
	journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Du, Nan and Ahmed, Amr and Smola, Alexander J},
	year = {2015},
	note = {ISBN: 9781450336642},
	keywords = {all or part of, dirichlet process, document modeling, hawkes process, is granted without fee, or hard copies of, permission to make digital, personal or classroom use, provided that copies are, this work for},
	pages = {219--228},
}

@article{Viegas2007,
	title = {Talk before you type: coordination in {Wikipedia}},
	volume = {1},
	issn = {1098-6596},
	doi = {10.1017/CBO9781107415324.004},
	journal = {40th Hawaii International Conference on System Sciences},
	author = {Viégas, Fernanda B. and Wattenberg, Martin and Kriss, Jesse and van Ham, Frank},
	year = {2007},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
	keywords = {icle},
	pages = {1--10},
}

@article{keegan_editors_2012,
	title = {Do editors or articles drive collaboration? {Multilevel} {Statistical} {Network} {Analysis} of {Wikipedia} {Coauthorship}},
	url = {http://doi.acm.org/10.1145/2145204.2145271\nhttp://dl.acm.org/citation.cfm?doid=2145204.2145271},
	doi = {10.1145/2145204.2145271},
	abstract = {Prior scholarship on Wikipedia's collaboration processes has examined the properties of either editors or articles, but not the interactions between both. We analyze the coauthorship network of Wikipedia articles about breaking news demanding intense coordination and compare the properties of these articles and the editors who contribute to them to articles about contemporary and historical events. Using \{p*/ERGM\} methods to test a multi-level, multi-theoretical model, we identify how editors' attributes and editing patterns interact with articles' attributes and authorship history. Editors' attributes like prior experience have a stronger influence on collaboration patterns, but article attributes also play significant roles. Finally, we discuss the implications our findings and methods have for understanding the socio-material duality of collective intelligence systems beyond Wikipedia.},
	journal = {Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work - CSCW '12},
	author = {Keegan, Brian and Gergle, Darren and Contractor, Noshir},
	year = {2012},
	note = {ISBN: 9781450310864},
	pages = {427--436},
}

@article{duma_citation_nodate,
	title = {Citation {Resolution}: {A} method for evaluating context-based citation recommendation systems},
	abstract = {Wouldn't it be helpful if your text edi-tor automatically suggested papers that are relevant to your research? Wouldn't it be even better if those suggestions were contextually relevant? In this paper we name a system that would accomplish this a context-based citation recommendation (CBCR) system. We specifically present Citation Resolution, a method for the eval-uation of CBCR systems which exclusively uses readily-available scientific articles. Exploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation. We ap-ply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the sur-rounding contexts of citations to the doc-ument found in other documents, and c) a mixture of the two.},
	author = {Duma, Daniel and Ewan Klein, smsedacuk},
	pages = {358--363},
}

@article{venugopal_syntax_2006,
	title = {Syntax {Augmented} {Machine} {Translation} via {Chart} {Parsing} with {Integrated} {Language} {Modelling}},
	journal = {Proceedings of the Workshop on Statistical Machine Translation},
	author = {Venugopal, Ashish and Zollmann, Andreas},
	year = {2006},
	keywords = {Machine Translation, samt},
	pages = {138--141},
}

@article{noauthor_c12-1082_nodate,
	title = {C12-1082},
}

@article{gerges_romanized_2012,
	title = {Romanized {Arabic} {Transliteration}},
	volume = {2},
	number = {December},
	journal = {24th International Conference on …},
	author = {Gerges, ACH},
	year = {2012},
	pages = {89--96},
}

@book{diab_emnlp_2014,
	title = {{EMNLP} 2014: {First} {Workshop} on {Computational} {Approaches} to {Code} {Switching}},
	isbn = {978-1-937284-96-1},
	abstract = {Code-switching (CS) is the phenomenon by which multilingual speakers switch back and forth between their common languages in written or spoken communication. CS is pervasive in informal text communications such as news groups, tweets, blogs, and other social media of multilingual communities. Such genres are increasingly being studied as rich sources of social, commercial and political information. Apart from the informal genre challenge associated with such data within a single language processing scenario, the CS phenomenon adds another significant layer of complexity to the processing of the data. Efficiently and robustly processing CS data presents a new frontier for our NLP algorithms on all levels. The goal of this workshop is to bring together researchers interested in exploring these new frontiers, discussing state of the art research in CS, and identifying the next steps in this fascinating research area. The workshop program includes exciting papers discussing new approaches for CS data and the development of linguistic resources needed to process and study CS. We received a total of 17 regular workshop submissions of which we accepted eight for publication (47\% acceptance rate), five of them as workshop talks and three as posters. The accepted workshop submissions cover a wide variety of language combinations from languages such as English, Hindi, Bengali, Turkish, Dutch, German, Italian, Romansh, Mandarin, Dialectical Arabic and Modern Standard Arabic. Although most papers focus on some kind of social media data, there is also work on more formal genres, such as that from the Canadian Hansard. Another component of the workshop is the First Shared Task on Language Identification of CS Data. The shared task focused on social media and included four language pairs: Mandarin-English, Modern Standard Arabic-Dialectal Arabic, Nepali-English, and Spanish-English. We received a total of 42 system runs from seven different teams. Each team submitted a shared task paper describing their system. All shared task systems will be presented during the workshop poster session and a subset of them will also present a talk. We would like to thank all authors who submitted their contributions to this workshop and all shared task participants for taking on the challenge of language identification in code switched data. We also thank the program committee members for their help in providing meaningful reviews. Lastly, we thank the EMNLP 2014 organizers for the opportunity to put together this workshop.},
	author = {Diab, Mona and Hirchberg, Julia and Fung, Pascale and Solorio, Thamar},
	year = {2014},
	note = {Publication Title: First Workshop on Computational Approaches to Code Switching},
}

@article{knight_squibs_1999,
	title = {Squibs and {Discussions} {Decoding} {Complexity} in {Word}-{Replacement} {Translation} {Models}},
	journal = {English},
	author = {Knight, Kevin},
	year = {1999},
}

@article{gimpel_structured_2012,
	title = {Structured {Ramp} {Loss} {Minimization} for {Machine} {Translation}},
	abstract = {This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. Instead, most have im-plicit connections to particular forms of ramp loss. We propose to minimize ramp loss di-rectly and present a training algorithm that is easy to implement and that performs compa-rably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches.},
	journal = {Naacl-2012},
	author = {Gimpel, Kevin and Smith, Noah A.},
	year = {2012},
	note = {ISBN: 978-1-937284-20-6},
	pages = {221--231},
}

@article{kalchbrenner_convolutional_2014,
	title = {A {Convolutional} {Neural} {Network} for {Modelling} {Sentences}},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, \{ACL\} 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers},
	author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	year = {2014},
	note = {arXiv: 1404.2188v1
ISBN: 9781937284725},
	pages = {655--665},
}

@article{galley_scalable_2006,
	title = {Scalable {Inference} and {Training} of {Context}-{Rich} {Syntactic} {Translation} {Models} º - ¥ ì  * {X} Õ ý},
	number = {July},
	journal = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
	author = {Galley, Michel and Graehl, Jonathan and Knight, Kevin and Marcu, Daniel and Deneefe, Steve and Wang, Wei and Thayer, Ignacio and York, New and Rey, Marina and Rey, Marina},
	year = {2006},
	keywords = {Machine Translation, string-to-tree},
	pages = {961--968},
}

@article{recasens_linguistic_2013,
	title = {Linguistic {Models} for {Analyzing} and {Detecting} {Biased} {Language}},
	abstract = {Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiq- uitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits de- signed to remove bias from Wikipedia ar- ticles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and episte- mological bias, related to whether propo- sitions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- sifiers. These insights help us develop fea- tures for a model to solve a new prediction task of practical importance: given a bi- ased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.},
	journal = {Proceedings of the 51st Annual Meeting on Association for Computational Linguistics},
	author = {Recasens, Marta and Danescu-Niculescu-Mizil, Cristian and Jurafsky, Dan},
	year = {2013},
	note = {ISBN: 9781937284503},
	pages = {1650--1659},
}

@article{smith_minimum_2006,
	title = {Minimum {Risk} {Annealing} for {Training} {Log}-{Linear} {Models}},
	doi = {10.3115/1273073.1273174},
	abstract = {When training the parameters for a natural language system,{\textbackslash}none would prefer to minimize 1-best loss (error) on an evaluation{\textbackslash}nset. Since the error surface for many natural language{\textbackslash}nproblems is piecewise constant and riddled with local minima,{\textbackslash}nmany systems instead optimize log-likelihood, which is{\textbackslash}nconveniently differentiable and convex. We propose training{\textbackslash}ninstead to minimize the expected loss, or risk. We define this{\textbackslash}nexpectation using a probability distribution over hypotheses{\textbackslash}nthat we gradually sharpen (anneal) to focus on the 1-best hypothesis.{\textbackslash}nBesides the linear loss functions used in previous{\textbackslash}nwork, we also describe techniques for optimizing nonlinear{\textbackslash}nfunctions such as precision or the BLEU metric. We present{\textbackslash}nexperiments training log-linear combinations of models for{\textbackslash}ndependency parsing and for machine translation. In machine{\textbackslash}ntranslation, annealed minimum risk training achieves significant{\textbackslash}nimprovements in BLEU over standard minimum error{\textbackslash}ntraining. We also show improvements in labeled dependency{\textbackslash}nparsing.},
	number = {July},
	journal = {Acl-2006},
	author = {Smith, David A. and Eisner, Jason},
	year = {2006},
	note = {ISBN: 0001401106},
	pages = {787--794},
}

@article{dyer_simple_2013,
	title = {A simple, fast, and effective parameterization of \{{IBM}\} model 2},
	number = {June},
	journal = {Naacl},
	author = {Dyer, Chris and Chahuneau, Victor and Smith, Noah},
	year = {2013},
	pages = {644--648},
}

@article{koehn_open_2006,
	title = {Open {Source} {Toolkit} for {Statistical} {Machine} {Translation}},
	doi = {10.3115/1557769.1557821},
	abstract = {We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.},
	number = {June},
	journal = {Proceedings of ACL},
	author = {Koehn, Philipp and Shen, Wade and Federico, Marcello and Bertoldi, Nicola and Callison-Burch, Chris and Cowan, Brooke and Dyer, Chris and Hoang, Hieu and Bojar, Ondrej and Zens, Richard and Constantin, Alexandra and Herbst, Evan and Moran, Christine},
	year = {2006},
	pages = {177--180},
}

@misc{fainsilber_metaphorical_nodate,
	title = {Metaphorical {Uses} of {Language} in the {Expression} of {Emotions}},
	author = {Fainsilber, Lynn and Ortony, Andrew},
}

@article{wen_extracting_2013,
	title = {Extracting {Events} with {Informal} {Temporal} {References} in {Personal} {Histories} in {Online} {Communities}},
	journal = {ACL short},
	author = {Wen, Miaomiao and Zheng, Zeyu and Jang, Hyeju and Xiang, Guang and Penstein Rosé, Carolyn},
	year = {2013},
	note = {ISBN: 9781937284510},
	keywords = {carolyn penstein ros, guang xiang, histories in online communities, hyeju jang, miaomiao wen, racting events with informal, temporal references in personal, zeyu zheng},
	pages = {836--842},
}

@article{petrenz_stable_2011,
	title = {Stable {Classification} of {Text} {Genres}},
	volume = {37},
	issn = {0891-2017},
	doi = {10.1162/COLI_a_00052},
	abstract = {Every text has at least one topic and at least one genre. Evidence for a text's topic and genre comes, in part, from its lexical and syntactic features-features used in both Automatic Topic Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should be stable in the face of changes in topic distribution, we assess five previously published AGC methods with respect to both performance on the same topic-genre distribution on which they were trained and stability of that performance across changes in topic-genre distribution. Our experiments lead us to conclude that (1) stability in the face of changing topical distributions should be added to the evaluation critera for new approaches to AGC, and (2) Part-of-Speech features should be considered individually when developing a high-performing, stable AGC system for a particular, possibly changing corpus.},
	number = {July 2010},
	journal = {Computational Linguistics},
	author = {Petrenz, Philipp and Webber, Bonnie},
	year = {2011},
	note = {ISBN: 0891-2017},
	keywords = {genres, stable classification of text},
	pages = {385--393},
}

@article{nguyen_tea_2015,
	title = {Tea {Party} in the {House}: {A} {Hierarchical} {Ideal} {Point} {Topic} {Model} and {Its} {Application} to {Republican} {Legislators} in the 112th {Congress}},
	url = {http://www.aclweb.org/anthology/P15-1139},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Nguyen, Viet-An and Boyd-Graber, Jordan and Resnik, Philip and Miler, Kristina},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1438--1448},
}

@article{arun_monte_2009,
	title = {Monte {Carlo} inference and maximization for phrase-based translation},
	volume = {23},
	url = {http://www.aclweb.org/anthology/W09-1114},
	doi = {10.3115/1596374.1596394},
	abstract = {Recent advances in statistical machine translation have used beam search for approximate NP-complete inference within probabilistic translation models. We present an alternative approach of sampling from the posterior distribution defined by a translation model. We define a novel Gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution. In doing so we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as finding the maximum probability translation and minimum expected risk training and decoding.},
	number = {4},
	journal = {Computational Linguistics},
	author = {Arun, Abhishek and Blunsom, Phil and Dyer, Chris and Lopez, Adam and Haddow, Barry and Koehn, Philipp},
	year = {2009},
	note = {ISBN: 978-1-932432-29-9},
	pages = {102 -- 110},
}

@book{garot_where_2007,
	title = {"{Where} {You} {From}!": {Gang} {Identity} as {Performance}},
	volume = {36},
	isbn = {0891241606287},
	url = {http://jce.sagepub.com/cgi/doi/10.1177/0891241606287364},
	abstract = {This article investigates how young people in an inner-city ecology invoke the relevance of gangs by demanding, "Where you from!" Such a challenge creates a lively venue for performing identity and emotional manipulation for both the instigator who offers the challenge and the respondent. Rather than conceptualizing young people as gang members and gangs as a static group, this analysis shows how the doing of gangs is strategic and context sensitive. Such an approach provides an alternative to conceptualizing identity, and especially gang identity, not as a fixed personal characteristic but as a sensual response to a moment's vicissitudes.},
	author = {Garot, R.},
	year = {2007},
	doi = {10.1177/0891241606287364},
	note = {Publication Title: Journal of Contemporary Ethnography
Issue: 1
ISSN: 0891-2416},
}

@article{adams_gang_1997,
	title = {Gang graffitti as a discourse genre},
	volume = {1},
	number = {3},
	journal = {Journal of Sociolinguistics},
	author = {Adams, Karen L. and Winter, Anne},
	year = {1997},
	keywords = {antilanguage, argumenta-, discourse genre, gang graffiti},
	pages = {337--360},
}

@article{filippova_overcoming_2013,
	title = {Overcoming the {Lack} of {Parallel} {Data} in {Sentence} {Compression}},
	number = {October},
	journal = {Emnlp},
	author = {Filippova, Katja and Altun, Yasemin},
	year = {2013},
	note = {ISBN: 9781937284978},
	pages = {1481--1491},
}

@article{barzilay_learning_2003,
	title = {Learning to {Paraphrase}: {An} {Unsupervised} {Approach} {Using} {Multiple}-{Sequence} {Alignment}},
	url = {http://arxiv.org/abs/cs/0304006},
	doi = {10.3115/1073445.1073448},
	abstract = {We address the text-to-text generation problem of sentence-level paraphrasing -- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.},
	number = {June},
	journal = {Naacl-2003},
	author = {Barzilay, Regina and Lee, Lillian},
	year = {2003},
	note = {arXiv: cs/0304006},
	pages = {16--23},
}

@article{che_sentence_2015,
	title = {Sentence {Compression} for {Aspect}-{Based} {Sentiment} {Analysis}},
	volume = {23},
	issn = {23299290},
	doi = {10.1109/TASLP.2015.2443982},
	abstract = {Sentiment analysis, which addresses the computational treatment of opinion, sentiment, and subjectivity in text, has received considerable attention in recent years. In contrast to the traditional coarse-grained sentiment analysis tasks, such as document-level sentiment classification, we are interested in the fine-grained aspect-based sentiment analysis that aims to identify aspects that users comment on and these aspects' polarities. Aspect-based sentiment analysis relies heavily on syntactic features. However, the reviews that this task focuses on are natural and spontaneous, thus posing a challenge to syntactic parsers. In this paper, we address this problem by proposing a framework of adding a sentiment sentence compression (Sent-Comp) step before performing the aspect-based sentiment analysis. Different from the previous sentence compression model for common news sentences, Sent-Comp seeks to remove the sentiment-unnecessary information for sentiment analysis, thereby compressing a complicated sentiment sentence into one that is shorter and easier to parse. We apply a discriminative conditional random field model, with certain special features, to automatically compress sentiment sentences. Using the Chinese corpora of four product domains, Sent-Comp significantly improves the performance of the aspect-based sentiment analysis. The features proposed for Sent-Comp, especially the potential semantic features, are useful for sentiment sentence compression.},
	number = {12},
	journal = {IEEE/ACM Transactions on Speech and Language Processing},
	author = {Che, Wanxiang and Zhao, Yanyan and Guo, Honglei and Su, Zhong and Liu, Ting},
	year = {2015},
	keywords = {Aspect-based sentiment analysis, potential semantic features, sentence compression, sentiment analysis},
	pages = {2111--2124},
}

@article{filippova_fast_2015,
	title = {Fast k-best {Sentence} {Compression}},
	url = {http://arxiv.org/abs/1510.08418},
	abstract = {A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.},
	author = {Filippova, Katja and Alfonseca, Enrique},
	year = {2015},
	note = {arXiv: 1510.08418},
}

@article{filippova_sentence_2015,
	title = {Sentence {Compression} by {Deletion} with {LSTMs}},
	volume = {利用lstm的sen},
	abstract = {利用lstm的sentence to sentence 框架进行句子压缩},
	number = {September},
	journal = {Emnlp},
	author = {Filippova, Katja and Alfonseca, Enrique and Colmenares, Carlos A and Kaiser, Lukasz and Vinyals, Oriol},
	year = {2015},
	note = {ISBN: 9781941643327},
	pages = {360--368},
}

@article{mcdonald_discriminative_2006,
	title = {Discriminative {Sentence} {Compression} with {Soft} {Syntactic} {Evidence}},
	abstract = {We present a model for sentence compression that uses a discriminative large margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.},
	journal = {Eacl},
	author = {McDonald, Ryan T},
	year = {2006},
	note = {ISBN: 1-932432-59-0},
	keywords = {summarization},
	pages = {297--304},
}

@article{chaudhuri_leveraging_2009,
	title = {Leveraging {Structural} {Relations} for {Fluent} {Compressions} at {Multiple} {Compression} {Rates}},
	url = {http://www.aclweb.org/anthology/P/P09/P09-2026},
	number = {August},
	journal = {Proceedings of the ACL-IJCNLP 2009 Conference Short Papers},
	author = {Chaudhuri, Sourish and Gupta, Naman K and Smith, Noah A and Rose, Carolyn P},
	year = {2009},
	note = {ISBN: 9781617382581},
	pages = {101--104},
}

@article{filippova_multi-sentence_2010,
	title = {Multi-{Sentence} {Compression}: {Finding} {Shortest} {Paths} in {Word} {Graphs}},
	abstract = {We consider the task of summarizing a cluster of related sentences with a short sentence which we call multi-sentence compression and present a simple ap- proach based on shortest paths in word graphs. The advantage and the novelty of the proposed method is that it is syntax- lean and requires little more than a tok- enizer and a tagger. Despite its simplic- ity, it is capable of generating grammati- cal and informative summaries as our ex- periments with English and Spanish data demonstrate.},
	number = {August},
	journal = {Proceedings of the 23rd International Conference on Computational Linguistics (COLING10)},
	author = {Filippova, Katja},
	year = {2010},
	pages = {322--330},
}

@article{gupta_evaluating_2009,
	title = {Evaluating the {Syntactic} {Transformations} in {Gold} {Standard} {Corpora} for {Statistical} {Sentence} {Compression}},
	number = {June},
	journal = {Naacl2009},
	author = {Gupta, Naman K and Chaudhuri, Sourish and Rosé, Carolyn P},
	year = {2009},
	pages = {145--148},
}

@article{bannard_paraphrasing_2005,
	title = {Paraphrasing with bilingual parallel corpora},
	url = {http://portal.acm.org/citation.cfm?id=1219914&amp;dl=GUIDE,},
	doi = {10.3115/1219840.1219914},
	abstract = {Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.},
	number = {June},
	journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
	author = {Bannard, Collin and Callison-Burch, Chris},
	year = {2005},
	note = {ISBN: 1932432515},
	pages = {597--604},
}

@article{xu_paraphrasing_2012,
	title = {Paraphrasing for {Style}},
	volume = {4},
	number = {December},
	journal = {Coling-2012},
	author = {Xu, Wei and Ritter, Alan and Dolan, William B. and Grishman, Ralph and Cherry, Colin},
	year = {2012},
	pages = {2899--2914},
}

@article{ganitkevitch_ppdb_2013,
	title = {{PPDB} : {The} {Paraphrase} {Database}},
	abstract = {We present the 1.0 release of our para- phrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million para- phrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which cap- ture many meaning-preserving syntactic trans- formations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similar- ity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our re- lease includes pruning tools that allowusers to determine their own precision/recall tradeoff.},
	number = {June},
	journal = {Proceedings of NAACL-HLT},
	author = {Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
	year = {2013},
	note = {ISBN: 9781937284473},
	pages = {758--764},
}

@article{jang_effects_2015,
	title = {Effects of {Situational} {Factors} on {Metaphor} {Detection} in an {Online} {Discussion} {Forum}},
	author = {Jang, Hyeju and Wen, Miaomiao},
	year = {2015},
	pages = {1--10},
}

@article{Tausczik2010,
	title = {The {Psychological} {Meaning} of {Words}: {LIWC} and {Computerized} {Text} {Analysis} {Methods}},
	volume = {29},
	issn = {0261-927X},
	doi = {10.1177/0261927X09351676},
	abstract = {We are in the midst of a technological revolution whereby, for the first time, researchers can link daily word use to a broad array of real-world behaviors. This article reviews several computerized text analysis methods and describes how Linguistic Inquiry and Word Count (LIWC) was created and validated. LIWC is a transparent text analysis program that counts words in psychologically meaningful categories. Empirical results using LIWC demonstrate its ability to detect meaning in a wide variety of experimental settings, including to show attentional focus, emotionality, social relationships, thinking styles, and individual differences.},
	number = {1},
	journal = {Journal of Language and Social Psychology},
	author = {Tausczik, Y. R. and Pennebaker, J. W.},
	year = {2010},
	note = {ISBN: 0261-927X},
	keywords = {attention, computerized text analysis, deception, dominance, liwc, relationships},
	pages = {24--54},
}

@article{jang_metaphor_2015,
	title = {Metaphor {Detection} in {Discourse}},
	number = {September},
	journal = {Sigdd2015},
	author = {Jang, Hyeju and Moon, Seunghwan and Jo, Yohan and Ros, Carolyn Penstein},
	year = {2015},
	pages = {384--392},
}

@article{collins_clause_2005,
	title = {Clause restructuring for statistical machine translation},
	url = {http://dx.doi.org/10.3115/1219840.1219906},
	doi = {10.3115/1219840.1219906},
	abstract = {We describe a method for incorporating syntactic information in statistical{\textbackslash}nmachine translation systems. The first step of the method is to parse{\textbackslash}nthe source language string that is being translated. The second step{\textbackslash}nis to apply a series of transformations to the parse tree, effectively{\textbackslash}nreordering the surface string on the source language side of the{\textbackslash}ntranslation system. The goal of this step is to recover an underlying{\textbackslash}nword order that is closer to the target language word-order than{\textbackslash}nthe original string. The reordering approach is applied as a pre-processing{\textbackslash}nstep in both the training and decoding phases of a phrase-based statistical{\textbackslash}nMT system. We describe experiments on translation from German to{\textbackslash}nEnglish, showing an improvement from 25.2\% Bleu score for a baseline{\textbackslash}nsystem to 26.8\% Bleu score for the system with reordering, a statistically{\textbackslash}nsignificant improvement.},
	number = {June},
	journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
	author = {Collins, Michael and Koehn, Philipp and Kučerová, Ivona},
	year = {2005},
	note = {ISBN: 1932432515},
	keywords = {compling, machinetranslation, syntax},
	pages = {531--540},
}

@article{jang_conversational_2014,
	title = {Conversational {Metaphors} in {Use}: {Exploring} the {Contrast} between {Technical} and {Everyday} {Notions} of {Metaphor}},
	url = {http://www.aclweb.org/anthology/W/W14/W14-2301},
	number = {June},
	journal = {Proceedings of the Second Workshop on Metaphor in NLP},
	author = {Jang, Hyeju and Piergallini, Mario and Wen, Miaomiao and Rose, Carolyn},
	year = {2014},
	pages = {1--10},
}

@article{veale_creating_2013,
	title = {Creating similarity: {Lateral} thinking for vertical similarity judgments},
	volume = {1},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84901838293&partnerID=tZOtx3y1},
	abstract = {Just as observing is more than just seeing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguistic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views complement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, generative similarity measure for WordNet. © 2013 Association for Computational Linguistics.},
	journal = {ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {Veale, Tony and Li, Guofu},
	year = {2013},
	note = {ISBN: 9781937284503},
	pages = {660--670},
}

@article{joty_s._codra:_2015,
	title = {{CODRA}: {A} novel discriminative framework for rhetorical analysis},
	volume = {41,3,,385,},
	number = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84941276880\&partnerID=40\&md5=27d849a2af6353acc038bd658cac4359},
	journal = {Computational Linguistics},
	author = {Joty S., Carenini G Ng R T},
	year = {2015},
	pages = {1--50},
}

@article{goldberg_living_2008,
	title = {Living and dormant collective memories as contexts of history learning},
	volume = {18},
	issn = {09594752},
	doi = {10.1016/j.learninstruc.2007.04.005},
	abstract = {This article investigates the effect of the vitality of historical issues in collective memory on students' history learning processes and products. Forty 12th grade students of different ethnic background participated in two historical problem-solving learning tasks. The historical issues were found to differ in their vitality in collective memory as signified by students' consensus, certainty, and reference to the present. Findings showed effects of issue vitality on narrative and argumentative change, and on the relation of historical source evaluation with narrative change. An interaction was found between issue vitality and ethnicity in the source evaluation: more vital collective memory narratives were more resistant to change and more prone to ethnic identity bias. ?? 2007 Elsevier Ltd. All rights reserved.},
	number = {3},
	journal = {Learning and Instruction},
	author = {Goldberg, Tsafrir and Schwarz, Baruch B. and Porat, Dan},
	year = {2008},
	note = {ISBN: 09594752 (ISSN)},
	keywords = {Argumentation, Collective memory, Historical thinking, History learning},
	pages = {223--237},
}

@article{al_khatib_automatic_2012,
	title = {Automatic {Detection} of {Point} of {View} {Differences} in {Wikipedia}},
	volume = {2},
	number = {December},
	journal = {Coling-2012},
	author = {Al Khatib, Khalid and Schuetze, Hinrich and Kantner, Cathleen},
	year = {2012},
	pages = {33--50},
}

@article{feng_linear-time_2014,
	title = {A {Linear}-{Time} {Bottom}-{Up} {Discourse} {Parser} with {Constraints} and {Post}-{Editing}},
	abstract = {Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73\%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be ap- plied in practice. In this work, we develop a much faster model whose time complex- ity is linear in the number of sentences. Our model adopts a greedy bottom-up ap- proach, with two linear-chain CRFs ap- plied in cascade as local classifiers. To en- hance the accuracy of the pipeline, we add additional constraints in theViterbi decod- ing of the first CRF. In addition to effi- ciency, our parser also significantly out- performs the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy. 1},
	journal = {Acl},
	author = {Feng, Vanessa Wei and Hirst, Graeme},
	year = {2014},
	note = {ISBN: 9781937284725},
	pages = {511--521},
}

@article{surdeanu_two_2015,
	title = {Two {Practical} {Rhetorical} {Structure} {Theory} {Parsers}},
	journal = {Naacl2015},
	author = {Surdeanu, Mihai and Hicks, Thomas and Valenzuela-esc, Marco A},
	year = {2015},
	pages = {1--5},
}

@article{papineni_bleu:_2002,
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	issn = {00134686},
	url = {http://dl.acm.org/citation.cfm?id=1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	number = {July},
	journal = {… of the 40Th Annual Meeting on …},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
	year = {2002},
	note = {ISBN: 1-55860-883-4},
	pages = {311--318},
}

@article{shutova_statistical_2013,
	title = {Statistical {Metaphor} {Processing}},
	volume = {39},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00124},
	doi = {10.1162/COLI_a_00124},
	abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
	number = {2},
	journal = {Computational Linguistics},
	author = {Shutova, Ekaterina and Teufel, Simone and Korhonen, Anna},
	month = jun,
	year = {2013},
	note = {ISBN: 9781608459858},
	pages = {301--353},
}

@article{suh_us_2007,
	title = {Us vs. {Them}: {Understanding} social dynamics in wikipedia with revert graph visualizations},
	doi = {10.1109/VAST.2007.4389010},
	abstract = {Wikipedia is a wiki-based encyclopedia that has become one of the most popular collaborative on-line knowledge systems. As in any large collaborative system, as Wikipedia has grown, conflicts and coordination costs have increased dramatically. Visual analytic tools provide a mechanism for addressing these issues by enabling users to more quickly and effectively make sense of the status of a collaborative environment. In this paper we describe a model for identifying patterns of conflicts in Wikipedia articles. The model relies on users' editing history and the relationships between user edits, especially revisions that void previous edits, known as "reverts". Based on this model, we constructed Revert Graph, a tool that visualizes the overall conflict patterns between groups of users. It enables visual analysis of opinion groups and rapid interactive exploration of those relationships via detail drill- downs. We present user patterns and case studies that show the effectiveness of these techniques, and discuss how they could generalize to other systems.},
	journal = {VAST IEEE Symposium on Visual Analytics Science and Technology 2007, Proceedings},
	author = {Suh, Bongwon and Chi, Ed H. and Pendleton, Bryan A. and Kittur, Aniket},
	year = {2007},
	note = {ISBN: 9781424416592},
	keywords = {Collaboration, Graph, Revert, User model, Visualization, Wiki, Wikipedia},
	pages = {163--170},
}

@article{Tsur2015,
	title = {A {Frame} of {Mind}: {Using} {Statistical} {Models} for {Detection} of {Framing} and {Agenda} {Setting} {Campaigns}},
	abstract = {Framing is a sophisticated form of dis- course in which the speaker tries to in- duce a cognitive bias through consis- tent linkage between a topic and a spe- cific context (frame). We build on po- litical science and communication theory and use probabilistic topic models com- bined with time series regression analy- sis (autoregressive distributed-lag models) to gain insights about the language dy- namics in the political processes. Pro- cessing four years of public statements is- sued by members of the U.S. Congress, our results provide a glimpse into the com- plex dynamic processes of framing, atten- tion shifts and agenda setting, commonly known as ‘spin’. We further provide new evidence for the divergence in party disci- pline in U.S. politics},
	journal = {ACL},
	author = {Tsur, Oren and Calacci, Dan and Lazer, David},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1629--1638},
}

@article{jain_unsupervised_2012,
	title = {An {Unsupervised} {Dynamic} {Bayesian} {Network} {Approach} to {Measuring} {Speech} {Style} {Accommodation}},
	journal = {Eacl},
	author = {Jain, Mahaveer and McDonough, John W and Gweon, Gahgene and Raj, Bhiksha and Rosé, Carolyn Penstein},
	year = {2012},
	pages = {787--797},
}

@article{prabhakaran_who_2013,
	title = {Who {Had} the {Upper} {Hand} ? {Ranking} {Participants} of {Interactions} {Based} on {Their} {Relative} {Power}},
	number = {October},
	journal = {Ijcnlp2013},
	author = {Prabhakaran, Vinodkumar and John, Ajita},
	year = {2013},
	pages = {365--373},
}

@article{Danescu-Niculescu-Mizil2012,
	title = {Echoes of power: {Language} effects and power differences in social interaction},
	url = {http://dl.acm.org/citation.cfm?id=2187931\nhttp://arxiv.org/abs/1112.3670\nhttp://dl.acm.org/citation.cfm?doid=2187836.2187931},
	doi = {10.1145/2187836.2187931},
	abstract = {Understanding social interaction within groups is key to analyzing online communities. Most current work focuses on structural properties: who talks to whom, and how such interactions form larger network structures. The interactions themselves, however, generally take place in the form of natural language --- either spoken or written --- and one could reasonably suppose that signals manifested in language might also provide information about roles, status, and other aspects of the group's dynamics. To date, however, finding such domain-independent language-based signals has been a challenge. Here, we show that in group discussions power differentials between participants are subtly revealed by how much one individual immediately echoes the linguistic style of the person they are responding to. Starting from this observation, we propose an analysis framework based on linguistic coordination that can be used to shed light on power relationships and that works consistently across multiple types of power --- including a more "static" form of power based on status differences, and a more "situational" form of power in which one individual experiences a type of dependence on another. Using this framework, we study how conversational behavior can reveal power relationships in two very different settings: discussions among Wikipedians and arguments before the U.S. Supreme Court.},
	journal = {Proceedings of the 21st international conference on World Wide Web - WWW '12},
	author = {Danescu-Niculescu-Mizil, Cristian and Lee, Lillian and Pang, Bo and Kleinberg, Jon},
	year = {2012},
	note = {arXiv: 1112.3670
ISBN: 9781450312295},
	keywords = {accommodation, coordination, dependence, has been a long-acknowledged, language, linguistic, linguistic convergence, missing, munities, online com-, power, relations, social status, style, tent of these interactions},
	pages = {699},
}

@article{chen_discovering_2013,
	title = {Discovering coherent topics using general knowledge},
	url = {http://dl.acm.org/citation.cfm?doid=2505515.2505519},
	doi = {10.1145/2505515.2505519},
	abstract = {Topic models have been widely used to discover latent topics in text documents. However, they may produce topics that are not interpretable for an application. Researchers have proposed to incorporate prior domain knowledge into topic models to help produce coherent topics. The knowledge used in existing models is typically domain dependent and assumed to be correct. However, one key weakness of this knowledge-based approach is that it requires the user to know the domain very well and to be able to provide knowledge suitable for the domain, which is not always the case because in most real-life applications, the user wants to find what they do not know. In this paper, we propose a framework to leverage the general knowledge in topic models. Such knowledge is domain independent. Specifically, we use one form of general knowledge, i.e., lexical semantic relations of words such as synonyms, antonyms and adjective attributes, to help produce more coherent topics. However, there is a major obstacle, i.e., a word can have multiple meanings/senses and each meaning often has a different set of synonyms and antonyms. Not every meaning is suitable or correct for a domain. Wrong knowledge can result in poor quality topics. To deal with wrong knowledge, we propose a new model, called GK-LDA, which is able to effectively exploit the knowledge of lexical relations in dictionaries. To the best of our knowledge, GK-LDA is the first such model that can incorporate the domain independent knowledge. Our experiments using online product reviews show that GK-LDA performs significantly better than existing state-of-the-art models.},
	journal = {Proc. of the 22nd ACM international conference on Conference on information \& knowledge management, CIKM '13},
	author = {Chen, Zhiyuan and Mukherjee, Arjun and Liu, Bing and Hsu, Meichun and Castellanos, Malu and Ghosh, Riddhiman},
	year = {2013},
	note = {ISBN: 9781450322638},
	keywords = {15, 5, a powerful framework for, and lda, extracting latent topics in, general knowledge, lexical relations, provide, statistical topic models, such as plsa, text docu-, topic models},
	pages = {209--218},
}

@article{zhai_constrained_2011,
	title = {Constrained {LDA} for grouping product features in opinion mining},
	volume = {6634 LNAI},
	issn = {03029743},
	doi = {10.1007/978-3-642-20841-6-37},
	abstract = {In opinion mining of product reviews, one often wants to produce a summary of opinions based on product features. However, for the same feature, people can express it with different words and phrases. To produce an effective summary, these words and phrases, which are domain synonyms, need to be grouped under the same feature. Topic modeling is a suitable method for the task. However, instead of simply letting topic modeling find groupings freely, we believe it is possible to do better by giving it some pre-existing knowledge in the form of automatically extracted constraints. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), with the ability to process large scale constraints. Then, two novel methods are proposed to extract two types of constraints automatically. Finally, the resulting constrained-LDA and the extracted constraints are applied to group product features. Experiments show that constrained-LDA outperforms the original LDA and the latest mLSA by a large margin.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Zhai, Zhongwu and Liu, Bing and Xu, Hua and Jia, Peifa},
	year = {2011},
	note = {ISBN: 9783642208409},
	keywords = {Constrained LDA, Feature Grouping, Opinion Mining},
	pages = {448--459},
}

@article{snover_study_2006,
	title = {A {Study} of {Translation} {Edit} {Rate} with {Targeted} {Human} {Annotation}},
	issn = {1041-1135},
	doi = {10.1.1.129.4369},
	abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEUeven when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well asor better thana second human judgment does.},
	journal = {Proceedings of Association for Machine Translation in the Americas},
	author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
	year = {2006},
	pages = {223--231},
}

@article{bak_self-disclosure_2014,
	title = {Self-disclosure topic model for {Twitter} conversations},
	url = {http://www.aclweb.org/anthology/W/W14/W14-2706},
	journal = {Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media},
	author = {Bak, JinYeong and Lin, Chin-Yew and Oh, Alice},
	year = {2014},
	pages = {42--49},
}

@article{yuret_noisy_2010,
	title = {The noisy channel model for unsupervised word sense disambiguation},
	volume = {36},
	number = {October 2008},
	journal = {Computational Linguistics},
	author = {Yuret, Deniz and Yatbaz, Mehmet},
	year = {2010},
	pages = {111--127},
}

@article{kernighan_spelling_1990,
	title = {A {Spelling} {Correction} {Program} {Based} on a {Noisy} {Channel} {Model}},
	author = {Kernighan, Mark and Church, Kenneth W and Gale, William A},
	year = {1990},
	pages = {205--210},
}

@article{jagarlamudi_incorporating_2012,
	title = {Incorporating {Lexical} {Priors} into {Topic} {Models}},
	volume = {-},
	abstract = {We propose a simple and effective way to guide topic models to learn topics of specific interest to a user.},
	journal = {EACL},
	author = {Jagarlamudi, Jagadeesh and Daum, Hal},
	year = {2012},
	note = {ISBN: 978-1-937284-19-0},
	pages = {204--213},
}

@article{al-badrashiny_automatic_2014,
	title = {Automatic {Transliteration} of {Romanized} {Dialectal} {Arabic}},
	journal = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
	author = {Al-Badrashiny, Mohamed and Eskander, Ramy and Habash, Nizar and Rambow, Owen},
	year = {2014},
	pages = {30--38},
}

@article{Danescu-Niculescu-Mizil2013,
	title = {A computational approach to politeness with application to social factors},
	abstract = {We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.},
	journal = {The 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)},
	author = {Danescu-Niculescu-Mizil, Cristian and Sudhof, Moritz and Jurafsky, Dan and Leskovec, Jure and Potts, Christopher},
	year = {2013},
}

@article{paul_cross-cultural_2009,
	title = {Cross-{Cultural} {Analysis} of {Blogs} and {Forums} with {Mixed}-{Collection} {Topic} {Models}},
	url = {http://dl.acm.org/citation.cfm?id=1699687},
	doi = {10.3115/1699648.1699687},
	abstract = {Dans notre monde actuel, les interactions interculturelles sont fréquentes et dépendent fortement du langage. Or les ressources sur le Web telles que les forums et les blogs contiennent de riches informations sur le langage et la culture. Cet article propose un modèle thématique permettant d'effectuer une analyse inter-culturelle de ces ressources. Ce modèle thématique, nommé ccLDA (cross-collection LDA), suppose que chaque mot d'un document provient soit d'un modèle de langue général, soit d'un modèle de langue spécifique à la collection de laquelle le document est issu. L'évaluation est basée sur deux collections : la première contient des posts provenant d'un forum de touristes vis-à-vis de trois destinations (Grande Bretagne, Inde, Singapour) et la seconde contient des blogs de locaux de ces trois mêmes pays. Une analyse qualitative des deux collections montrent que les thèmes découverts sont cohérents et révèlent bien les spécificités de chaque culture ; une analyse quantitative mesurant la vraisemblance des modèles révèlent la plus grand cohérence de ccLDA comparé aux références ccMix et LDA.},
	number = {August},
	journal = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
	author = {Paul, Michael J. and Girju, Roxana},
	year = {2009},
	note = {ISBN: 978-1-932432-63-3},
	pages = {1408--1417},
}

@article{kocisky_learning_2014,
	title = {Learning {Bilingual} {Word} {Representations} by {Marginalizing} {Alignments}},
	url = {http://www.aclweb.org/anthology/P/P14/P14-2037},
	abstract = {We present a probabilistic model that si-multaneously learns alignments and dis-tributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic con-text than prior work relying on hard align-ments. The advantage of this approach is demonstrated in a cross-lingual classifica-tion task, where we outperform the prior published state of the art.},
	number = {1993},
	journal = {Acl-2014},
	author = {Kocisky, Tomas and Hermann, Karl Moritz and Blunsom, Phil},
	year = {2014},
	note = {arXiv: 1405.0947v1
ISBN: 9781937284732},
	pages = {224--229},
}

@article{schoenemann_computing_2010,
	title = {Computing optimal alignments for the {IBM}-3 translation model},
	url = {http://dl.acm.org/citation.cfm?id=1870581},
	number = {July},
	journal = {Proceedings of the Fourteenth Conference on …},
	author = {Schoenemann, Thomas},
	year = {2010},
	note = {ISBN: 9781932432831},
	pages = {98--106},
}

@article{paul_two-dimensional_2010,
	title = {A two-dimensional {Topic}-{Aspect} {Model} for discovering multi-faceted topics},
	volume = {1},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77958531129&partnerID=40&md5=34abc23f6dc1614493d1421b6d07f7af},
	abstract = {This paper presents the Topic-Aspect Model (TAM), a Bayesian mixture model which jointly discovers topics and aspects. We broadly define an aspect of a document as a characteristic that spans the document, such as an underlying theme or perspective. Unlike previous models which cluster words by topic or aspect, our model can generate token assignments in both of these dimensions, rather than assuming words come from only one of two orthogonal models. We present two applications of the model. First, we model a corpus of computational linguistics abstracts, and find that the scientific topics identified in the data tend to include both a computational aspect and a linguistic aspect. For example, the computational aspect of GRAMMAR emphasizes parsing, whereas the linguistic aspect focuses on formal languages. Secondly, we show that the model can capture different viewpoints on a variety of topics in a corpus of editorials about the Israeli-Palestinian conflict. We show both qualitative and quantitative improvements in TAM over two other state-of-the-art topic models.Copyright © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	journal = {Proceedings of the National Conference on Artificial Intelligence},
	author = {Paul, M and Girju, R},
	year = {2010},
	note = {ISBN: 9781577354642},
	keywords = {Artificial intelligence, Aspect model, Bayesian mixture model, Computational aspects, Computational linguistics, Formal languages},
	pages = {545--550},
}

@inproceedings{toutanova_extentions_2002,
	title = {Extentions to {HMM}-based {Statistical} {Word} {Alignment} {Models}},
	booktitle = {Proceedings of the 2002 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Toutanova, Kristina and Ilhan, Tolga and Manning, Christopher},
	year = {2002},
	note = {Issue: July},
	pages = {87--94},
}

@article{doreian_partitioning_2009,
	title = {Partitioning signed social networks},
	volume = {31},
	issn = {03788733},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378873308000397},
	doi = {10.1016/j.socnet.2008.08.001},
	number = {1},
	journal = {Social Networks},
	author = {Doreian, Patrick and Mrvar, Andrej},
	year = {2009},
	pages = {1--11},
}

@article{mikolov_recurrent_2010,
	title = {Recurrent {Neural} {Network} based {Language} {Model}},
	abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
	number = {September},
	journal = {Interspeech},
	author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
	year = {2010},
	pages = {1045--1048},
}

@article{kittur_he_2007,
	title = {He says, she says: conflict and coordination in {Wikipedia}},
	issn = {15237052},
	url = {http://portal.acm.org/citation.cfm?id=1240624.1240698},
	doi = {10.1145/1240624.1240698},
	abstract = {Wikipedia, a wiki-based encyclopedia, has become one of the most successful experiments in collaborative knowledge building on the Internet. As Wikipedia continues to grow, the potential for conflict and the need for coordination increase as well. This article examines the growth of such non-direct work and describes the development of tools to characterize conflict and coordination costs in Wikipedia. The results may inform the design of new collaborative knowledge systems.},
	journal = {ACM Conference on Human Factors in Computing Systems},
	author = {Kittur, Aniket and Suh, Bongwon and Pendleton, Bryan A. and Chi, Ed H.},
	year = {2007},
	pmid = {20586442},
	note = {ISBN: 9781595935939},
	keywords = {Wiki, Wikipedia, collaboration, conflict, user model, visualization, web-based interaction},
	pages = {453 -- 462},
}

@article{brown_statistical_1990,
	title = {A statistical approach to machine translation},
	volume = {16},
	issn = {08912017},
	url = {http://portal.acm.org/citation.cfm?id=92860},
	doi = {10.3115991365.991407},
	abstract = {In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.},
	number = {2},
	journal = {Computational Linguistics},
	author = {Brown, Peter F. and Cocke, John and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Jelinek, Frederick and Lafferty, John D. and Mercer, Robert L. and Roossin, Paul S.},
	year = {1990},
	note = {ISBN: 0891-2017},
	pages = {79--85},
}

@article{langlet_improving_2015,
	title = {Improving social relationships in face-to-face human-agent interactions: when the agent wants to know user's likes and dislikes},
	url = {http://www.aclweb.org/anthology/P15-1103},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Langlet, Caroline and Clavel, Chloé},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1064--1073},
}

@article{niculae_linguistic_nodate,
	title = {Linguistic {Harbingers} of {Betrayal} : {A} {Case} {Study} on an {Online} {Strategy} {Game}},
	author = {Niculae, Vlad and Kumar, Srijan and Boyd-graber, Jordan and Danescu-niculescu-mizil, Cristian},
	note = {arXiv: 1506.04744v1
ISBN: 9781941643723},
}

@article{gayo_avello_limits_2011,
	title = {Limits of {Electoral} {Predictions} using {Twitter}},
	url = {http://digibuo.uniovi.es/dspace/handle/10651/11899},
	abstract = {Fifth International AAAI Conference on Weblogs and Social, 17-21 July 2011, Barcelona, Spain},
	author = {Gayo Avello, Daniel and Metaxas, Panagiotis T. and Mustafaraj, Eni},
	year = {2011},
	note = {ISBN: 978-1-57735-505-2},
	keywords = {info:eu-repo/semantics/conferenceObject},
	pages = {490--493},
}

@article{somasundaran_recognizing_2009,
	title = {Recognizing {Stances} in {Online} {Debates}},
	number = {August},
	author = {Somasundaran, Swapna and Wiebe, Janyce},
	year = {2009},
	pages = {226--234},
}

@article{yu_detecting_2015,
	title = {Detecting {Deceptive} {Groups} {Using} {Conversations} and {Network} {Analysis}},
	url = {http://www.aclweb.org/anthology/P15-1083},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Yu, Dian and Tyshchuk, Yulia and Ji, Heng and Wallace, William},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {857--866},
}

@article{klebanov_game-theoretic_2010,
	title = {A {Game}-{Theoretic} {Model} of {Metaphorical} {Bargaining}},
	abstract = {We present a game-theoretic model of bar-gaining over a metaphor in the context of political communication, find its equilib-rium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling dis-course as a dynamic resulting from a num-ber of conflicting pressures, and suggest applications of interest to computational linguists.},
	author = {Klebanov, Beata Beigman and Beigman, Eyal},
	year = {2010},
	pages = {698--709},
}

@article{Ferschke2015,
	title = {A {Lightly} {Supervised} {Approach} to {Role} {Identification} in {Wikipedia} {Talk} {Page} {Discussions}},
	number = {2009},
	author = {Ferschke, Oliver and Yang, Diyi and Rosé, Carolyn P.},
	year = {2015},
	pages = {43--47},
}

@article{Yang2015,
	title = {Weakly {Supervised} {Role} {Identification} in {Teamwork} {Interactions}},
	url = {http://www.aclweb.org/anthology/P15-1161},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Yang, Diyi and Wen, Miaomiao and Rose, Carolyn},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1671--1680},
}

@article{marin_detecting_2011,
	title = {Detecting {Forum} {Authority} {Claims} in {Online} {Discussions}},
	url = {http://www.aclweb.org/anthology/W11-0706\nhttp://www.aclweb.org/anthology/W/W11/W11-0706},
	number = {June},
	journal = {Proceedings of the Workshop on Language in Social Media (LSM 2011)},
	author = {Marin, Alex and Zhang, Bin and Ostendorf, Mari},
	year = {2011},
	note = {ISBN: 978-1-932432-96-1},
	pages = {39--47},
}

@book{nivre_maltparser:_2005,
	title = {{MaltParser}: {A} language-independent system for data-driven dependency parsing},
	volume = {13},
	isbn = {1351324906004},
	abstract = {Parsing unrestricted text is useful for many language technology applications but requires parsing methods that are both robust and efficient. MaltParser is a language-independent sys- tem for data-driven dependency parsing that can be used to induce a parser for a new language from a treebank sample in a simple yet flexible manner. Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data.},
	author = {Nivre, Joakim and Hall, Johan and Nilsson, Jens and Chanev, Atanas and Eryigit, Gülsen and Kübler, Sandra and Marinov, Svetoslav and Marsi, Erwin},
	year = {2005},
	doi = {10.1017/S1351324906004505},
	note = {Publication Title: Natural Language Engineering
Issue: January
ISSN: 1351-3249},
}

@article{titov_joint_2008,
	title = {A joint model of text and aspect ratings for sentiment summarization},
	volume = {51},
	abstract = {Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.},
	number = {June},
	journal = {Proceedings of ACL08 HLT},
	author = {Titov, Ivan and McDonald, Ryan},
	year = {2008},
	pages = {308--316},
}

@article{choi_getting_2011,
	title = {Getting the {Most} out of {Transition}-based {Dependency} {Parsing}},
	volume = {2},
	url = {http://dl.acm.org/citation.cfm?id=2002736.2002869},
	abstract = {This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches evaluated on the same data set.},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL '11): shortpapers},
	author = {Choi, Jinho D. and Palmer, Martha},
	year = {2011},
	note = {ISBN: 978-1-932432-88-6},
	pages = {687--692},
}

@article{mcdonald_non-projective_2005,
	title = {Non-projective dependency parsing using spanning tree algorithms},
	volume = {18},
	url = {http://portal.acm.org/citation.cfm?doid=1220575.1220641},
	doi = {10.3115/1220575.1220641},
	abstract = {We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.},
	number = {October},
	journal = {Emnlp-2005},
	author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Hajic, Jan},
	year = {2005},
	pages = {523--530},
}

@article{Zhou2015,
	title = {A {Neural} {Probabilistic} {Structured}-{Prediction} {Model} for {Transition}-{Based} {Dependency} {Parsing}},
	url = {http://www.aclweb.org/anthology/P15-1117},
	number = {2014},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Zhou, Hao and Zhang, Yue and Huang, Shujian and Chen, Jiajun},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1213--1222},
}

@article{jian_layer-based_2009,
	title = {Layer-{Based} {Dependency} {Parsing} *},
	number = {95},
	author = {Jian, Ping and Zong, Chengqing},
	year = {2009},
	keywords = {dependency layer, dependency parsing, sequence labeling},
	pages = {230--239},
}

@article{bohnet_best_2012,
	title = {The {Best} of {Both} {Worlds} – {A} {Graph}-based {Completion} {Model} for {Transition}-based {Parsers}},
	url = {http://www.aclweb.org/anthology/E12-1009},
	abstract = {Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining advantages from transition-based and graph-based approaches. We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Czech, and German and report competitive labeled and unlabeled attachment scores.},
	journal = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL '12)},
	author = {Bohnet, Bernd and Kuhn, Jonas},
	year = {2012},
	pages = {77--87},
}

@article{zhang_transition-based_2011,
	title = {Transition-based {Dependency} {Parsing} with {Rich} {Non}-local {Features}},
	url = {http://dl.acm.org/citation.cfm?id=2002736.2002777},
	abstract = {Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4\% to 92.9\%, giving the best results so far for transition-based parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL '11): shortpapers},
	author = {Zhang, Yue and Nivre, Joakim},
	year = {2011},
	note = {ISBN: 978-1-932432-88-6},
	pages = {188--193},
}

@article{duan_probabilistic_2007,
	title = {Probabilistic {Parsing} {Action} {Models} for {Multi}-lingual {Dependency} {Parsing}},
	number = {June},
	journal = {Computational Linguistics},
	author = {Duan, Xiangyu and Xu, Bo},
	year = {2007},
	pages = {940--946},
}

@article{collins_incremental_2004,
	title = {Incremental parsing with the perceptron algorithm},
	url = {http://portal.acm.org/citation.cfm?doid=1218955.1218970},
	doi = {10.3115/1218955.1218970},
	abstract = {This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.},
	journal = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics ACL 04},
	author = {Collins, M and Roark, B},
	year = {2004},
	pages = {111--es},
}

@article{zhang_tale_2008,
	title = {A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search},
	url = {http://dl.acm.org/citation.cfm?id=1613715.1613784},
	doi = {10.3115/1613715.1613784},
	abstract = {Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1\% and 86.2\%, respectively.},
	number = {2},
	journal = {EMNLP '08 Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Zhang, Yue and Clark, Stephen},
	year = {2008},
	pages = {562--571},
}

@article{weiss_structured_2015,
	title = {Structured {Training} for {Neural} {Network} {Transition}-{Based} {Parsing}},
	url = {http://www.aclweb.org/anthology/P15-1032},
	abstract = {We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat-ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc-tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26\% un-labeled and 92.41\% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.},
	number = {2012},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	author = {Weiss, David and Alberti, Chris and Collins, Michael and Petrov, Slav},
	year = {2015},
	note = {arXiv: 1506.06158
ISBN: 9781941643723},
	pages = {323--333},
}

@article{andreas_annotating_2012,
	title = {Annotating {Agreement} and {Disagreement} in {Threaded} {Discussion}},
	abstract = {A/D},
	journal = {Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)},
	author = {Andreas, Jacob and Rosenthal, Sara and McKeown, Kathleen},
	year = {2012},
	note = {ISBN: 978-2-9517408-7-7},
	keywords = {agreement, annotation, online discussion},
	pages = {818--822},
}

@article{rosenthal_i_2015,
	title = {I {Couldn}'t {Agree} {More}: {The} {Role} of {Conversational} {Structure} in {Agreement} and {Disagreement} {Detection} in {Online} {Discussions}},
	url = {http://aclweb.org/anthology/W15-4625},
	number = {September},
	journal = {Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	author = {Rosenthal, Sara and McKeown, Kathy},
	year = {2015},
	pages = {168--177},
}

@article{Bender2011,
	title = {Annotating social acts: {Authority} claims and alignment moves in wikipedia talk pages},
	abstract = {We present the AAWD corpus, a collection of 365 discussions drawn from Wikipedia talk pages and annotated with labels capturing two kinds of social acts: alignment moves and authority claims. We describe these social acts and our annotation process, and analyze the resulting data set for interactions between participant status and social acts and between the social acts themselves.},
	number = {June},
	journal = {Proceedings of the Workshop on Language in Social Media (LSM 2011)},
	author = {Bender, E.M. and Morgan, J.T. and Oxley, Meghan and Zachry, Mark and Hutchinson, Brian and Marin, Alex and Zhang, Bin and Ostendorf, Mari},
	year = {2011},
	note = {ISBN: 978-1-932432-96-1},
	pages = {48--57},
}

@misc{dasigi_genre_2012,
	title = {Genre {Independent} {Subgroup} {Detection} in {Online} {Discussion} {Threads}: {A} {Pilot} {Study} of {Implicit} {Attitude} using {Latent} {Textual} {Semantics}},
	author = {Dasigi, Pradeep and Guo, Weiwei and Diab, Mona},
	year = {2012},
	note = {Pages: 65-69},
}

@article{greene_more_2009,
	title = {More than words: syntactic packaging and implicit sentiment},
	issn = {19448244},
	url = {http://portal.acm.org/citation.cfm?id=1620827},
	doi = {10.1021/am100309w},
	abstract = {Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text. In this paper, we introduce a new approach to the problem that focuses not on lexical indicators, but on the syntactic “packaging” of ideas, which is well suited to investigating the identi?cation of implicit sentiment, or perspective. We establish a strong predictive connection between linguistically well motivated features and implicit sentiment, and then show how computational approximations of these features can be used to improve on existing state-of-the-art sentiment classi?cation results.},
	number = {June},
	journal = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Greene, Stephan and Resnik, Philip},
	year = {2009},
	note = {ISBN: 9180236006},
	pages = {503--511},
}

@article{hardisty_modeling_2010,
	title = {Modeling perspective using adaptor grammars},
	url = {papers2://publication/uuid/DC22CDE0-51E6-4DDE-A161-CD0D4B526A00},
	abstract = {Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conserva- tive rather than progressive viewpoint. How- ever, going beyond unigram or bigram features in perspective classification gives rise to prob- lems of data sparsity. We address this prob- lem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., 2006). We demonstrate that an adaptive na ̈ıve Bayes model captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classifica- tion results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view.},
	journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	author = {Hardisty, E A and Boyd-Graber, J and Resnik, P},
	year = {2010},
	pages = {284--292},
}

@article{pollack_collaboration_2012,
	title = {Collaboration amidst disagreement and moral judgment: {The} dynamics of {Jewish} and {Arab} students' collaborative inquiry of their joint past},
	volume = {7},
	issn = {15561607 (ISSN)},
	url = {http://link.springer.com/10.1007/s11412-011-9138-4\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84858000729&partnerID=40&md5=4b2c32e02464dcc1e59105cde8b0444b},
	doi = {10.1007/s11412-011-9138-4},
	abstract = {We present an instructional model involving a computer-supported collaborative learning environment, in which students from two conflicting groups collaboratively investigate an event relevant to their past using historical texts. We traced one enactment of the model by a group comprised of two Israeli Jewish and two Israeli Arab students. Our data sources included the texts participants wrote-pre-, post- and during the activity, jointly and individually-the transcripts of the e-discussion and reflections written after the activity. The setting enabled us to further our understanding of what collaboration means when students' voices do not converge. We examined whether the activity was productive in terms of learning, and the dynamics of collaboration within the milieu, especially the intersubjective meaning making. The e-discussion that was co-constructed by participants was a chain of disagreements. However, participants' reflections reveal that the group structure and the e-communication method were perceived as affording sensitive collaboration. Furthermore, a comparison between the individual texts, pre- and post- the group discussion, revealed that the activity was productive, since students moved from a one-sided presentation of the event to a more multi-sided representation. Based on the analysis of the e-discussion, we conclude that the setting provided students with opportunities to examine their voices in light of alternatives. We propose the term fission to articulate certain moments of intersubjectivity, where a crack is formed in one's voice as the Other's voice impacts it, and one's voice become more polyphonic. © 2012 International Society of the Learning Sciences, Inc.; Springer Science + Business Media, LLC.},
	number = {1},
	journal = {International Journal of Computer-Supported Collaborative Learning},
	author = {Pollack, Sarah and Kolikant, Y.B.-D. and Ben-David Kolikant, Yifat},
	year = {2012},
	note = {ISBN: 1141201191384},
	keywords = {Polyphony, Wiki, collaborative learning, conflict, historical thinking, intersubjectivity},
	pages = {109--128},
}

@article{lin_are_2006,
	title = {Are {These} {Documents} {Written} from {Different} {Perspectives}? {A} {Test} of {Different} {Perspectives} {Based} {On} {Statistical} {Distribution} {Divergence}},
	doi = {10.3115/1220175.1220308},
	abstract = {In this paper we investigate how to auto- matically determine if two document col- lections are written from different per- spectives. By perspectives we mean a point of view, for example, from the per- spective of Democrats or Republicans. We propose a test of different perspectives based on distribution divergence between the statistical models of two collections. Experimental results show that the test can successfully distinguish document collec- tions of different perspectives from other types of collections.},
	number = {July},
	journal = {Acl 06},
	author = {Lin, W.H. and A., Hauptmann},
	year = {2006},
	note = {ISBN: 1932432655},
	pages = {1057--1064},
}

@article{lin_which_2006,
	title = {Which side are you on?: identifying perspectives at the document and sentence levels},
	url = {papers2://publication/uuid/AE3F593A-DE16-4363-AF9C-D7E3BD0A1D11},
	doi = {http://acl.ldc.upenn.edu//W/W06/W06-2915.pdf},
	abstract = {In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Repub- licans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a per- spective. Can computers learn to identify which sentences strongly convey a partic- ular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed mod- els on articles about the Israeli-Palestinian conflict. The results show that the pro- posed models successfully learn how per- spectives are reflected in word usage and can identify the perspective of a document with high accuracy.},
	number = {June},
	journal = {Proceedings of the Tenth Conference on Computational Natural Language Learning},
	author = {Lin, W H and Wilson, T and Wiebe, J and Hauptmann, a},
	year = {2006},
	pages = {109--116},
}

@article{nguyen_analysis_2010,
	title = {An analysis of perspectives in interactive settings},
	abstract = {In this paper we investigate the effect of the context of in-teraction on the extent to which a contributor's perspective bias is displayed through their lexical choice. We present a series of experiments on political discussion data. Our experiments indicate that (i) when people quote contribu-tors with an opposing view, they tend to quote the words that are less strongly associated with the opposing view. (ii) Nevertheless, in quoting their opponents, the displayed bias of their word distributions shifts towards that of their opponents. (iii) The personal bias of the speaker is dis-played most clearly through the words that are not quoted, (iv) although characteristics of the quoted message do have a measurable effect on the words that are included in the contribution. And, finally, (v) posts are influenced by the displayed bias of previous posts in a thread.},
	journal = {Workshop on Social Media Analytics 2010},
	author = {Nguyen, Dong and Mayfield, Elijah and Rosé, Carolyn P},
	year = {2010},
	note = {ISBN: 9781450302173},
	keywords = {Natural Language Process-ing—text analysis, perspective, political discourse, sentiment analysis, social media},
}

@article{blei_latent_2012,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	issn = {15324435},
	url = {http://www.cs.princeton.edu/~blei/lda-c/\npapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993\npapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A\npapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6\nhttp://www.crossref.org/jmlr},
	doi = {10.1162/jmlr.2003.3.4-5.993},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	number = {4-5},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	year = {2012},
	pmid = {21362469},
	note = {arXiv: 1111.6189v1
ISBN: 9781577352815},
	keywords = {lda, topic model},
	pages = {993--1022},
}
